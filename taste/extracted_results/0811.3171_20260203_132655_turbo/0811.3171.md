# Quantum algorithm for linear systems of equations

Aram W. Harrow,1 Avinatan Hassidim,2 and Seth Lloyd3 $^ { 1 }$ Department of Mathematics, University of Bristol, Bristol, BS8 1TW, U.K. ${ \boldsymbol { \mathscr { Z } } }$ MIT - Research Laboratory for Electronics, Cambridge, MA 02139, USA $_ { 3 }$ MIT - Research Laboratory for Electronics and Department of Mechanical Engineering, Cambridge, MA 02139, USA

Solving linear systems of equations is a common problem that arises both on its own and as a subroutine in more complex problems: given a matrix $A$ and a vector $\vec { b }$ , find a vector $\vec { x }$ such that $A { \overrightarrow { x } } = { \overrightarrow { b } }$ . We consider the case where one doesn’t need to know the solution $\vec { x }$ itself, but rather an approximation of the expectation value of some operator associated with $\vec { x }$ , e.g., ${ \vec { x } } ^ { \dagger } M { \vec { x } }$ for some matrix $M$ . In this case, when $A$ is sparse, $N \times N$ and has condition number $\kappa$ , classical algorithms can find $\vec { x }$ and estimate ${ \vec { x } } ^ { \dagger } M { \vec { x } }$ in ${ \tilde { O } } ( N { \sqrt { \kappa } } )$ time. Here, we exhibit a quantum algorithm for this task that runs in p $\mathrm { { J y } } ( \log N , \kappa )$ time, an exponential improvement over the best classical algorithm.

# I. INTRODUCTION

Quantum computers are devices that harness quantum mechanics to perform computations in ways that classical computers cannot. For certain problems, quantum algorithms supply exponential speedups over their classical counterparts, the most famous example being Shor’s factoring algorithm [1]. Few such exponential speedups are known, and those that are (such as the use of quantum computers to simulate other quantum systems [2]) have so far found limited use outside the domain of quantum mechanics. This paper presents a quantum algorithm to estimate features of the solution of a set of linear equations. Compared to classical algorithms for the same task, our algorithm can be as much as exponentially faster.

Linear equations play an important role in virtually all fields of science and engineering. The sizes of the data sets that define the equations are growing rapidly over time, so that terabytes and even petabytes of data may need to be processed to obtain a solution. In other cases, such as when discretizing partial differential equations, the linear equations may be implicitly defined and thus far larger than the original description of the problem. For a classical computer even to approximate the solution of $N$ linear equations in $N$ unknowns in general requires time that scales at least as $N$ . Indeed, merely to write out the solution takes time of order $N$ . Frequently, however, one is interested not in the full solution to the equations, but rather in computing some function of that solution, such as determining the total weight of some subset of the indices. We show that in some cases, a quantum computer can approximate the value of such a function in time which scales logarithmically in $N$ , and polynomially in the condition number (defined below) and desired precision. The dependence on $N$ is exponentially better than what is achievable classically, while the dependence on condition number is comparable, and the dependence on error is worse. Thus our algorithm can achieve useful, and even exponential, speedups in a wide variety of settings where $N$ is large and the condition number is small.

We sketch here the basic idea of our algorithm, and then discuss it in more detail in the next section. Given a   
Hermitian $N \times N$ matrix $A$ , and a unit vector $\vec { b }$ , suppose we would like to find $\vec { x }$ satisfying $A { \overrightarrow { x } } = { \overrightarrow { b } }$ . (We discuss   
later questions of efficiency as well as how the assumptions we have made about $A$ and $\vec { b }$ can be relaxed.) First, the   
algorithm represents $\vec { b }$ as a quantum state $\begin{array} { r } { \left| b \right. = \sum _ { i = 1 } ^ { N } b _ { i } \left| i \right. } \end{array}$ . Next, we use techniques of Hamiltonian simulation[3, 4]   
to apply $e ^ { i A t }$ to $| b \rangle$ for a superposition of different times $t$ . This ability to exponentiate $A$ translates, via the well  
known technique of phasecorresponding eigenvalues timation[5–7], into the ability to decompose . Informally, the state of the system after thi $| b \rangle$ in the eigenbatage is close to $A$ find the, where $\begin{array} { r } { \sum _ { j = 1 } ^ { N } \beta _ { j } \left| u _ { j } \right. \left| \lambda _ { j } \right. } \end{array}$ $u _ { j }$ is the eigenvector basis of $A$ , and $\begin{array} { r } { \left| b \right. = \sum _ { j = 1 } ^ { N } \beta _ { j } \left| u _ { j } \right. } \end{array}$ j=1   . We would then like to perform the linear map taking $| \lambda _ { j } \rangle$   
to $C \lambda _ { j } ^ { - 1 } \left| \lambda _ { j } \right.$ , where is a normalizing constant. As this operation is not unitary, it has some probability of failing,   
left with a state proportional to which will enter into our discussion of the run-time below. After it succeeds, we uncompute the $\underset { \ b { \mathscr { a } } } { \sum } \beta _ { j } \lambda _ { j } ^ { - 1 } \left| u _ { j } \right. = A ^ { - 1 } \left| b \right. = \left| x \right.$ . $| \lambda _ { j } \rangle$ register and are

An important factor in the performance of the matrix inversion algorithm is $\kappa$ , the condition number of $A$ , or the ratio between $A$ ’s largest and smallest eigenvalues. As the condition number grows, $A$ becomes closer to a matrix which cannot be inverted, and the solutions become less stable. Such a matrix is said to be “ill-conditioned.” Our algorithms will generally assume that the singular values of $A$ lie between $1 / \kappa$ and 1; equivalently $\kappa ^ { - 2 } I \leq A ^ { \dagger } A \leq I$ . In this case, our runtime will scale as $\kappa ^ { 2 } \log ( N ) / \epsilon$ , where $\epsilon$ is the additive error achieved in the output state $| x \rangle$ . Therefore, the greatest advantage our algorithm has over classical algorithms occurs when both $\kappa$ and $1 / \epsilon$ are poly $\log ( N )$ , in which case it achieves an exponential speedup. However, we will also discuss later some techniques for handling ill-conditioned matrices.

This procedure yields a quantum-mechanical representation $| x \rangle$ of the desired vector $\vec { x }$ . Clearly, to read out all the components of $\vec { x }$ would require one to perform the procedure at least $N$ times. However, often one is interested not in $\vec { x }$ itself, but in some expectation value ${ \vec { x } } ^ { I } M { \vec { x } }$ , where $M$ is some linear operator (our procedure also accommodates nonlinear operators as described below). By mapping $M$ to a quantum-mechanical operator, and performing the quantum measurement corresponding to $M$ , we obtain an estimate of the expectation value $\langle x \vert M \vert x \rangle = { \vec { x } } ^ { T } M { \vec { x } }$ , as desired. A wide variety of features of the vector $\vec { x }$ can be extracted in this way, including normalization, weights in different parts of the state space, moments, etc.

A simple example where the algorithm can be used is to see if two different stochastic processes have similar stable state [8]. Consider a stochastic process $\vec { x } _ { t } = A \vec { x } _ { t - 1 } + \vec { b }$ , where the $i$ ’th coordinate in the vector ${ \vec { x } } _ { t }$ represents the abundance of item $i$ in time $t$ . The stable state of this distribution is given by $\left| x \right. = ( I - A ) ^ { - 1 } \left| b \right.$ . Let $\vec { x } _ { t } ^ { \prime } = A ^ { \prime } \vec { x } _ { t - 1 } ^ { \prime } + \vec { b } ^ { \prime }$ , and $\left| x ^ { \prime } \right. = ( I - A ^ { \prime } ) ^ { - 1 } \left| b ^ { \prime } \right.$ . To know if $| x \rangle$ and $| x ^ { \prime } \rangle$ are similar, we perform the SWAP test between them [9]. We note that classically finding out if two probability distributions are similar requires at least $O ( { \sqrt { N } } )$ samples [10].

The strength of the algorithm is that it works only with ${ \cal O } ( \log N )$ -qubit registers, and never has to write down all of $A$ , $\vec { b }$ or $\vec { x }$ . In situations (detailed below) where the Hamiltonian simulation and our non-unitary step incur only poly $\log ( N )$ overhead, this means our algorithm takes exponentially less time than a classical computer would need even to write down the output. In that sense, our algorithm is related to classical Monte Carlo algorithms, which achieve dramatic speedups by working with samples from a probability distribution on $N$ objects rather than by writing down all $N$ components of the distribution. However, while these classical sampling algorithms are powerful, we will prove that in fact any classical algorithm requires in general exponentially more time than our quantum algorithms to perform the same matrix inversion task.

Outline The rest of the Letter proceeds by first describing our algorithm in detail, analyzing its run-time and comparing it with the best known classical algorithms. Next, we prove (modulo some complexity-theoretic assumptions) hardness results for matrix inversion that imply both that our algorithm’s run-time is nearly optimal, and that it runs exponentially faster than any classical algorithm. We conclude with a discussion of applications, generalizations and extensions.

Related work Previous papers gave quantum algorithms to perform linear algebraic operations in a limited setting [11]. Our work was extended by [12] to solving nonlinear differential equations.

# II. ALGORITHM

We now give a more detailed explanation of the algorithm. First, we want to transform a given Hermitian matrix $A$ into a unitary operator $e ^ { i A t }$ which we can apply at will. This is possible (for example) if $A$ is $s$ -sparse and efficiently row computable, meaning it has at most $s$ nonzero entries per row and given a row index these entries can be computed in time $O ( s )$ . Under these assumptions, Ref. [3] shows how to simulate $e ^ { i A t }$ in time

$$
{ \tilde { O } } ( \log ( N ) s ^ { 2 } t ) ,
$$

where the $\ddot { O }$ suppresses more slowly-growing terms (described in [13]). If $A$ is not Hermitian, define

$$
C = \left( \begin{array} { l l } { { 0 } } & { { A } } \\ { { A ^ { \dagger } } } & { { 0 } } \end{array} \right)
$$

As $C$ is Hermitian, we can solve the equation $C { \vec { y } } = { \binom { \vec { b } } { 0 } }$ to obtain $y = { \binom { 0 } { \vec { x } } }$ . Applying this reduction if necessary, the rest of the Letter assumes that is Hermitian.

We also need an efficient procedure to prepare $| b \rangle$ . For example, if $b _ { i }$ and $\textstyle \sum _ { i = i _ { 1 } } ^ { i _ { 2 } } | b _ { i } | ^ { 2 }$ are efficiently computable then we can use the procedure of Ref. [14] to prepare $| b \rangle$ . Alternatively, our algorithm could be a subroutine in a larger quantum algorithm of which some other component is responsible for producing $| b \rangle$ .

The next step is to decompose $| b \rangle$ in the eigenvector basis, using phase estimation [5–7]. Denote by $| u _ { j } \rangle$ the eigenvectors of $A$ (or equivalently, of $e ^ { i A t }$ ), and by $\lambda _ { j }$ the corresponding eigenvalues. Let

$$
\left| \Psi _ { 0 } \right. : = \sqrt { \frac { 2 } { T } } \sum _ { \tau = 0 } ^ { T - 1 } \sin \frac { \pi ( \tau + \frac { 1 } { 2 } ) } { T } \left| \tau \right.
$$

for some large $T$ . The coefficients of $\left| { \Psi _ { 0 } } \right.$ are chosen (following [5, 7]) to minimize a certain quadratic loss function which appears in our error analysis (see [13] for details).

Next we apply the conditional Hamiltonian evolution $\begin{array} { r } { \sum _ { \tau = 0 } ^ { T - 1 } | \tau \rangle \langle \tau | ^ { C } \otimes e ^ { i A \tau t _ { 0 } / T } } \end{array}$ on $\left| \Psi _ { 0 } \right. ^ { C } \otimes \left| b \right.$ , where $t _ { 0 } = O ( \kappa / \epsilon )$ . Fourier transforming the first register gives the state

$$
\sum _ { j = 1 } ^ { N } \sum _ { k = 0 } ^ { T - 1 } \alpha _ { k \mid j } \beta _ { j } \left. k \right. \left. u _ { j } \right. ,
$$

whe $| k \rangle$ are the Fourier basis states, and $| \alpha _ { k | j } |$ is large if and only if $\begin{array} { r } { \lambda _ { j } \approx \frac { 2 \pi k } { t _ { 0 } } } \end{array}$ . Defining $\bar { \lambda } _ { k } : = 2 \pi k / t _ { 0 }$ , we can relabel $| k \rangle$

$$
\sum _ { j = 1 } ^ { N } \sum _ { k = 0 } ^ { T - 1 } \alpha _ { k | j } \beta _ { j } \left| \tilde { \lambda } _ { k } \right. | u _ { j } \rangle
$$

Adding an ancilla qubit and rotating conditioned on $\left| \tilde { \lambda } _ { k } \right.$ yields

$$
\sum _ { j = 1 } ^ { N } \sum _ { k = 0 } ^ { T - 1 } \alpha _ { k | j } \beta _ { j } \left| \tilde { \lambda } _ { k } \right. | u _ { j } \rangle \left( \sqrt { 1 - \frac { C ^ { 2 } } { \tilde { \lambda } _ { k } ^ { 2 } } } \left| 0 \right. + \frac { C } { \tilde { \lambda } _ { k } } \left| 1 \right. \right) ,
$$

where $C = O ( 1 / \kappa )$ . We now undo the phase estimation to uncompute the $\left| \tilde { \lambda } _ { k } \right.$ . If the phase estimation were perfect, we would have $\alpha _ { k | j } = 1$ if $\bar { \lambda } _ { k } = \lambda _ { j }$ , and $0$ otherwise. Assuming this for now, we obtain

$$
\sum _ { j = 1 } ^ { N } \beta _ { j } \left| u _ { j } \right. \left( \sqrt { 1 - \frac { C ^ { 2 } } { \lambda _ { j } ^ { 2 } } } \left| 0 \right. + \frac { C } { \lambda _ { j } } \left| 1 \right. \right)
$$

To finish the inversion we measure the last qubit. Conditioned on seeing 1, we have the state

$$
\sqrt { \frac { 1 } { \sum _ { j = 1 } ^ { N } C ^ { 2 } | \beta _ { j } | ^ { 2 } / | \lambda _ { j } | ^ { 2 } } } \sum _ { j = 1 } ^ { N } \beta _ { j } \frac { C } { \lambda _ { j } } \left| u _ { j } \right.
$$

which corresponds to $\begin{array} { r } { | x \rangle = \sum _ { j = 1 } ^ { n } \beta _ { j } \lambda _ { j } ^ { - 1 } | u _ { j } \rangle } \end{array}$ up to normalization. We can determine the normalization factor from the probability of obtaining 1. Finally, we make a measurement $M$ whose expectation value $\langle x \vert M \vert x \rangle$ corresponds to the feature of $\vec { x }$ that we wish to evaluate.

Run-time and error analysis We present an informal description of the sources of error; the exact error analysis and runtime considerations are presented in [13]. Performing the phase estimation is done by simulating $e ^ { i A t }$ . Assuming that $A$ is $s$ -sparse, this can be done with error $\epsilon$ in time proportional to $t s ^ { 2 } ( t / \epsilon ) ^ { o ( 1 ) } = : \tilde { O } ( t s ^ { 2 } )$ .

The dominant source of error is phase estimation. This step errs by $O ( 1 / t _ { 0 } )$ in estimating $\lambda$ , which translates into a relative error of $O ( 1 / \lambda t _ { 0 } )$ in $\lambda ^ { - 1 }$ . If $\lambda \geq 1 / \kappa$ taking $t _ { 0 } = O ( \kappa / \epsilon )$ induces a final error of $\epsilon$ . Finally, we consider the success probability of the post-selection process. Since $C = O ( 1 / \kappa )$ and $\lambda \leq 1$ , this probability is at least $\Omega ( 1 / \kappa ^ { 2 } )$ . Using amplitude amplification [15], we find that $O ( \kappa )$ repetitions are sufficient. Putting this all together, we obtain the stated runtime of $\tilde { O } \left( \log ( N ) s ^ { 2 } \kappa ^ { 2 } / \epsilon \right)$ .

# III. OPTIMALITY

Classical matrix inversion algorithms To put our algorithm in context, one of the best general-purpose classical matrix inversion algorithms is the conjugate gradient method [16], which, when $A$ is positive definite, uses ${ \cal O } ( \sqrt { \kappa } \log ( 1 / \epsilon ) )$ matrix-vector multiplications each taking time $O ( N s )$ for a total runtime of ${ \cal O } ( N s \sqrt { \kappa } \log ( 1 / \epsilon ) )$ . (If $A$ is not positive definite, $O ( \kappa \log ( 1 / \epsilon ) )$ multiplications are required, for a total time of ${ \cal O } ( N s \kappa \log ( 1 / \epsilon ) )$ .) An important question is whether classical methods can be improved when only a summary statistic of the solution, such as ${ \vec { x } } ^ { \dagger } M { \vec { x } }$ , is required. Another question is whether our quantum algorithm could be improved, say to achieve error $\epsilon$ in time proportional to poly $\log ( 1 / \epsilon )$ . We show that the answer to both questions is negative, using an argument from complexity theory. Our strategy is to prove that the ability to invert matrices (with the right choice of parameters) can be used to simulate a general quantum computation.

The complexity of matrix inversion We show that a quantum circuit using $n$ qubits and $T$ gates can be simulated by inverting an $O ( 1 )$ -sparse matrix $A$ of dimension $N = O ( 2 ^ { n } \kappa )$ . The condition number $\kappa$ is $O ( T ^ { 2 } )$ if we need $A$ to be positive definite or $O ( T )$ if not. This implies that a classical poly $( \log N , \kappa , 1 / \epsilon )$ -time algorithm would be able to simulate a poly $( n )$ -gate quantum algorithm in $\mathrm { p o l y } ( n )$ time. Such a simulation is strongly conjectured to be false, and is known to be impossible in the presence of oracles [17].

The reduction from a general quantum circuit to a matrix inversion problem also implies that our algorithm cannot be substantially improved (under standard assumptions). If the run-time could be made polylogarithmic in $\kappa$ , then any problem solvable on $n$ qubits could be solved in poly(n) time (i.e. BQP=PSPACE), a highly unlikely possibility. Even improving our $\kappa$ -dependence to $\kappa ^ { 1 - \partial }$ for $\delta > 0$ would allow any time- $T$ quantum algorithm to be simulated in time $o ( T )$ ; iterating this would again imply that BQP=PSPACE. Similarly, improving the error dependence to poly $\log ( 1 / \epsilon )$ would imply that BQP includes PP, and even minor improvements would contradict oracle lower bounds [18].

The reduction We now present the key reduction from simulating a quantum circuit to matrix inversion. Let $\boldsymbol { \mathscr { C } }$ be a quantum circuit acting on $n = \log N$ qubits which applies $T$ two-qubit gates $U _ { 1 } , \dots U _ { T }$ . The initial state is $\left| 0 \right. ^ { \otimes n }$ and the answer is determined by measuring the first qubit of the final state.

Now adjoin an ancilla register of dimension $3 T$ and define a unitary

$$
U = \sum _ { t = 1 } ^ { T } | t + 1 \rangle \langle t | \otimes U _ { t } + | t + T + 1 \rangle \langle t + T | \otimes I + | t + 2 T + 1 \bmod 3 T \rangle \langle t + 2 T | \otimes U _ { 3 T + 1 - t } ^ { \dag } .
$$

We have chosen $U$ so that for $T + 1 \leq t \leq 2 T$ , applying $U ^ { t }$ to $\left| 1 \right. \left| \psi \right.$ yields $\left| t + 1 \right. \otimes U _ { T } \cdot \cdot \cdot U _ { 1 } \left| \psi \right.$ . If we now define $A = I - U e ^ { - 1 / T }$ then $\kappa ( A ) = O ( T )$ , and we can expand

$$
A ^ { - 1 } = \sum _ { k \geq 0 } U ^ { k } e ^ { - k / T } ,
$$

This can be interpreted as applying $U ^ { t }$ for $t$ a geometrically-distributed random variable. Since $U ^ { 3 T ^ { \prime } } = I$ , we can assume $1 \leq t \leq 3 T$ . If we measure the first register and obtain $T + 1 \leq t \leq 2 T$ (which occurs with probability $e ^ { - 2 } / ( 1 + e ^ { - 2 } + e ^ { - 4 } ) \geq 1 / 1 0$ ) then we are left with the second register in the state $U _ { T } \cdots U _ { 1 } | \psi \rangle$ , corresponding to a successful computation. Sampling from $| x \rangle$ allows us to sample from the results of the computation. This establishes that matrix inversion is BQP-complete, and proves our above claims about the difficulty of improving our algorithm.

# IV. DISCUSSION

There are a number of ways to extend our algorithm and relax the assumptions we made while presenting it. We will discuss first how to invert a broader class of matrices and then consider measuring other features of $\vec { x }$ and performing operations on $A$ other than inversion.

Certain non-sparse $A$ can be simulated and therefore inverted; see [4] for techniques and examples. It is also possible to invert non-square matrices, using the reduction presented from the non-Hermitian case to the Hermitian one.

The matrix inversion algorithm can also handle ill-conditioned matrices by inverting only the part of $| b \rangle$ which is in the well-conditioned part of the matrix. Formally, instead of transforming $\begin{array} { r } { \left| b \right. = \sum _ { j } \beta _ { j } \left| u _ { j } \right. } \end{array}$ to $\begin{array} { r } { \left| x \right. = \sum _ { j } \lambda _ { j } ^ { - 1 } \beta _ { j } \left| u _ { j } \right. } \end{array}$ , we transform it to a state which is close to

$$
\sum _ { j , \lambda _ { j } < 1 / \kappa } \lambda _ { j } ^ { - 1 } \beta _ { j } \left| u _ { j } \right. \left| { \mathrm { w e l l } } \right. + \sum _ { j , \lambda _ { j } \geq 1 / \kappa } \beta _ { j } \left| u _ { j } \right. \left| { \mathrm { i l l } } \right.
$$

in time proportional to $\kappa ^ { 2 }$ for any chosen $\kappa$ (i.e. not necessarily the true condition number of $A$ ). The last qubit is a flag which enables the user to estimate what the size of the ill-conditioned part, or to handle it in any other way she wants. This behavior can be advantageous if we know that $A$ is not invertible and we are interested in the projection of $| b \rangle$ on the well-conditioned part of $A$ .

Another method that is often used in classical algorithms to handle ill-conditioned matrices is to apply a preconditioner[19]. If we have a method of generating a preconditioner matrix $B$ such that $\kappa ( A B )$ is smaller than $\kappa ( A )$ , then we can solve $A { \overrightarrow { x } } = { \overrightarrow { b } }$ by instead solving the possibly easier matrix inversion problem $( A B ) \vec { c } = B \vec { b }$ . Further, if $A$ and $B$ are both sparse, then $A B$ is as well. Thus, as long as a state proportional to $B \left| b \right.$ can be efficiently prepared, our algorithm could potentially run much faster if a suitable preconditioner is used.

The outputs of the algorithm can also be generalized. We can estimate degree- $2 k$ polynomials in the entries of $\vec { x }$ by generating $k$ copies of $| x \rangle$ and measuring the $n k$ -qubit observable

$$
\sum _ { i _ { 1 } , . . . , i _ { k } , j _ { 1 } , . . . , j _ { k } } M _ { i _ { 1 } , . . . , i _ { k } , j _ { 1 } , . . . , j _ { k } } \left| i _ { 1 } , . . . , i _ { k } \right. \left. j _ { 1 } , . . . , j _ { k } \right|
$$

on the state $\left| x \right. ^ { \otimes k }$ . Alternatively, one can use our algorithm to generate a quantum analogue of Monte-Carlo, where given $A$ and $\vec { b }$ we sample from the vector $\vec { x }$ , meaning that the value $i$ occurs with probability $| \vec { x } _ { i } | ^ { 2 }$ .

Perhaps the most far-reaching generalization of the matrix inversion algorithm is not to invert matrices at all! Instead, it can compute $f ( A ) \left| b \right.$ for any computable $f$ . Depending on the degree of nonlinearity of $f$ , nontrivial tradeoffs between accuracy and efficiency arise. Some variants of this idea are considered in [4, 12, 20].

Acknowledgements. We thank the W.M. Keck foundation for support, and AWH thanks them as well as MIT for hospitality while this work was carried out. AWH was also funded by the U.K. EPSRC grant “QIP IRC.” SL thanks R. Zecchina for encouraging him to work on this problem. We are grateful as well to R. Cleve, D. Farmer, S. Gharabian, J. Kelner, S. Mitter, P. Parillo, D. Spielman and M. Tegmark for helpful discussions.

# APPENDIX A: PROOF DETAILS

In this appendix, we describe and analyze our algorithm in full detail. While the body of the paper attempted to convey the spirit of the procedure and left out various improvements, here we take the opposite approach and describe everything, albeit possibly in a less intuitive way. We also describe in more detail our reductions from non-Hermitian matrix inversion to Hermitian matrix inversion (Section A 4) and from a general quantum computation to matrix inversion (Section A 5).

As inputs we require a procedure to produce the state $| b \rangle$ , a method of producing the $\leq s$ non-zero elements of any row of $A$ and a choice of cutoff $\kappa$ . Our run-time will be roughly quadratic in $\kappa$ and our algorithm is guaranteed to be correct if $\| A \| \leq 1$ and $\| A ^ { - 1 } \| \leq \kappa$ .

The condition number is a crucial parameter in the algorithm. Here we present one possible method of handling ill-conditioned matrices. We will define the well-conditioned part of $A$ to be the span of the eigenspaces corresponding to eigenvalues $\ge 1 / \kappa$ and the ill-conditioned part to be the rest. Our strategy will be to flag the ill-conditioned part of the matrix (without inverting it), and let the user choose how to further handle this. Since we cannot exactly resolve any eigenvalue, we can only approximately determine whether vectors are in the well- or ill-conditioned subspaces. Accordingly, we choose some $\kappa ^ { \prime } > \kappa$ (say $\kappa ^ { \prime } = 2 \kappa$ ). Our algorithm then inverts the well-conditioned part of the matrix, flags any eigenvector with eigenvalue $\le 1 / \kappa ^ { \prime }$ as ill-conditioned, and interpolates between these two behaviors when $1 / \kappa ^ { \prime } < | \lambda | < 1 / \kappa$ . This is described formally in the next section. We present this strategy not because it is necessarily ideal in all cases, but because it gives a concrete illustration of the key components of our algorithm.

Finally, the algorithm produces $| x \rangle$ only up to some error $\epsilon$ which is given as part of the input. We work only with pure states, and so define error in terms of distance between vectors, i.e. $\parallel | \alpha \rangle - | \beta \rangle \parallel = { \sqrt { 2 ( 1 - \operatorname { R e } \left. \alpha | \beta \right. ) } }$ . Since ancilla states are produced and then imperfectly uncomputed by the algorithm, our output state will technically have high fidelity not with $| x \rangle$ but with $\left| x \right. \left| 0 0 0 \ldots \right.$ . In general we do not write down ancilla qubits in the $| 0 \rangle$ state, so we write $| x \rangle$ instead of $\left| x \right. \left| 0 0 0 \ldots \right.$ for the target state, $| b \rangle$ instead of $\left| b \right. \left| 0 0 0 \ldots \right.$ for the initial state, and so on.

# 1. Detailed description of the algorithm

To produce the input state $| b \rangle$ , we assume that there exists an efficiently-implementable unitary $B$ , which when applied to |initiali produces the state $| b \rangle$ , possibly along with garbage in an ancilla register. We make no further assumption about $B$ ; it may represent another part of a larger algorithm, or a standard state-preparation procedure such as [14]. Let $T _ { B }$ be the number of gates required to implement $B$ . We neglect the possibility that $B$ errs in producing $| b \rangle$ since, without any other way of producing or verifying the state $| b \rangle$ , we have no way to mitigate these errors. Thus, any errors in producing $| b \rangle$ necessarily translate directly into errors in the final state $| x \rangle$ .

Next, we define the state

$$
\left| { \Psi _ { 0 } } \right. = \sqrt { \frac { 2 } { T } } \sum _ { \tau = 0 } ^ { T - 1 } \sin \frac { \pi ( \tau + \frac { 1 } { 2 } ) } { T } \left| { \tau } \right.
$$

for a $T$ to be chosen later. Using [14], we can prepare $\left| { \Psi _ { 0 } } \right.$ up to error $\epsilon _ { \Psi }$ in time poly $\log ( T / \epsilon _ { \Psi } )$ .

One other subroutine we will need is Hamiltonian simulation. Using the reductions described in Section $\textrm { A 4 }$ , we can assume that $A$ is Hermitian. To simuluate $e ^ { i A t }$ for some $t \geq 0$ , we use the algorithm of [3]. If $A$ is $s$ -sparse, $t \leq t _ { 0 }$ and we want to guarantee that the error is $\leq \epsilon _ { H }$ , then this requires time

$$
T _ { H } = { \cal O } ( \log ( N ) ( \log ^ { * } ( N ) ) ^ { 2 } s ^ { 2 } t _ { 0 } 9 \sqrt { \log ( s ^ { 2 } t _ { 0 } / \epsilon _ { H } ) } ) = \tilde { \cal O } ( \log ( N ) s ^ { 2 } t _ { 0 } )
$$

The scaling here is better than any power of $1 / \epsilon _ { H }$ , which means that the additional error introduced by this step introduces is negligible compared with the rest of the algorithm, and the runtime is almost linear with $t _ { 0 }$ . Note that this is the only step where we require that $A$ be sparse; as there are some other types of Hamiltonians which can be simulated efficiently (e.g. [3, 4, 21]), this broadens the set of matrices we can handle.

The key subroutine of the algorithm, denoted $U _ { \mathrm { i n v e r t } }$ , is defined as follows:

1. Prepare $\left| \Psi _ { 0 } \right. ^ { C }$ from $| 0 \rangle$ up to error $\epsilon _ { \Psi }$ .

2. Apply the conditional Hamiltonian evolution $\begin{array} { r } { \sum _ { \tau = 0 } ^ { T - 1 } | \tau \rangle \langle \tau | ^ { C } \otimes e ^ { i A \tau t _ { 0 } / T } } \end{array}$ up to error $\epsilon _ { H }$

3. Apply the Fourier transform to the register $C$ . Denote the resulting basis states with $| k \rangle$ , for $k = 0 , . . . T - 1$ Define $\ddot { \lambda } _ { k } : = 2 \pi k / t _ { 0 }$ .

4. Adjoin a three-dimensional register $S$ in the state

$$
\left| h ( \tilde { \lambda } _ { k } ) \right. ^ { S } : = \sqrt { 1 - f ( \tilde { \lambda } _ { k } ) ^ { 2 } - g ( \tilde { \lambda } _ { k } ) ^ { 2 } } \left| \mathrm { n o t h i n g } \right. ^ { S } + f ( \tilde { \lambda } _ { k } ) \left| \mathrm { w e l l } \right. ^ { S } + g ( \tilde { \lambda } _ { k } ) \left| \mathrm { i l l } \right. ^ { S } ,
$$

for functions $f ( \lambda ) , g ( \lambda )$ defined below in (A3). Here ‘nothing’ indicates that the desired matrix inversion hasn’t taken place, ‘well’ indicates that it has, and ‘ill’ means that part of $| b \rangle$ is in the ill-conditioned subspace of $A$ .

5. Reverse steps 1-3, uncomputing any garbage produced along the way.

The functions $f ( \lambda ) , g ( \lambda )$ are known as filter functions[22], and are chosen so that for some constant $C ~ > ~ 1$ $f ( \lambda ) = 1 / C \kappa \lambda$ for $\lambda \geq 1 / \kappa$ , $g ( \lambda ) = 1 / C$ for $\lambda \le 1 / \kappa ^ { \prime } : = 1 / 2 \kappa$ and $f ^ { 2 } ( \lambda ) + g ^ { 2 } ( \lambda ) \leq 1$ for all $\lambda$ . Additionally, $f ( \lambda )$ should satisfy a certain continuity property that we will describe in the next section. Otherwise the functions are arbitrary. One possible choice is

$$
\begin{array} { r l } & { f ( \lambda ) = \left\{ \begin{array} { l l } { \frac { 1 } { 2 \kappa \lambda } } & { \mathrm { w h e n ~ } \lambda \geq 1 / \kappa } \\ { \frac { 1 } { 2 } \sin \left( \frac { \pi } { 2 } \cdot \frac { \lambda - \frac { 1 } { \kappa ^ { \prime } } } { \frac { 1 } { \kappa } - \frac { 1 } { \kappa ^ { \prime } } } \right) } & { \mathrm { w h e n ~ } \frac { 1 } { \kappa } > \lambda \geq \frac { 1 } { \kappa ^ { \prime } } } \\ { 0 } & { \mathrm { w h e n ~ } \frac { 1 } { \kappa ^ { \prime } } > \lambda } \end{array} \right. } \\ & { g ( \lambda ) = \left\{ \begin{array} { l l } { 0 } & { \mathrm { w h e n ~ } \lambda \geq 1 / \kappa } \\ { \frac { 1 } { 2 } \cos \left( \frac { \pi } { 2 } \cdot \frac { \lambda - \frac { 1 } { \kappa ^ { \prime } } } { \frac { 1 } { \kappa } - \frac { 1 } { \kappa ^ { \prime } } } \right) } & { \mathrm { w h e n ~ } \frac { 1 } { \kappa } > \lambda \geq \frac { 1 } { \kappa ^ { \prime } } } \\ { \frac { 1 } { 2 } } & { \mathrm { w h e n ~ } \frac { 1 } { \kappa ^ { \prime } } > \lambda } \end{array} \right. } \end{array}
$$

If $U _ { \mathrm { i n v e r t } }$ is applied to $\left| u _ { j } \right.$ it will, up to an error we will discuss below, adjoin the state $\vert h ( \lambda _ { j } ) \rangle$ . Instead if we apply $U _ { \mathrm { i n v e r t } }$ to $| b \rangle$ (i.e. a superposition of different $\left| u _ { j } \right.$ ), measure $S$ and obtain the outcome ‘well’, then we will have approximately applied an operator proportional to $A ^ { - 1 }$ . Let $\ddot { p }$ (computed in the next section) denote the success probability of this measurement. Rather than repeating $1 / \tilde { p }$ times, we will use amplitude amplification [15] to obtain the same results with $O ( 1 / \sqrt { \tilde { p } } )$ repetitions. To describe the procedure, we introduce two new operators:

$$
R _ { \mathrm { s u c c } } = I ^ { S } - 2 | \mathrm { w e l l } \rangle \langle \mathrm { w e l l } | ^ { S } ,
$$

acting only on the $S$ register and

$$
R _ { \mathrm { i n i t } } = I - 2 | \mathrm { i n i t i a l } \rangle \langle \mathrm { i n i t i a l } | .
$$

Our main algorithm then follows the amplitude amplification procedure: we start with $U _ { \mathrm { i n v e r t } } B \left| \mathrm { i n i t i a l } \right.$ and repeatedly apply $U _ { \mathrm { i n v e r t } } B R _ { \mathrm { i n i t } } B ^ { \dagger } U _ { \mathrm { i n v e r t } } ^ { \dagger } R _ { \mathrm { s u c c } }$ . Finally we measure $S$ and stop when we obtain the result ‘well’. The number of repetitions would ideally be $\pi / 4 \sqrt { \tilde { p } }$ , which in the next section we will show is $O ( \kappa )$ . While $\tilde { p }$ is initially unknown, the procedure has a constant probability of success if the number of repetitions is a constant fraction of $\pi / 4 \tilde { p }$ . Thus, following [15] we repeat the entire procedure with a geometrically increasing number of repetitions each time: 1, 2, 4, 8, . . . , until we have reached a power of two that is $\geq \kappa$ . This yields a constant probability of success using $\leq 4 \kappa$ repetitions.

Putting everything together, the run-time is $\tilde { O } ( \kappa ( T _ { B } + t _ { 0 } s ^ { 2 } \log ( N ) )$ , where the $\tilde { O }$ suppresses the more-slowly growing terms of $( \log ^ { * } ( N ) ) ^ { 2 }$ , $\exp ( O ( 1 / \sqrt { \log ( t _ { 0 } / \epsilon _ { H } ) } ) )$ and poly $\log ( T / \epsilon _ { \Psi } )$ . In the next section, we will show that $t _ { 0 }$ can be taken to be $O ( \kappa / \epsilon )$ so that the total run-time is $\tilde { O } ( \kappa T _ { B } + \kappa ^ { 2 } s ^ { 2 } \log ( N ) / \epsilon )$ .

# 2. Error Analysis

In this section we show that taking $t _ { 0 } = O ( \kappa / \epsilon )$ introduces an error of $\leq \epsilon$ in the final state. The main subtlety in analyzing the error comes from the post-selection step, in which we choose only the part of the state attached to the $| \mathrm { w e l l } \rangle$ register. This can potentially magnify errors in the overall state. On the other hand, we may also be interested in the non-postselected state, which results from applying $U _ { \mathrm { i n v e r t } }$ a single time to $| b \rangle$ . For instance, this could be used to estimate the amount of weight of $| b \rangle$ lying in the ill-conditioned components of $A$ . Somewhat surprisingly, we show that the error in both cases is upper-bounded by $O ( \kappa / t _ { 0 } )$ .

In this section, it will be convenient to ignore the error terms $\epsilon _ { H }$ and $\epsilon _ { \Psi }$ , as these can be made negligible with relatively little effort and it is the errors from phase estimation that will dominate. Let $\tilde { U }$ denote a version of $U _ { \mathrm { i n v e r t } }$ in which everything except the phase estimation is exact. Since $\lVert \tilde { U } - U _ { \mathrm { i n v e r t } } \rVert \leq O ( \epsilon _ { H } + \epsilon _ { \Psi } )$ , it is sufficient to work with $\ddot { U }$ . Define $U$ to be the ideal version of $U _ { \mathrm { i n v e r t } }$ in which there is no error in any step.

Theorem 1 (Error bounds).

1. In the case when no post-selection is performed, the error is bounded as

$$
\| \tilde { U } - U \| \leq O ( \kappa / t _ { 0 } ) .
$$

2. If we post-select on the flag register being in the space spanned by $\{ | w e l l \rangle , | i l l \rangle \}$ and define the normalized ideal state to be $| x \rangle$ and our actual state to be $| \tilde { x } \rangle$ then

$$
\| | \tilde { x } \rangle - | x \rangle \| \leq O ( \kappa / t _ { 0 } ) .
$$

3. If $| b \rangle$ is entirely within the well-conditioned subspace of $A$ and we post-select on the flag register being |welli then

$$
\| | \tilde { x } \rangle - | x \rangle \| \leq O ( \kappa / t _ { 0 } ) .
$$

The third claim is often of the most practical interest, but the other two are useful if we want to work with the ill-conditioned space, or estimate its weight.

The rest of the section is devoted to the proof of Theorem 1. We first show that the third claim is a corollary of the second, and then prove the first two claims more or less independently. To prove (A5 assuming (A4), observe that if $| b \rangle$ is entirely in the well-conditioned space, the ideal state $| x \rangle$ is proportional to $A ^ { - 1 } \left| b \right. \left| { \mathrm { w e l l } } \right.$ . Model the post-selection on $| \mathrm { w e l l } \rangle$ by a post-selection first on the space spanned by $\{ \vert \mathrm { w e l l } \rangle , \vert \mathrm { i l l } \rangle \}$ , followed by a post-selection onto $| \mathrm { w e l l } \rangle$ . By (A4), the first post-selection leaves us with error $O ( \kappa / t _ { 0 } )$ . This implies that the second post-selection will succeed with probability $\geq 1 - O ( \kappa ^ { 2 } / t _ { 0 } ^ { 2 } )$ and therefore will increase the error by at most $O ( \kappa / t _ { 0 } )$ . The final error is then $O ( \kappa / t _ { 0 } )$ as claimed in (A6).

Now we turn to the proof of (A4). A crucial piece of the proof will be the following statement about the continuity of $| h ( \lambda ) \rangle$ .

Lemma 2. The map $\lambda \mapsto | h ( \lambda ) \rangle$ is $O ( \kappa )$ -Lipschitz, meaning that for any $\lambda _ { 1 } \neq \lambda _ { 2 }$ ,

$$
\begin{array} { r } { \parallel \left| h ( \lambda _ { 1 } ) \right. - \left| h ( \lambda _ { 2 } ) \right. \parallel = \sqrt { 2 ( 1 - R e \left. h ( \lambda _ { 1 } ) \vert h ( \lambda _ { 2 } ) \right. ) } \le c \kappa \vert \lambda _ { 1 } - \lambda _ { 2 } \vert , } \end{array}
$$

for some $c = O ( 1 )$ .

Proof. Since $\lambda \mapsto | h ( \lambda ) \rangle$ is continuous everywhere and differentiable everywhere except at $1 / \kappa$ and $1 / \kappa ^ { \prime }$ , it suffices to bound the norm of the derivative of $| h ( \lambda ) \rangle$ . We consider it piece by piece. When $\lambda > 1 / \kappa$ ,

$$
\frac { d } { d \lambda } \left| h ( \lambda ) \right. = \frac { 1 } { 2 \kappa ^ { 2 } \lambda ^ { 3 } \sqrt { 1 - 1 / 2 \kappa ^ { 2 } \lambda ^ { 2 } } } \left| \mathrm { n o t h i n g } \right. - \frac { 1 } { 2 \kappa \lambda ^ { 2 } } \left| \mathrm { w e l l } \right. ,
$$

which has squared norm $\begin{array} { r } { \frac { 1 } { 2 \kappa ^ { 2 } \lambda ^ { 4 } ( 2 \kappa ^ { 2 } \lambda ^ { 2 } - 1 ) } + \frac { 1 } { 4 \kappa ^ { 2 } \lambda ^ { 4 } } \leq \kappa ^ { 2 } } \end{array}$ . Next, when $1 / \kappa ^ { \prime } < \lambda < 1 / \kappa$ , the norm of $\begin{array} { r } { \frac { d } { d \lambda } \left| h ( \lambda ) \right. } \end{array}$ is

$$
{ \frac { 1 } { 2 } } \cdot { \frac { \pi } { 2 } } \cdot { \frac { 1 } { { \frac { 1 } { \kappa } } - { \frac { 1 } { \kappa ^ { \prime } } } } } = { \frac { \pi } { 2 } } \kappa .
$$

Finally $\begin{array} { r } { \frac { d } { d \lambda } \left| h ( \lambda ) \right. = 0 } \end{array}$ when $\lambda < 1 / \kappa ^ { \prime }$ . This completes the proof, with $\begin{array} { r } { c = \frac { \pi } { 2 } } \end{array}$

Now we return to the proof of (A4). Let $\tilde { P }$ denote the first three steps of the algorithm. They can be thought of as mapping the initial zero qubits to a $| k \rangle$ register, together with some garbage, as follows:

$$
\tilde { P } = \sum _ { j = 1 } ^ { n } | u _ { j } \rangle \langle u _ { j } | \otimes \sum _ { k } \alpha _ { k | j } | k \rangle | \mathrm { g a r b a g e } ( j , k ) \rangle \langle \mathrm { i n i t i a l } | ,
$$

where the guarantee that the phase estimation algorithm gives us is that $\alpha _ { k | j }$ is concentrated around $\lambda _ { j } \approx 2 \pi k / t _ { 0 } = :$ $\ddot { \lambda } _ { k }$ . Technically, $\tilde { P }$ should be completed to make it a unitary operator by defining some arbitrary behavior on inputs other than |initiali in the last register.

Consider a test state $\begin{array} { r } { \left| b \right. = \sum _ { j = 1 } ^ { N } \beta _ { j } \left| u _ { j } \right. } \end{array}$ . The ideal functionality is defined by

$$
\left| \varphi \right. = U \left| { b } \right. = \sum _ { j = 1 } ^ { N } \beta _ { j } \left| { u _ { j } } \right. \left| { h ( \lambda _ { j } ) } \right. ,
$$

while the actual algorithm produces the state

$$
\left| { \tilde { \varphi } } \right. = { \tilde { U } } \left| { b } \right. = { \tilde { P } } ^ { \dagger } \sum _ { j = 1 } ^ { N } \beta _ { j } \left| { u _ { j } } \right. \sum _ { k } \alpha _ { k \left| { j } \right. } \left| { k } \right. \left| { h ( \tilde { \lambda } _ { k } ) } \right. ,
$$

We wish to calculate $\langle \tilde { \varphi } | \varphi \rangle$ , or equivalently the inner product between $\tilde { P } \left| \tilde { \varphi } \right.$ and $\begin{array} { r } { \mathinner { \tilde { P } \mathopen { | \varphi \rangle } } = \sum _ { j , k } \beta _ { j } \alpha _ { k | j } \left. u _ { j } \right. \left. k \right. \left. h ( \lambda _ { j } ) \right. } \end{array}$ . This inner product is

$$
\langle \tilde { \varphi } | \varphi \rangle = \sum _ { j = 1 } ^ { N } | \beta _ { j } | ^ { 2 } \sum _ { k } | \alpha _ { k | j } | ^ { 2 } \left. h ( \tilde { \lambda } _ { k } ) | h ( \lambda _ { j } ) \right. : = \mathbb { E } _ { j } \mathbb { E } _ { k } \left. h ( \tilde { \lambda } _ { k } ) | h ( \lambda _ { j } ) \right. ,
$$

where we think of $j$ and $k$ as random variables with joint distribution $\mathrm { P r } ( j , k ) = | \beta _ { j } | ^ { 2 } | \alpha _ { k | j } | ^ { 2 }$ . Thus

$$
\begin{array} { r } { \operatorname { R e } \left. \tilde { \varphi } | \varphi \right. = \mathbb { E } _ { j } \mathbb { E } _ { k } \operatorname { R e } \left. h ( \tilde { \lambda } _ { k } ) | h ( \lambda _ { j } ) \right. . } \end{array}
$$

Let $\delta = \lambda _ { j } t _ { 0 } - 2 \pi k = t _ { 0 } ( \lambda _ { j } - \ddot { \lambda } _ { k } )$ . From Lemma 2, $\mathrm { R e } \left. h ( \tilde { \lambda } _ { k } ) | h ( \lambda _ { j } ) \right. \geq 1 - c ^ { 2 } \kappa ^ { 2 } \delta ^ { 2 } / 2 t _ { 0 } ^ { 2 }$ , where $c \leq \frac { \pi } { 2 }$ is a constant. There are two sources of infidelity. For $\delta \leq 2 \pi$ , the inner product is at least $1 - 2 \pi ^ { 2 } c ^ { 2 } \kappa ^ { 2 } / t _ { 0 } ^ { 2 }$ . For larger values of $\delta$ , we use the bound $\lvert \alpha _ { k \lvert j } \rvert ^ { 2 } \leq 6 4 \pi ^ { 2 } / ( \lambda _ { j } t _ { 0 } - 2 \pi k ) ^ { 4 }$ (proved in Section A 3) to find an infidelity contribution that is

$$
\leq 2 \sum _ { k = \frac { \lambda _ { j } t _ { 0 } } { 2 \pi } + 1 } ^ { \infty } \frac { 6 4 \pi ^ { 2 } } { \delta ^ { 4 } } \frac { c ^ { 2 } \kappa ^ { 2 } \delta ^ { 2 } } { 2 t _ { 0 } ^ { 2 } } = \frac { 6 4 \pi ^ { 2 } c ^ { 2 } \kappa ^ { 2 } } { t _ { 0 } ^ { 2 } } \sum _ { k = 1 } ^ { \infty } \frac { 1 } { 4 \pi ^ { 2 } k ^ { 2 } } = \frac { 8 \pi ^ { 2 } c ^ { 2 } } { 3 } \cdot \frac { \kappa ^ { 2 } } { t _ { 0 } ^ { 2 } } .
$$

Summarizing, we find that $\mathrm { R e } \left. \tilde { \varphi } | \varphi \right. \geq 1 - 5 \pi ^ { 2 } c ^ { 2 } \kappa ^ { 2 } / t _ { 0 } ^ { 2 }$ , which translates into $\| ~ | \tilde { \varphi } \rangle - | \varphi \rangle ~ \| ~ \leq 4 \pi c \kappa / t _ { 0 } = 2 \pi ^ { 2 } \kappa / t _ { 0 }$ . Since the initial state $| b \rangle$ was arbitrary, this bounds the operator distance $\lVert \dot { U } - U \rVert$ as claimed in (A4). Turning now to the post-selected case, we observe that

$$
\begin{array} { r l } { | x \rangle : = \frac { f ( A ) | b \rangle | \mathrm { w e l l } \rangle + g ( A ) | b \rangle | \mathrm { i l l } \rangle } { \sqrt { \langle b | ( f ( A ) ^ { 2 } + g ( A ) ^ { 2 } ) | b \rangle } } } \\ { = } & { \frac { \sum _ { j } \beta _ { j } | u _ { j } \rangle ( f ( \lambda _ { j } ) | \mathrm { w e l l } \rangle + g ( \lambda _ { j } ) | \mathrm { i l l } \rangle ) } { \sqrt { \sum _ { j } | \beta _ { j } | ^ { 2 } ( f ( \lambda _ { j } ) ^ { 2 } + g ( \lambda _ { j } ) ^ { 2 } ) } } } \\ { = : \frac { \sum _ { j } \beta _ { j } | u _ { j } \rangle ( f ( \lambda _ { j } ) | \mathrm { w e l l } \rangle + g ( \lambda _ { j } ) | \mathrm { i l l } \rangle ) } { \sqrt { p } } . } \end{array}
$$

Where in the last step we have defined

$$
p : = \mathbb { E } _ { j } [ f ( \lambda _ { j } ) ^ { 2 } + g ( \lambda _ { j } ) ^ { 2 } ]
$$

to be the probability that the post-selection succeeds. Naively, this post-selection could magnify the errors by as much as $1 / { \sqrt { p } }$ , but by careful examination of the errors, we find that this worst-case situation only occurs when the

errors are small in the first place. This is what will allow us to obtain the same $O ( \kappa / t _ { 0 } )$ error bound even in the post-selected state.

Now write the actual state that we produce as

$$
\begin{array} { r l r } & { } & { \mathrm { ~ } } \\ & { \mathrm { ~ } = \mathrm { ~ } \frac { \tilde { P } ^ { \dagger } \sum _ { j = 1 } ^ { N } \beta _ { j } \left| u _ { j } \right. \sum _ { k } \alpha _ { k \left| j \right. } \left| k \right. \left( f \left( \tilde { \lambda } _ { k } \right) \left| \mathrm { w e l l } \right. + g \left( \tilde { \lambda } _ { k } \right) \left| \mathrm { i l l } \right. \right) } { \sqrt { \mathbb { E } _ { j , k } f ( \tilde { \lambda } _ { k } ) ^ { 2 } + g ( \tilde { \lambda } _ { k } ) ^ { 2 } } } } \\ & { } & { \mathrm { ~ } } \\ & { } & { \mathrm { ~ } = : \frac { \tilde { P } ^ { \dagger } \sum _ { j = 1 } ^ { N } \beta _ { j } \left| u _ { j } \right. \sum _ { k } \alpha _ { k \left| j \right. } \left| k \right. \left( f ( \tilde { \lambda } _ { k } ) \left| \mathrm { w e l l } \right. + g ( \tilde { \lambda } _ { k } ) \left| \mathrm { i l l } \right. \right) } { \sqrt { \tilde { p } } } , } \end{array}
$$

where we have defined $\tilde { p } = \mathbb { E } _ { j , k } [ f ( \bar { \lambda } _ { k } ) ^ { 2 } + g ( \bar { \lambda } _ { k } ) ^ { 2 } ]$ .

Recall that $j$ and $k$ are random variables with joint distribution $\mathrm { P r } ( j , k ) = | \beta _ { j } | ^ { 2 } | \alpha _ { k | j } | ^ { 2 }$ . We evaluate the contribution of a single $j$ value. Define $\lambda : = \lambda _ { j }$ and $\ddot { \lambda } : = 2 \pi k / t _ { 0 }$ . Note that $\delta = t _ { 0 } ( \lambda - \ddot { \lambda } )$ and that $\mathbb { E } \delta , \mathbb { E } \delta ^ { 2 } = O ( 1 )$ . Here $\delta$ depends implicitly on both $j$ and $k$ , and the above bounds on its expectations hold even when conditioning on an arbitrary value of $j$ . We further abbreviate $f : = f ( \lambda )$ , $\tilde { f } : = \tilde { f } ( \lambda )$ , $g : = g ( \lambda )$ and $\tilde { g } \ = \ \tilde { g } ( \lambda )$ . Thus $p : = \mathbb { E } [ f ^ { 2 } + g ^ { 2 } ]$ and $\tilde { p } = \mathbb { E } [ \bar { f } ^ { 2 } + \tilde { g } ^ { 2 } ]$ .

Our goal is to bound $\Vert | x  - | \tilde { x }  |$ in (A5). We work instead with the fidelity

$$
\begin{array} { r l } { F : = \langle \tilde { x } | x \rangle = \frac { \mathbb { E } [ f \tilde { f } + g \tilde { g } ] } { \sqrt { p \tilde { p } } } = \frac { \mathbb { E } [ f ^ { 2 } + g ^ { 2 } ] + \mathbb { E } [ ( \tilde { f } - f ) f + ( \tilde { g } - g ) g ] } { p \sqrt { 1 + \frac { \tilde { p } - p } { p } } } } & { } \\ { = } & { \frac { 1 + \frac { \mathbb { E } [ ( \tilde { f } - f ) f + ( \tilde { g } - g ) g ] } { p } } { \sqrt { 1 + \frac { \tilde { p } - p } { p } } } \geq \left( 1 + \frac { \mathbb { E } [ ( \tilde { f } - f ) f + ( \tilde { g } - g ) g ] } { p } \right) \left( 1 - \frac { 1 } { 2 } \cdot \frac { \tilde { p } - p } { p } \right) } \end{array}
$$

Next we expand

$$
\begin{array} { r l } & { \tilde { p } - p = \mathbb { E } [ \tilde { f } ^ { 2 } - f ^ { 2 } ] + \mathbb { E } [ \tilde { g } ^ { 2 } - g ^ { 2 } ] } \\ & { \qquad = \mathbb { E } [ ( \tilde { f } - f ) ( \tilde { f } + f ) ] + \mathbb { E } [ ( \tilde { g } - g ) ( \tilde { g } + g ) ] } \\ & { \qquad = 2 \mathbb { E } [ ( \tilde { f } - f ) f ] + 2 \mathbb { E } [ ( \tilde { g } - g ) g ] + \mathbb { E } [ ( \tilde { f } - f ) ^ { 2 } ] + \mathbb { E } [ ( \tilde { g } - g ) ^ { 2 } ] } \end{array}
$$

Substituting into (A13), we find

$$
F \geq 1 - \frac { \mathbb { E } [ ( \tilde { f } - f ) ^ { 2 } + ( \tilde { g } - g ) ^ { 2 } ] } { 2 p } - \frac { \mathbb { E } [ ( \tilde { f } - f ) f + ( \tilde { g } - g ) g ] } { p } \cdot \frac { \tilde { p } - p } { 2 p }
$$

We now need an analogue of the Lipschitz condition given in Lemma 2.

Lemma 3. Let $f , { \bar { f } } , g , { \tilde { g } }$ be defined as above, with $\kappa ^ { \prime } = 2 \kappa$ . Then

$$
| f - \tilde { f } | ^ { 2 } + | g - \tilde { g } | ^ { 2 } \leq c \frac { \kappa ^ { 2 } } { t _ { 0 } ^ { 2 } } \delta ^ { 2 } | f ^ { 2 } + g ^ { 2 } |
$$

where $c = \pi ^ { 2 } / 2$ .

Proof. Remember that $\tilde { f } = f ( \lambda - \delta / t _ { 0 } )$ and similarly for $\tilde { g }$ .

Consider the case first when $\lambda \geq 1 / \kappa$ . In this case $g = 0$ , and we need to show that

$$
| f - \tilde { f } | \le 2 \frac { \kappa | \delta f | } { t _ { 0 } } = \frac { | \lambda - \tilde { \lambda } | } { \lambda }
$$

To prove this, we consider four cases. First, suppose $\bar { \lambda } \geq 1 / \kappa$ . Then 12κ |λ˜−λ|˜ ≤ |δ|/2t0λ. Next, suppose $\lambda = 1 / \kappa$ (so $f = 1 / 2$ ) and $\tilde { \lambda } < 1 / \kappa$ . Since $\sin { \frac { \pi } { 2 } } \alpha \geq \alpha$ for $0 \leq \alpha \leq 1$ , we have

$$
| f - \tilde { f } | \le \frac 1 2 - \frac 1 2 \frac { \tilde { \lambda } - \frac 1 { \kappa ^ { \prime } } } { \frac 1 \kappa - \frac 1 { \kappa ^ { \prime } } } = \frac 1 2 - \kappa ( \tilde { \lambda } - \frac 1 2 ) = \kappa ( \frac 1 \kappa - \tilde { \lambda } ) ,
$$

and using $\lambda = 1 / \kappa$ we find that $\begin{array} { r } { | f - \tilde { f } | = \frac { \lambda - \bar { \lambda } } { \lambda } } \end{array}$ , as desired. Next, if $\tilde { \lambda } < 1 / \kappa < \lambda$ and $f < \tilde { f }$ then replacing $\lambda$ with $1 / \kappa$ only makes the inequality tighter. Finally, suppose $\tilde { \lambda } < 1 / \kappa < \lambda$ and $\ddot { f } < f$ . Using (A19) and $\lambda > 1 / \kappa$ we find that $f - { \bar { f } } \leq 1 - \kappa { \bar { \lambda } } < 1 - { \bar { \lambda } } / { \lambda } = ( \lambda - { \bar { \lambda } } ) / { \lambda }$ , as desired.

Now, suppose that $\lambda < 1 / \kappa$ . Then

$$
| f - \tilde { f } | ^ { 2 } \le \frac { \delta ^ { 2 } } { t _ { 0 } ^ { 2 } } \operatorname* { m a x } | f ^ { \prime } | ^ { 2 } = \frac { \pi ^ { 2 } } { 4 } \frac { \delta ^ { 2 } } { t _ { 0 } ^ { 2 } } \kappa ^ { 2 } .
$$

And similarly

$$
| g - \tilde { g } | ^ { 2 } \leq \frac { \delta ^ { 2 } } { t _ { 0 } ^ { 2 } } \operatorname* { m a x } | g ^ { \prime } | ^ { 2 } = \frac { \pi ^ { 2 } } { 4 } \frac { \delta ^ { 2 } } { t _ { 0 } ^ { 2 } } \kappa ^ { 2 } .
$$

Finally $f ( \lambda ) ^ { 2 } + g ( \lambda ) ^ { 2 } = 1 / 2$ for any $\lambda \le 1 / \kappa$ , implying the result.

Now we use Lemma 3 to bound the two error contributions in (A13). First bound

$$
\frac { \mathbb { E } [ ( \widetilde { f } - f ) ^ { 2 } + ( \widetilde { g } - g ) ^ { 2 } ] } { 2 p } \le O \left( \frac { \kappa ^ { 2 } } { t _ { 0 } ^ { 2 } } \right) \cdot \frac { \mathbb { E } [ ( f ^ { 2 } + g ^ { 2 } ) \delta ^ { 2 } ] } { \mathbb { E } [ f ^ { 2 } + g ^ { 2 } ] } \le O \left( \frac { \kappa ^ { 2 } } { t _ { 0 } ^ { 2 } } \right)
$$

The first inequality used Lemma 3 and the second used the fact that $\mathbb { E } [ \delta ^ { 2 } ] \le { \cal { O } } ( 1 )$ even when conditioned on an arbitrary value of $j$ (or equivalently $\lambda _ { j }$ ).

Next,

$$
\begin{array} { r } { \frac { \cdot f ) f + ( \widetilde g - g ) g \rfloor } { p } \leq \frac { \mathbb { E } \left[ \sqrt { \left( ( \widetilde f - f ) ^ { 2 } + ( \widetilde g - g ) ^ { 2 } \right) ( f ^ { 2 } + g ^ { 2 } ) } \right] } { p } \quad \leq \frac { \mathbb { E } \left[ \sqrt { \frac { \delta ^ { 2 } \kappa ^ { 2 } } { t _ { 0 } ^ { 2 } } ( f ^ { 2 } + g ^ { 2 } ) ^ { 2 } } \right] } { p } \leq O \left( \begin{array} { l l } { \frac { \delta ^ { 2 } } { t _ { 0 } ^ { 2 } } < \frac { 1 } { t _ { 0 } ^ { 2 } } < \frac { 1 } { t _ { 0 } ^ { 2 } } < \frac { 1 } { t _ { 0 } ^ { 2 } } } \end{array} \right) . } \end{array}
$$

where the first inequality is Cauchy-Schwartz, the second is Lemma 3 and the last uses the fact that $\begin{array} { r } { \mathbb { E } [ | \delta | ] \leq \sqrt { \mathbb { E } [ \delta ^ { 2 } ] } = } \end{array}$ $O ( 1 )$ even when conditioned on $j$ .

We now substitute (A20) and (A21) into (A16) (and assume $\kappa \leq t _ { 0 }$ ) to find

$$
\frac { \left| \tilde { p } - p \right| } { p } \leq O \left( \frac { \kappa } { t _ { 0 } } \right) .
$$

Substituting (A20), (A21) and (A22) into (A17), we find $\mathrm { R e } \left. \tilde { x } | x \right. \ge 1 - O ( \kappa ^ { 2 } / t _ { 0 } ^ { 2 } )$ , or equivalently, that $\| \left| \widetilde { x } \right. - | x \rangle \| \leq \epsilon$ .   
This completes the proof of Theorem 1.

# 3. Phase estimation calculations

Here we describe, in our notation, the improved phase-estimation procedure of [5, 7], and prove the concentration bounds on $| \alpha _ { k | j } |$ . Adjoin the state

$$
\left| \Psi _ { 0 } \right. = \sqrt { \frac { 2 } { T } } \sum _ { \tau = 0 } ^ { T - 1 } \sin \frac { \pi ( \tau + \frac { 1 } { 2 } ) } { T } \left| \tau \right. .
$$

Apply the conditional Hamiltonian evolution $\begin{array} { r } { \sum _ { \tau } | \tau \rangle \langle \tau | \otimes e ^ { i A \tau t _ { 0 } / T } } \end{array}$ . Assume the target state is $| u _ { j } \rangle$ , so this becomes simply the conditional phase $\begin{array} { r } { \sum _ { \tau } | \tau \rangle \langle \tau | e ^ { i \lambda _ { j } t _ { 0 } \tau / T } } \end{array}$ . The resulting state is

$$
\left| \Psi _ { \lambda _ { j } t _ { 0 } } \right. = \sqrt { \frac { 2 } { T } } \sum _ { \tau = 0 } ^ { T - 1 } e ^ { \frac { i \lambda _ { j } t _ { 0 } \tau } { T } } \sin \frac { \pi ( \tau + \frac { 1 } { 2 } ) } { T } \left| \tau \right. \left| u _ { j } \right. .
$$

We now measure in the Fourier basis, and find that the inner product with $\begin{array} { r } { { \frac { 1 } { \sqrt { T } } } \sum _ { \tau = 0 } ^ { T - 1 } e ^ { \frac { 2 \pi i k \tau } { T } } \left| \tau \right. \left| u _ { j } \right. } \end{array}$ is (defining $\begin{array} { r } { \delta : = \lambda _ { j } t _ { 0 } - 2 \pi k _ { \mathrm { , } } } \end{array}$ ):

$$
\begin{array} { r l } { \sigma _ { 4 , 4 } } & { = \frac { \sigma _ { 4 , 2 } ^ { 2 } } { \gamma \sigma _ { 5 } } \frac { \sigma _ { 4 , 3 } ^ { 2 } ( \sigma _ { 4 , 3 } - 2 \sigma _ { 4 , 4 } ) \sigma _ { 4 , 3 } ( \sigma _ { 4 , 3 } - \sigma _ { 4 , 4 } ) } { \gamma } } \\ & { = \frac { \sigma _ { 4 , 4 } } { \gamma \sigma _ { 5 } } \frac { \sigma _ { 4 , 3 } ^ { 2 } ( \sigma _ { 4 , 3 } - 2 \sigma _ { 4 , 4 } ) } { \gamma } } \\ & { = \frac { \sigma _ { 4 , 4 } } { \gamma \sigma _ { 5 } } \frac { \sigma _ { 4 , 3 } ^ { 2 } ( \sigma _ { 4 , 3 } - \sigma _ { 4 , 4 } ) } { \gamma } \frac { \sigma _ { 4 , 4 } ^ { 2 } ( \sigma _ { 4 , 3 } - \sigma _ { 4 , 4 } ) } { \gamma } } \\ & { = \frac { \sigma _ { 4 , 4 } } { \gamma \sigma _ { 5 } } \frac { \sigma _ { 4 , 3 } ^ { 2 } ( \sigma _ { 4 , 3 } - \sigma _ { 4 , 4 } ) } { \gamma } - \frac { \sigma _ { 4 , 4 } \sigma _ { 4 , 4 } \sigma _ { 5 } ^ { 2 } } { \gamma } } \\ & { = \frac { \sigma _ { 4 , 4 } } { \gamma \sigma _ { 5 } } ( \frac { \sigma _ { 4 , 1 } } { \sigma _ { 5 } } \frac { \sigma _ { 4 , 3 } ^ { 2 } ( \sigma _ { 4 , 3 } - \sigma _ { 4 , 4 } ) } { \gamma \sigma _ { 5 } } - \frac { \sigma _ { 4 , 4 } \sigma _ { 4 , 4 } } { 1 - \sigma _ { 5 } \sigma _ { 5 } } ) } \\ &  = \frac { \sigma _ { 4 , 4 } } { \gamma \sigma _ { 5 } } ( \frac { \sigma _ { 4 , 3 } } { \gamma \sigma _ { 5 } } \frac { \sigma _ { 4 , 3 } ^ { 2 } ( \sigma _ { 4 , 3 } - \sigma _ { 4 , 4 } ) } { \gamma } - \frac  \sigma _  4  \end{array}
$$

Following [5, 7], we make the assumption that $2 \pi \leq \delta \leq T / 1 0$ . Further using $\alpha - \alpha ^ { 3 } / 6 \leq \sin \alpha \leq \alpha$ and ignoring phases we find that

$$
| \alpha _ { k | j } | \le \frac { 4 \pi \sqrt { 2 } } { ( \delta ^ { 2 } - \pi ^ { 2 } ) ( 1 - \frac { \delta ^ { 2 } + \pi ^ { 2 } } { 3 T ^ { 2 } } ) } \le \frac { 8 \pi } { \delta ^ { 2 } } .
$$

Thus $| \alpha _ { k | j } | ^ { 2 } \leq 6 4 \pi ^ { 2 } / \delta ^ { 2 }$ whenever $| k - \lambda _ { j } t _ { 0 } / 2 \pi | \geq 1$ .

# 4. The non-Hermitian case

Suppose $A \in \mathbb { C } ^ { M \times N }$ with $M \leq N$ . Generically $A x = b$ is now underconstrained. Let the singular value decomposition of $A$ be

$$
A = \sum _ { j = 1 } ^ { M } \lambda _ { j } \left| u _ { j } \right. \left. v _ { j } \right| ,
$$

with $| u _ { j } \rangle \in \mathbb { C } ^ { M }$ , $| v _ { j } \rangle \in \mathbb { C } ^ { N }$ and $\lambda _ { 1 } \geq \cdots \lambda _ { M } \geq 0$ . Let $V = \operatorname { s p a n } \{ \left| v _ { 1 } \right. , \ldots , \left| v _ { M } \right. \}$ . Define

$$
H = \left( \begin{array} { l l } { { 0 } } & { { A } } \\ { { A ^ { \dagger } } } & { { 0 } } \end{array} \right) .
$$

is Hermitian with eigenvalues $\pm \lambda _ { 1 } , \ldots , \pm \lambda _ { M }$ , corresponding to eigenvectors $\begin{array} { r } { \left| w _ { j } ^ { \pm } \right. : = \frac { 1 } { \sqrt { 2 } } ( \left| 0 \right. \left| u _ { j } \right. \pm \left| 1 \right. \left| v _ { j } \right. ) } \end{array}$ . It also as $N - M$ zero eigenvalues, corresponding to the orthogonal complement of $V$ .

To run our algorithm we use the input $\left| 0 \right. \left| b \right.$ . If $\begin{array} { r } { \left| b \right. = \sum _ { j = 1 } ^ { M } \beta _ { j } \left| u _ { j } \right. } \end{array}$ then

$$
\left| 0 \right. \left| b \right. = \sum _ { j = 1 } ^ { M } \beta _ { j } \frac { 1 } { \sqrt { 2 } } ( \left| w _ { j } ^ { + } \right. + \left| w _ { j } ^ { - } \right. )
$$

and running the inversion algorithm yields a state proportional to

$$
H ^ { - 1 } \left| 0 \right. \left| b \right. = \sum _ { j = 1 } ^ { M } \beta _ { j } \lambda _ { j } ^ { - 1 } \frac { 1 } { \sqrt { 2 } } ( \left| w _ { j } ^ { + } \right. - \left| w _ { j } ^ { - } \right. ) = \sum _ { j = 1 } ^ { M } \beta _ { j } \lambda _ { j } ^ { - 1 } \left| 1 \right. \left| v _ { j } \right. .
$$

Dropping the inital $| 1 \rangle$ , this defines our solution $| x \rangle$ . Note that our algorithm does not produce any component in $V ^ { \perp }$ , although doing so would have also yielded valid solutions. In this sense, it could be said to be finding the $| x \rangle$ that minimizes $\langle x | x \rangle$ while solving $A \left| x \right. = \left| b \right.$ .

On the other hand, if $M \geq N$ then the problem is overconstrained. Let $U = \operatorname { s p a n } \{ \left| u _ { 1 } \right. , \ldots , \left| u _ { N } \right. \}$ . The equation $A \left| x \right. = \left| b \right.$ is satisfiable only if $\vert b \rangle \in U$ . In this case, applying $H$ to $\left| 0 \right. \left| b \right.$ will return a valid solution. But if $| b \rangle$ has some weight in $U ^ { \perp }$ , then $\left| 0 \right. \left| b \right.$ will have some weight in the zero eigenspace of $H$ , which will be flagged as ill-conditioned by our algorithm. We might choose to ignore this part, in which case the algorithm will return an $| x \rangle$ satisfying $\begin{array} { r } { A \left| x \right. = \sum _ { j = 1 } ^ { N } \left| u _ { j } \right. \left. u _ { j } \right| \left| b \right. } \end{array}$ .

# 5. Optimality

In this section, we explain in detail two important ways in which our algorithm is optimal up to polynomial factors. First, no classical algorithm can perform the same matrix inversion task; and second, our dependence on condition number and accuracy cannot be substantially improved.

We present two versions of our lower bounds; one based on complexity theory, and one based on oracles. We say that an algorithm solves matrix inversion if its input and output are

1. Input: An $O ( 1 )$ -sparse matrix $A$ specified either via an oracle or via a poly( $\log ( N ) ,$ )-time algorithm that returns the nonzero elements in a row.   
2. Output: A bit that equals one with probability $\langle x | M | x \rangle \pm \epsilon$ , where $M = | 0 \rangle \langle 0 | \otimes I _ { N / 2 }$ corresponds to measuring the first qubit and $| x \rangle$ is a normalized state proportional to $A ^ { - 1 } \left| b \right.$ for $\left| b \right. = \left| 0 \right.$ .

Further we demand that $A$ is Hermitian and $\kappa ^ { - 1 } I \leq A \leq I$ . We take $\epsilon$ to be a fixed constant, such as $1 / 1 0 0$ , and later deal with the dependency in $\epsilon$ . If the algorithm works when $A$ is specified by an oracle, we say that it is relativizing. Even though this is a very weak definition of inverting matrices, this task is still hard for classical computers.

Theorem 4. 1. If a quantum algorithm exists for matrix inversion running in time $\kappa ^ { 1 - \partial }$ · poly $\log ( N )$ for some $\delta > 0$ , then BQP=PSPACE.

2. No relativizing quantum algorithm can run in time $\kappa ^ { 1 - \delta } \cdot \mathrm { p o l y l o g } ( N )$ .

3. If a classical algorithm exists for matrix inversion running in time poly $( \kappa , \log ( N ) )$ , then BPP=BQP.

Given an $n$ -qubit $T$ -gate quantum computation, define $U$ as in (4). Define

$$
\begin{array}{c} A = { \binom { 0 } { I - U ^ { \dagger } e ^ { - { \frac { 1 } { T } } } } } \quad I - U e ^ { - { \frac { 1 } { T } } }  \\ { 0 } \end{array} ) .
$$

Note that $A$ is Hermitian, has condition number $\kappa \leq 2 T$ and dimension $N = 6 T 2 ^ { n }$ . Solving the matrix inversion problem corresponding to $A$ produces an $\epsilon$ -approximation of the quantum computation corresponding to applying $U _ { 1 } , \dots , U _ { T }$ , assuming we are allowed to make any two outcome measurement on the output state $| x \rangle$ . Recall that

$$
\left( I - U e ^ { - \frac { 1 } { T } } \right) ^ { - 1 } = \sum _ { k \geq 0 } U ^ { k } e ^ { - k / T } .
$$

We define a measurement $M _ { 0 }$ , which outputs zero if the time register $t$ is between $T + 1$ and $2 T$ , and the original measurement’s output was one. As $\operatorname* { P r } ( T + 1 \le k \le 2 T ) = e ^ { - 2 } / ( 1 + e ^ { - 2 } + e ^ { - 4 } )$ and is independent of the result of the measurement $M$ , we can estimate the expectation of $M$ with accuracy $\epsilon$ by iterating this procedure $O \left( 1 / \epsilon ^ { 2 } \right)$ times. In order to perform the simulation when measuring only the first qubit, define

$$
B = \binom { I _ { 6 T 2 ^ { n } } } { 0 } \ I _ { 3 T 2 ^ { n } } - U e ^ { - \frac { 1 } { T } } \Big ) .
$$

We now define $\tilde { B }$ to be the matrix $B$ , after we permuted the rows and columns such that if

$$
C = \left( \begin{array} { l l } { { 0 } } & { { \tilde { B } } } \\ { { \tilde { B } ^ { \dagger } } } & { { 0 } } \end{array} \right) .
$$

and $C \vec { y } = \binom { \vec { b } } { 0 }$ , then measuring the first qubit of $| y \rangle$ would correspond to perform $M _ { 0 }$ on $| x \rangle$ . The condition number of $C$ is equal to that of $A$ , but the dimension is now $N = 1 8 T 2 ^ { n }$ .

Now suppose we could solve matrix inversion in time $\kappa ^ { 1 - \delta } ( \log ( N ) / \epsilon ) ^ { c _ { 1 } }$ for constants $c _ { 1 } \geq 2 , \delta > 0$ . Given a computation with $T \leq 2 ^ { 2 n } / 1 8$ , let $\begin{array} { r } { m = \frac { 2 } { \delta } \frac { \log ( 2 n ) } { \log ( \log ( n ) ) } } \end{array}$ and $ \epsilon = 1 / 1 0 0 m$ . For sufficiently large $n$ , $\epsilon \geq 1 / \log ( n )$ . Then

$$
\kappa ^ { 1 - \delta } \left( \frac { \log ( N ) } { \epsilon } \right) ^ { c _ { 1 } } \leq ( 2 T ) ^ { 1 - \delta } \left( \frac { 3 n } { \epsilon } \right) ^ { c _ { 1 } } \leq T ^ { 1 - \delta } c _ { 2 } ( n \log ( n ) ) ^ { c _ { 1 } } ,
$$

where $c _ { 2 } = 2 ^ { 1 - \delta } 3 ^ { c _ { 1 } }$ is another constant.

We now have a recipe for simulating an $n _ { i }$ -qubit $T _ { i }$ -gate computation with $n _ { i + 1 } = n _ { i } + \log ( 1 8 T _ { i } )$ qubits, $T _ { i + 1 } =$ $T _ { i } ^ { 1 - \delta } c _ { 3 } ( n _ { i } \log ( n _ { i } ) ) ^ { c _ { 1 } }$ gates and error $\epsilon$ . Our strategy is to start with an $n _ { 0 }$ -qubit $T _ { 0 }$ -gate computation and iterate this either after simulation $\ell \leq m$ $m$ ≤ steps, or whenever times, ending with an $T _ { i + 1 } > T _ { i } ^ { 1 - \delta / 2 }$ $n \ell$ -qubit ≤ ≤, whichever comes first. In the latter case, we set -gate computation with error $\leq m \epsilon \leq 1 / 1 0 0$ . We stop iterating $\ell$ equal to the first i for which Ti+1 > T 1−δ/2i .

On the other hand, suppose we stop for some In the case where we iterated the reduction $m$ $\ell < m$ times, we have . For each $T _ { i } \le T ^ { ( 1 - \delta / 2 ) ^ { 2 } } \le 2 ^ { ( 1 - \delta / 2 ) ^ { 2 } 2 n _ { 0 } }$ $i < \ell$ we have $T _ { i + 1 } \leq T _ { i } ^ { 1 - \delta / 2 }$ , implying that . Thus $T _ { i } \le 2 ^ { ( 1 - \delta / 2 ) ^ { \iota } 2 n _ { 0 } }$ $T _ { m } \leq n _ { 0 }$ for each $i ~ \leq ~ \ell$ . This allows us to bound $\begin{array} { r } { n _ { i } \ = \ n _ { 0 } + \sum _ { j = 0 } ^ { i - 1 } \log ( 1 8 T _ { i } ) \ = \ n _ { 0 } + 2 n _ { 0 } \sum _ { j = 0 } ^ { i - 1 } ( 1 - \delta / 2 ) ^ { j } + i \log ( 1 8 ) \ \leq \quad } \end{array}$ $\begin{array} { r } { \left( \frac { 4 } { \delta } + 1 \right) n _ { 0 } + m \log ( 1 8 ) } \end{array}$ . Defining yet another constant, this implies that $T _ { i + 1 } \leq T _ { i } ^ { 1 - \delta } c _ { 3 } ( n _ { 0 } \log ( n _ { 0 } ) ) ^ { c _ { 1 } }$ . Combining this with our stopping condition $T _ { \ell + 1 } > T _ { \ell } ^ { 1 - \delta / 2 }$ we find that

$$
T _ { \ell } \le ( c _ { 3 } ( n _ { 0 } \log ( n _ { 0 } ) ) ^ { c _ { 1 } } ) ^ { \frac { 2 } { \delta } } = \mathrm { p o l y } ( n _ { 0 } ) .
$$

Therefore, the runtime of the procedure is polynomial in $n _ { 0 }$ regardless of the reason we stopped iterating the procedure The number of qubits used increases only linearly.

Recall that the TQBF (totally quantified Boolean formula satisfiability) problem is PSPACE-complete, meaning that any $k$ -bit problem instance for any language in PSPACE can be reduced to a TQBF problem of length $n =$ $\mathrm { p o l y } ( k )$ (see [23] for more information). The formula can be solved in time $T \leq 2 ^ { 2 n } / 1 8$ , by exhaustive enumeration over the variables. Thus a PSPACE computation can be solved in quantum polynomial time. This proves the first part of the theorem.

To incorporate oracles, note that our construction of $U$ in (4) could simply replace some of the $U _ { i }$ ’s with oracle queries. This preserves sparsity, although we need the rows of $A$ to now be specified by oracle queries. We can now iterate the speedup in exactly the same manner. However, we conclude with the ability to solve the OR problem on $2 ^ { n }$ inputs in $\mathrm { p o l y } ( n )$ time and queries. This, of course, is impossible [24], and so the purported relativizing quantum algorithm must also be impossible.

The proof of part 3 of Theorem 4 simply formulates a poly $( n )$ -time, $n$ -qubit quantum computation as a $\kappa = \mathrm { p o l y } ( n )$ , $N = 2 ^ { n } \cdot \mathrm { p o l y } ( n )$ matrix inversion problem and applies the classical algorithm which we have assumed exists. □

Theorem 4 established the universality of the matrix inversion algorithm. To extend the simulation to problems which are not decision problems, note that the algorithm actually supplies us with $| x \rangle$ (up to some accuracy). For example, instead of measuring an observable $M$ , we can measure $| x \rangle$ in the computational basis, obtaining the result $i$ with probability $\mid \langle i | x \rangle \mid ^ { 2 }$ . This gives a way to simulate quantum computation by classical matrix inversion algorithms. In turn, this can be used to prove lower bounds on classical matrix inversion algorithms, where we assume that the classical algorithms output samples according to this distribution.

Theorem 5. No relativizing classical matrix inversion algorithm can run in time $N ^ { \alpha } 2 ^ { \beta \kappa }$ unless $3 \alpha + 4 \beta \geq 1 / 2$ .

If we consider matrix inversion algorithms that work only on positive definite matrices, then the $N ^ { \alpha } 2 ^ { \beta \kappa }$ bound becomes N α2β√κ.

Proof. Recall Simon’s problem [17], in which we are given $f : \mathbb { Z } _ { 2 } ^ { n } \to \{ 0 , 1 \} ^ { 2 n }$ such that $f ( x ) = f ( y )$ iff $x + y = a$ for some $a \in \mathbb { Z } _ { 2 } ^ { n }$ that we would like to find. It can be solved by running a $3 n$ -qubit $2 n + 1$ -gate quantum computation $O ( n )$ times and performing a $\mathrm { p o l y } ( n )$ classical computation. The randomized classical lower bound is $\Omega ( 2 ^ { n / 2 } )$ from birthday arguments.

Converting Simon’s algorithm to a matrix $A$ yields $\kappa \approx 4 n$ and $N \approx 3 6 n 2 ^ { 3 n }$ . The run-time is $N ^ { \alpha } 2 ^ { \beta \kappa } \approx 2 ^ { ( 3 \alpha + 4 \beta ) n }$ $\mathrm { p o l y } ( n )$ . To avoid violating the oracle lower bound, we must have $3 \alpha + 4 \beta \geq 1 / 2$ , as required. □

Next, we argue that the accuracy of algorithm cannot be substantially improved. Returning now to the problem of estimating $\langle x \vert M \vert x \rangle$ , we recall that classical algorithms can approximate this to accuracy $\epsilon$ in time $O ( N \kappa \mathrm { p o l y } ( \log ( 1 / \epsilon ) ) )$ . This $\mathrm { p o l y } ( \log ( 1 / \epsilon ) )$ dependence is because when writing the vectors $| b \rangle$ and $| x \rangle$ as bit strings means that adding an additional bit will double the accuracy. However, sampling-based algorithms such as ours cannot hope for a better than $\mathrm { p o l y } ( 1 / \epsilon )$ dependence of the run-time on the error. Thus proving that our algorithm’s error performance cannot be improved will require a slight redefinition of the problem.

Define the matrix inversion estimation problem as follows. Given $A , b , M , \epsilon , \kappa , s$ with $\| A \| \leq 1 , \| A ^ { - 1 } \| \leq \kappa$ , $A$ ssparse and efficiently row-computable, $| b \rangle = | 0 \rangle$ and $M = | 0 \rangle \langle 0 | \otimes I _ { N / 2 }$ : output a number that is within $\epsilon$ of $\langle x \vert M \vert x \rangle$ with probability $\geq 2 / 3$ , where $| x \rangle$ is the unit vector proportional to $A ^ { - 1 } \left| b \right.$ .

The algorithm presented in our paper can be used to solve this problem with a small amount of overhead. By producing $| x \rangle$ up to trace distance $\epsilon / 2$ in time $\tilde { O } ( \log ( N ) \kappa ^ { 2 } s ^ { 2 } / \epsilon )$ , we can obtain a sample of a bit which equals one with probability $\mu$ with $| \mu - \left. x \right| M \left| x \right. | \leq \epsilon / 2$ . Since the variance of this bit is $\leq 1 / 4$ , taking $1 / 3 \epsilon ^ { 2 }$ samples gives us a $\geq 2 / 3$ probability of obtaining an estimate within $\epsilon / 2$ of $\mu$ . Thus quantum computers can solve the matrix inversion estimation problem in time $\tilde { O } ( \log ( N ) \kappa ^ { 2 } s ^ { 2 } / \epsilon ^ { 3 } )$ .

We can now show that the error dependence of our algorithm cannot be substantially improved.

Theorem 6. 1. If a quantum algorithm exists for the matrix inversion estimation problem running in time poly $( \kappa , \log ( N ) , \log ( 1 / \epsilon ) )$ then BQP=PP.

2. No relativizing quantum algorithm for the matrix inversion estimation problem can run in time $N ^ { \alpha } \mathrm { p o l y } ( \kappa ) / \epsilon ^ { \beta }$ unless $\alpha + \beta \geq 1$ .

Proof. 1. A complete problem for the class $\mathbf { P P }$ is to count the number of satisfying assignments to a SAT formula. Given such formula $\phi$ , a quantum circuit can apply it on a superposition of all $2 ^ { n }$ assignments for variables, generating the state

$$
\sum _ { z _ { 1 } , \ldots , z _ { n } \in \{ 0 , 1 \} } \left. z _ { 1 } , \ldots , z _ { n } \right. \left. \phi ( z _ { 1 } , \ldots z _ { n } ) \right. .
$$

The probability of obtaining 1 when measuring the last qubit is equal to the number of satisfying truth assignments divided by $2 ^ { n }$ . A matrix inversion estimation procedure which runs in time poly $\log ( 1 / \epsilon )$ would enable us to estimate this probability to accuracy $2 ^ { - 2 n }$ in time $\mathrm { p o l y } ( \log ( 2 ^ { 2 n } ) ) = \mathrm { p o l y } ( n )$ . This would imply that BQP $\mathbf { \Phi } = \mathbf { P } \mathbf { P }$ as required.

2. Now assume that $\phi ( z )$ is provided by the output of an oracle. Let $C$ denote the number of $z \in \{ 0 , 1 \} ^ { n }$ such that $\phi ( z ) = 1$ . From [18], we know that determining the parity of $C$ requires $\Omega ( 2 ^ { n } )$ queries to $\phi$ . However, exactly determining $C$ reduces to the matrix inversion estimation problem with $N = 2 ^ { n }$ , $\kappa = O ( n ^ { 2 } )$ and $\epsilon = 2 ^ { - n - 2 }$ . By assumption we can solve this in time $2 ^ { ( \alpha + \beta ) n } \cdot \mathrm { p o l y } ( n )$ , implying that $\alpha + \beta \geq 1$ .

[10] P. Valiant. Testing symmetric properties of distributions. In Proceedings of the 40th Annual ACM Symposium on Theory of computing (STOC), pages 383–392. ACM Press New York, NY, USA, 2008.   
[11] A. Klappenecker and M. Rotteler. Quantum Physics Title: Engineering Functional Quantum Algorithms. Phys. Rev. A, 67:010302(R), 2003.   
[12] S. K. Leyton and T. J. Osborne. A quantum algorithm to solve nonlinear differential equations, 2008. arXiv:0812.4423.   
[13] A.W. Harrow, A. Hassidim, and S. Lloyd. Quantum algorithm for solving linear systems of equations, 2008. arXiv:0811.3171.   
[14] L. Grover and T. Rudolph. Creating superpositions that correspond to efficiently integrable probability distributions. arXiv:quant-ph/0208112.   
[15] G. Brassard, P. Høyer, M. Mosca, and A. Tapp. Quantum Amplitude Amplification and Estimation, volume 305 of Contemporary Mathematics Series Millenium Volume. AMS, New York, 2002. arXiv:quant-ph/0005055.   
[16] Jonathan R. Shewchuk. An Introduction to the Conjugate Gradient Method Without the Agonizing Pain. Technical Report CMU-CS-94-125, School of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania, March 1994.   
[17] Daniel R. Simon. On the power of quantum computation. SIAM J. Comp., 26:1474–1483, 1997.   
[18] E. Farhi, J. Goldstone, S. Gutmann, and M. Sipser. A limit on the speed of quantum computation in determining parity. Phys. Rev. Lett., 81:5442–5444, 1998. arXiv:quant-ph/9802045.   
[19] K. Chen. Matrix preconditioning techniques and applications. Cambridge Univ. Press, Cambridge, U.K., 2005.   
[20] L. Sheridan, D. Maslov, and M. Mosca. Approximating fractional time quantum evolution, 2009. arXiv:0810.3843.   
[21] D. Aharonov and A. Ta-Shma. Adiabatic quantum state generation and statistical zero knowledge. In Proceedings of the 35th Annual ACM Symposium on Theory of computing (STOC), pages 20–29. ACM Press New York, NY, USA, 2003. arXiv:quant-ph/0301023.   
[22] P.C. Hansen. Rank-deficient and discrete ill-posted problems: Numerical aspects of linear inversion. SIAM, Philadelphia, PA, 1998.   
[23] M. Sipser. Introduction to the Theory of Computation. International Thomson Publishing, 1996.   
[24] C.H. Bennett, E. Bernstein, G. Brassard, and U. Vazirani. The strengths and weaknesses of quantum computation. SIAM Journal on Computing, 26:1510–1523, 1997.