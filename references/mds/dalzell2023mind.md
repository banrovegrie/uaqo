# Multi-Concept Customization of Text-to-Image Diffusion

Nupur Kumari1 Bingliang Zhang2 Richard Zhang3 Eli Shechtman3 Jun-Yan Zhu1 1Carnegie Mellon University 2Tsinghua University 3Adobe Research

![](images/1956a68ce158a0dc87813102bf2219cc3d444e46a86ab76ea021d23829c94c94.jpg)  
Figure 1. Given a few images of a new concept, our method augments a pre-trained text-to-image diffusion model, enabling new generations of the concept in unseen contexts. Example concepts include personal objects, animals, e.g., a pet dog, and classes not well generated by the model, e.g., moongate (a circular gate [83]). Furthermore, we propose a method for composing multiple new concepts together, for example, $\boldsymbol { \nabla } ^ { * }$ dog wearing sunglasses in front of a moongate. We denote personal categories with a new modifier token $\boldsymbol { \nabla } ^ { * }$ .

# Abstract

# 1. Introduction

While generative models produce high-quality images of concepts learned from a large-scale database, a user often wishes to synthesize instantiations of their own concepts (for example, their family, pets, or items). Can we teach a model to quickly acquire a new concept, given a few examples? Furthermore, can we compose multiple new concepts together? We propose Custom Diffusion, an efficient method for augmenting existing text-to-image models. We find that only optimizing a few parameters in the text-to-image conditioning mechanism is sufficiently powerful to represent new concepts while enabling fast tuning ( $\sim 6$ minutes). Additionally, we can jointly train for multiple concepts or combine multiple fine-tuned models into one via closed-form constrained optimization. Our fine-tuned model generates variations of multiple new concepts and seamlessly composes them with existing concepts in novel settings. Our method outperforms or performs on par with several baselines and concurrent works in both qualitative and quantitative evaluations while being memory and computationally efficient.

Recently released text-to-image models [59, 63, 66, 86] have represented a watershed year in image generation. By simply querying a text prompt, users are able to generate images of unprecedented quality. Such systems can generate a wide variety of objects, styles, and scenes – seemingly “anything and everything”.

However, despite the diverse, general capability of such models, users often wish to synthesize specific concepts from their own personal lives. For example, loved ones such as family, friends, pets, or personal objects and places, such as a new sofa or a recently visited garden, make for intriguing concepts. As these concepts are by nature personal, they are unseen during large-scale model training. Describing these concepts after the fact, through text, is unwieldy and unable to produce the personal concept with sufficient fidelity.

This motivates a need for model customization. Given the few user-provided images, can we augment existing text-toimage diffusion models with the new concept (for example, their pet dog or a “moongate” as shown in Figure 1)? The fine-tuned model should be able to generalize and compose them with existing concepts to generate new variations. This poses a few challenges – first, the model tends to forget [15, 40, 58] or change [39, 46] the meanings of existing concepts: e.g., the meaning of “moon” being lost when adding the “moongate” concept. Secondly, the model is prone to overfit the few training samples and reduce sampling variations.

Moreover, we study a more challenging problem, compositional fine-tuning – the ability to extend beyond tuning for a single, individual concept and compose multiple concepts together, e.g., pet dog in front of moongate (Figure 1). Improving compositional generation has been studied in recent works [45]. But composing multiple new concepts poses additional challenges, such as mixing unseen concepts.

In this work, we propose a fine-tuning technique, Custom Diffusion for text-to-image diffusion models. Our method is computationally and memory efficient. To overcome the above-mentioned challenges, we identify a small subset of model weights, namely the key and value mapping from text to latent features in the cross-attention layers [6, 76]. Fine-tuning these is sufficient to update the model with the new concept. To prevent model forgetting, we use a small set of real images with similar captions as the target images. We also introduce augmentation during fine-tuning, which leads to faster convergence and improved results. To inject multiple concepts, our method supports training on both simultaneously or training them separately and then merging.

We build our method on Stable Diffusion [2] and experiment on various datasets with as few as four training images. For adding single concepts, our method shows better text alignment and visual similarity to the target images than concurrent works and baselines. More importantly, our method can compose multiple new concepts efficiently, whereas concurrent methods struggle and often omit one. Finally, our method only requires storing a small subset of parameters $3 \%$ of the model weights) and reduces the fine-tuning time (6 minutes on 2 A100 GPUs, $2 - 4 \times$ faster compared to concurrent works). Please find the code and data at our website.

# 2. Related Work

Deep generative models Generative models aim to synthesize samples from a data distribution, given a set of training examples. These include GANs [10,24,34], VAEs [36], autoregressive [18, 61, 75], flow-based [16, 17], and diffusion models [14, 29, 71]. To improve controllability, these models can be conditioned on a class [10, 68], image [32, 48, 80, 93], or text prompt [50, 73, 94]. Our work mainly relates to textconditioned synthesis [47]. While earlier works [31, 62, 73, 85, 87, 94] were limited to a few classes [43, 77], recent text-to-image models [14, 50, 59, 60, 63, 66, 86], trained on extremely large-scale data, have demonstrated remarkable generalization ability. However, such models are by nature generalists and struggle to generate specific instances like personal toys or rare categories, e.g., “moongate”. We aim to adapt these models to become specialists in new concepts.

Image and model editing. While generative models can sample random images, a user often wishes to edit a single, specific image. Several works aim at leveraging the capabilities of generative models, such as GANs [3–5, 55, 92] or diffusion models [12, 35, 50] towards editing. A challenge is representing the specific image in the pre-trained model, and such methods employ per-image or per-edit optimization. A closely related line of work edits a generative model directly [7, 22, 78]. Whereas these methods aim to customize GANs, our focus is on text-to-image models.

Transfer learning. A method of efficiently producing a whole distribution of images is leveraging a pretrained model and then using transfer learning [22, 25, 41, 44, 49, 51–53, 81, 82, 89]. For example, one can adapt photorealistic faces into cartoons [22, 41, 49, 52, 53]. To adapt with just a few training images, efficient training techniques [33, 38, 67, 74, 90, 91] are often useful. Different from these works, which focus on tuning whole models to single domains, we wish to acquire multiple new concepts without catastrophic forgetting [20, 37, 40, 42, 58]. By preserving the millions of concepts captured in large-scale pretraining, we can synthesize the new concepts in composition with these existing concepts. Relatedly, several methods [23, 30, 56, 70] propose to train adapter modules or low rank updates for large-scale models in the discriminative setting. In contrast, we adapt a small number of existing parameters and do not require additional parameters.

Adapting text-to-image models. Similar to our goals, two concurrent works, DreamBooth [65] and Textual Inversion [21], adopt transfer learning to text-to-image diffusion models [63, 66] via either fine-tuning all the parameters [65] or introducing and optimizing a word vector [21] for the new concept. Our work differs in several aspects. First, our work tackles a challenging setting: compositional fine-tuning of multiple concepts, where concurrent works struggle. Second, we only fine-tune a subset of cross-attention layer parameters, which significantly reduces the fine-tuning time. We find these design choices lead to better results, validated by automatic metrics and human preference studies.

# 3. Method

Our proposed method for model fine-tuning, as shown in Figure 2, only updates a small subset of weights in the cross-attention layers of the model. In addition, we use a regularization set of real images to prevent overfitting on the few training samples of the target concepts. In this section, we explain our design choices and final algorithm in detail.

# 3.1. Single-Concept Fine-tuning

Given a pretrained text-to-image diffusion model, we aim to embed a new concept in the model given as few as four images and the corresponding text description. The finetuned model should retain its prior knowledge, allowing for novel generations with the new concept, based on the text prompt. This can be challenging as the updated text-to-image mapping might easily overfit the few available images.

![](images/7a36486f50a488a42d0404e3ebb0019d943d55faa22eb3c4127f7243f960c81f.jpg)  
Figure 2. Custom Diffusion. Given images of new concepts, we retrieve real images with similar captions as the given concepts and create the training dataset for fine-tuning, as shown on the left. To represent personal concepts of a general category, we introduce a new modifier token $\boldsymbol { \nabla } ^ { * }$ , used in front of the category name. During training, we optimize key and value projection matrices in the diffusion model cross-attention layers along with the modifier token. The retrieved real images are used as a regularization dataset during fine-tuning.

In our experiments, we use Stable Diffusion [2] as our backbone model, which is built on the Latent Diffusion Model (LDM) [63]. LDM first encodes images into a latent representation, using hybrid objectives of VAE [36], Patch-GAN [32], and LPIPS [88], such that running an encoderdecoder can recover an input image. They then train a diffusion model [29] on the latent representation with text condition injected in the model using cross-attention.

Learning objective of diffusion models. Diffusion models [29, 71] are a class of generative models that aim to approximate the original data distribution $q ( \mathbf { x } _ { 0 } )$ with $p _ { \theta } ( \mathbf { x } _ { 0 } )$ :

$$
p _ { \theta } ( \mathbf { x } _ { 0 } ) = \int \left[ p _ { \theta } ( \mathbf { x } _ { T } ) \prod p _ { \theta } ^ { t } ( \mathbf { x } _ { t - 1 } \vert \mathbf { x } _ { t } ) \right] d \mathbf { x } _ { 1 : T } ,
$$

where $\mathbf { x } _ { 1 }$ to $\mathbf { x } _ { T }$ are latent variables of a forward Markov chain s.t. $\mathbf { x } _ { t } = \sqrt { \alpha _ { t } } \mathbf { x } _ { 0 } + \sqrt { 1 - \alpha _ { t } } \epsilon$ . The model is trained to learn the reverse process of a fixed-length (usually 1000) Markov chain. Given noisy image $\mathbf { x } _ { t }$ at timestep $t$ , the model learns to denoise the input image to obtain $\mathbf { x } _ { t - 1 }$ . The training objective of the diffusion model can be simplified to:

$$
\begin{array} { r } { \mathbb { E } _ { \epsilon , \mathbf { x } , \mathbf { c } , t } [ w _ { t } | | \epsilon - \epsilon _ { \theta } ( \mathbf { x } _ { t } , \mathbf { c } , t ) | | ] , } \end{array}
$$

where $\epsilon _ { \theta }$ is the model prediction and $w _ { t }$ is a time-dependent weight on the loss. The model is conditioned on timestep $t$ and can be further conditioned on any other modality c, e.g., text. During inference, a random Gaussian image (or latent) $\mathbf { x } _ { T }$ is denoised for fixed timesteps using the model [72].

A na¨ıve baseline for the goal of fine-tuning is to update all layers to minimize the loss in Eqn. 2 for the given text-image pairs. This can be computationally inefficient for large-scale models and can easily lead to overfitting when training on a few images. Therefore, we aim to identify a minimal set of weights that is sufficient for the task of fine-tuning.

![](images/eba4a55b5ae703d319230b7a994f2efeae607d885823b89f09d3ec622712019e.jpg)  
Figure 3. Analysis of change in weights on updating all network weights during fine-tuning. The mean change in the cross-attention layers is significantly higher than other layers even though they only make up $5 \%$ of the total parameter count.

Rate of change of weights. Following Li et al. [41], we analyze the change in parameters for each layer in the finetuned model on the target dataset with the loss in Eqn. 2, i.e., $\Delta _ { l } = | | \theta _ { l } ^ { \prime } - \theta _ { l } | | / | | \theta _ { l } | |$ , where $\theta _ { l } ^ { \prime }$ and $\theta _ { l }$ are the updated and pretrained model parameters of layer $l$ . These parameters come from three types of layers – (1) cross-attention (between the text and image), (2) self-attention (within the image itself), and (3) the rest of the parameters, including convolutional blocks and normalization layers in the diffusion model U-Net. Figure 3 shows the mean $\Delta _ { l }$ for the three categories when the model is fine-tuned on “moongate” images. We observe similar plots for other datasets. As we see, the cross-attention layer parameters have relatively higher $\Delta$ compared to the rest of the parameters. Moreover, crossattention layers are only $5 \%$ of the total parameter count in the model. This suggests it plays a significant role during fine-tuning, and we leverage that in our method.

Model fine-tuning. Cross-attention block modifies the latent features of the network according to the condition features, i.e., text features in the case of text-to-image diffusion models. Given text features $\mathbf { c } \in \mathbb { R } ^ { s \times d }$ and latent image features $\mathbf { f } \in \mathbb { R } ^ { ( h \times w ) \times l }$ , a single-head cross-attention [76] operation consists of $Q = W ^ { q } \mathbf { f } , K = W ^ { k } \mathbf { c } , V = W ^ { v } \mathbf { c }$ , and a weighted sum over value features as:

$$
{ \mathrm { A t t e n t i o n } } ( Q , K , V ) = { \mathrm { S o f t m a x } } \Big ( \frac { Q K ^ { T } } { \sqrt { d ^ { \prime } } } \Big ) V ,
$$

![](images/b33efd8b29c30920cbd7a4efeb3617c41ba61e20a0e4a86cdff8a4b6dfe3458d.jpg)  
Figure 4. Single-head Cross-Attention. Latent image feature f and text feature c are projected into query $Q$ , key $K$ , and value $V$ . Output is a weighted sum of values, weighted by the similarity between the query and key features. We highlight the updated parameters $W ^ { k }$ and $W ^ { v }$ in our method.

where $W ^ { q }$ , $W ^ { k }$ , and $W ^ { v }$ map the inputs to a query, key, and value feature, respectively, and $d ^ { \prime }$ is the output dimension of key and query features. The latent feature is then updated with the attention block output. The task of fine-tuning aims at updating the mapping from given text to image distribution, and the text features are only input to $W ^ { k }$ and $W ^ { v }$ projection matrix in the cross-attention block. Therefore, we propose to only update $W ^ { k }$ and $W ^ { v }$ parameters of the diffusion model during the fine-tuning process. As shown in our experiments, this is sufficient to update the model with a new text-image paired concept. Figure 4 shows an instance of the cross-attention layer and the trainable parameters.

Text encoding. Given target concept images, we require a text caption as well. If there exists a text description, e.g., moongate, we use that as a text caption. For personalizationrelated use-case where the target concept is a unique instance of a general category, e.g., pet dog, we introduce a new modifier token embedding, i.e., $\boldsymbol { \nabla } ^ { * }$ dog. During training, $\boldsymbol { \nabla } ^ { * }$ is initialized with a rare occurring token embedding and optimized along with cross-attention parameters. An example text caption used during training is, photo of a $\boldsymbol { \nabla } ^ { * }$ dog.

Regularization dataset. Fine-tuning on the target concept and text caption pair can lead to the issue of language drift [39, 46]. For example, training on “moongate” will lead to the model forgetting the association of “moon” and “gate” with their previously trained visual concepts, as shown in Figure 5. Similarly, training on a personalized concept of $\boldsymbol { \nabla } ^ { * }$ tortoise plushy can leak, causing all examples with plushy to produce the specific target images. To prevent this, we select a set of 200 regularization images from the LAION-400M [69] dataset with corresponding captions that have a high similarity with the target text prompt, above threshold 0.85 in CLIP [57] text encoder feature space.

# 3.2. Multiple-Concept Compositional Fine-tuning

Joint training on multiple concepts. For fine-tuning with multiple concepts, we combine the training datasets for each individual concept and train them jointly with our method. To denote the target concepts, we use different modifier tokens, $\boldsymbol { \mathsf { V } } _ { i } ^ { * }$ , initialized with different rarely-occurring tokens and optimize them along with cross-attention key and value matrices for each layer. As shown in Figure 7, restricting the weight update to cross-attention key and value parameters leads to significantly better results for composing two concepts compared to methods like DreamBooth, which fine-tune all the weights.

![](images/ce24c7112685ed8cf2a61df2733b451d4f8c0bd46b21b2a60e7a1bbaf8535785.jpg)  
Figure 5. Role of regularization data in mitigating overfitting behavior during fine-tuning. $1 ^ { \mathrm { s t } }$ row: samples from pre-trained models. In $2 ^ { \mathrm { n d } }$ row, fine-tuning cross-attention key, value projection matrices without any regularization dataset leads to moongate like images on the text prompt photo of a moon. We largely mitigate this issue with the use of regularization datasets as shown in $3 ^ { \mathrm { r d } }$ row. More results can be found in Figure 24 (Appendix).

Constrained optimization to merge concepts. As our method only updates the key and value projection matrices corresponding to the text features, we can subsequently merge them to allow generation with multiple fine-tuned concepts. Let set $\{ W _ { 0 , l } ^ { k } , W _ { 0 , l } ^ { v } \} _ { l = 1 } ^ { L }$ represent the key and value matrices for all $L$ cross-attention layers in the pretrained model and {W kn,l $\{ W _ { n , l } ^ { k } , W _ { n , l } ^ { v } \} _ { l = 1 } ^ { L }$ represent the corresponding updated matrices for added concept $n \in \{ 1 \cdots N \}$ . As our subsequent optimization applies to all layers and key-value matrices, we will omit superscripts $\{ k , v \}$ and layer $l$ for notational clarity. We formulate the composition objective as the following constrained least squares problem:

$$
\begin{array} { r l } & { \hat { W } = \underset { W } { \arg \operatorname* { m i n } } | | W C _ { \mathrm { r e g } } ^ { \top } - W _ { 0 } C _ { \mathrm { r e g } } ^ { \top } | | _ { F } } \\ & { ~ \mathrm { s . t . } ~ W C ^ { \top } = V , ~ \mathrm { w h e r e } ~ C = [ \mathbf { c } _ { 1 } \cdot \cdot \cdot \mathbf { c } _ { N } ] ^ { \top } } \\ & { ~ \mathrm { a n d } ~ V = [ W _ { 1 } \mathbf { c } _ { 1 } ^ { \top } \cdot \cdot \cdot W _ { N } \mathbf { c } _ { N } ^ { \top } ] ^ { \top } . } \end{array}
$$

Here, $C \in \mathbb { R } ^ { s \times d }$ is the text features of dimension $d$ . These are compiled of $s$ target words across all $N$ concepts, with all captions for each concept flattened out and concatenated. Similarly, $C _ { \mathrm { r e g } } \in \mathbb { R } ^ { s _ { \mathrm { r e g } } \times d }$ consists of text features of $\sim 1 0 0 0$ randomly sampled captions for regularization. Intuitively, the above formulation aims to update the matrices in the original model, such that the words in target captions in $C$ are mapped consistently to the values obtained from finetuned concept matrices. The above objective can be solved in closed form, assuming $C _ { \mathrm { r e g } }$ is non-degenerate and the solution exists, by using the method of Lagrange multipliers [9]:

$$
\begin{array} { r } { \hat { W } = W _ { 0 } + \mathbf { v } ^ { \top } \mathbf { d } , \mathrm { w h e r e } \mathbf { d } = C ( C _ { \mathrm { r e g } } ^ { \top } C _ { \mathrm { r e g } } ) ^ { - 1 } } \\ { \mathbf { a n d } \mathbf { v } ^ { \top } = ( V - W _ { 0 } C ^ { \top } ) ( \mathbf { d } C ^ { \top } ) ^ { - 1 } . } \end{array}
$$

We show full derivation in the Appendix B. Compared to joint training, our optimization-based method is faster $( \sim 2$ seconds) if each individual fine-tuned model exists. Our proposed methods lead to the coherent generation of two new concepts in a single scene, as shown in Section 4.2.

Training details. We train with our method for 250 steps in single-concept and 500 steps in two-concept joint training, on a batch size of 8 and learning rate $8 \times 1 0 ^ { - 5 }$ . During training, we also randomly resize the target images from $0 . 4 - 1 . 4 \times$ and append the prompt “very small”, “far away” or “zoomed in”, “close up” accordingly to the text prompt based on resize ratio. We only backpropagate the loss on valid regions. This leads to faster convergence and improved results. We provide more training details in the Appendix E.

# 4. Experiments

In this section, we show the results of our method on multiple datasets in both single concept fine-tuning and composition of two concepts on the Stable Diffusion model [2]. Datasets. We perform experiments on ten target datasets spanning a variety of categories and varying training samples. It consists of two scene categories, two pets, and six objects, as shown in Figure 8. We also recently introduced a new dataset, CustomConcept101, consisting of 101 concepts, and show our results on it in Appendix A and the website.

Evaluation metrics. We evaluate our method on (1) Imagealignment, i.e., the visual similarity of generated images with the target concept, using similarity in CLIP image feature space [21], (2) Text-alignment of the generated images with given prompts, using text-image similarity in CLIP feature space [27], and (3) KID [8] on a validation set of 500 real images of a similar concept retrieved from LAION-400M to measure overfitting on the target concept (e.g., $\mathrm { V } ^ { \ast }$ dog)and forgetting of existing related concepts (e.g., dog). (4) We also perform a human preference study of our method with baselines. Unless mentioned otherwise, we use 200 steps of DDPM sampler with a scale 6. The prompts used for quantitative and human evaluation are shown on our website.

Baselines. We compare our method with the two concurrent works, DreamBooth [65] (third-party implementation [84]) and Textual Inversion [21]. DreamBooth fine-tunes all the parameters in the diffusion model, keeping the text transformer frozen, and uses generated images as the regularization dataset. Each target concept is represented by a unique identifier, followed by its category name, i.e., “[V] category”, where [V] is a rarely occurring token in the text token space and not optimized during fine-tuning. Textual Inversion optimizes a new $\boldsymbol { \nabla } ^ { * }$ token for each new concept. We also compare with the competitive baseline of Custom Diffusion (w/ fine-tune all), where we fine-tune all the parameters in the U-Net [64] diffusion model, along with the $\boldsymbol { \nabla } ^ { * }$ token embedding in our method. We provide implementation details for all baselines in the supplement.

# 4.1. Single-Concept Fine-tuning Results

Qualitative evaluation. We test each fine-tuned model on a set of challenging prompts. This includes generating the target concept in a new scene, in a known art style, composing it with another known object, and changing certain properties of the target concept: e.g., color, shape, or expression. Figure 6 shows the sample generations with our method, DreamBooth, and Textual Inversion. Our method, Custom Diffusion, has higher text-image alignment while capturing the visual details of the target object. It performs better than Textual Inversion and is on par with DreamBooth while having a lower training time and model storage $( \sim 5 \times$ faster and 75MB vs 3GB storage).

Quantitative evaluation. We evaluate on 20 text prompts and 50 samples per prompt for each dataset, resulting in a total of 1000 generated images. We use DDPM sampling with 50 steps and a classifier-free guidance scale of 6 across all methods. As shown in Figure 8, our method outperforms the concurrent methods [21, 65]. Also, Custom Diffusion works on par with our proposed baseline of fine-tuning all the weights in the diffusion model, while being more computationally and time efficient. Table 1 also shows the KID of generated images by each fine-tuned model on a reference dataset, with captions similar to the fine-tuned concept. As we observe, our method has lower KID than most baselines, which suggests less overfitting to the target concept. In Appendix C, we show that the updated matrices can be compressed to further reduce model storage.

Computational requirements Training time of our method is $\sim 6$ minutes (2 A100 GPUs), compared to 20 minutes for Ours (w/ fine-tune all) (4 A100s), 20 minutes for Textual Inversion (2 A100s), and $\sim 1$ hour for DreamBooth (4 A100s). Also, since we update only 75MB of weights, our method has low memory requirements for storing each concept model. We keep the batch size fixed at 8 across all.

# 4.2. Multiple-Concept Fine-tuning Results

We show the results of generating two new concepts in the same scene on the following five pairs: (1) Moongate $^ +$ Dog, (2) Cat $^ +$ Chair, (3) Wooden $\mathrm { P o t } + \mathrm { C a t }$ , (4) Wooden $\mathrm { P o t } +$ flower, and (5) Table $^ +$ Chair. We compare our method with DreamBooth training on the two datasets together, using two different $\left[ V _ { 1 } \right]$ and $[ V _ { 2 } ]$ tokens for each concept. For Textual Inversion, we perform inference using the individually fine-tuned tokens for each concept in the same sentence. We compare our method with the baselines on a set of $4 0 0 \mathrm { i m } \cdot$ - ages generated with 8 prompts for each composition setting in Figure 8 and Table 1. We also compare with our baseline of sequentially training on the two concepts or fine-tuning all weights in our method. Our method performs better on all except the “Table $^ +$ Chair” composition, where all methods perform comparably except Textual Inversion, which doesn’t perform well at composition as also shown in Figure 22 in the Appendix. This shows the importance of fine-tuning only the cross-attention parameters for composition. In the case of sequential training, we observe forgetting of the first concept. Figure 7 shows sample images of our proposed two methods and DreamBooth. As we can see, our method is able to generate the two objects in the same scene in a coherent manner while having high alignment with the input text prompt. We show more samples on our website.

![](images/31d3732628a6216729ebc00157ec01cc8dbe8587a4674a981faa446843c2f8ac.jpg)  
Figure 6. Single-concept fine-tuning results. Given images of a new concept (target images shown on the left), our method can generate images with the concept in unseen contexts and art styles. First row: representing the concept in an artistic style of watercolor paintings. Our method can also generate the mountains in the background, which DreamBooth and Textual Inversion omit. Second row: changing the background scene. Our method and DreamBooth perform similarly and better than Textual Inversion. Third row: adding another object, e.g., an orange sofa with the target table. Our method successfully adds the other object. Fourth row: changing object property, like color of petals. Fifth row: accessorizing personal pet cat with sunglasses. Our method preserves the visual similarity better than baselines while changing only the petal colors or adding sunglasses to the cat. For Textual Inversion, the input text prompt consists of only optimized $\boldsymbol { \nabla } ^ { * }$ instead of $\boldsymbol { \nabla } ^ { * }$ category. We show more samples on our website.

![](images/efe9499bc6c81c304fdd0feb025782dfd086f12addc727b8a5208f0e680520c5.jpg)  
Figure 7. Multi-concept fine-tuning results. First row: our method has higher visual similarity with the personal cat and chair images shown in the first column while following the text condition. Second row: DreamBooth omits the cat in 3 out of 4 images, whereas our method generates both cats and wooden pots. Third row: our method generates the target flower in the wooden pot while maintaining the visual similarity to the target images. Fourth row: generating the target table and chair together in a garden. For all settings, our optimization-based approach and joint training perform better than DreamBooth, and joint training performs better than the optimization-based method.

# 4.3. Human Preference Study

We perform the human preference study using Amazon Mechanical Turk. We do a paired test of our method with DreamBooth, Textual Inversion, and Ours (w/ fine-tune all).

![](images/7ce9f9aec92e4b6355ab43b6b69db8dc5fda84964982ee9ddba09b4898a3fecc.jpg)  
Figure 8. Text- and image-alignment for single-concept (left) and multi-concept (right) fine-tuning. Compared to DreamBooth, Textual Inversion, and our w/ fine-tune all baseline, our method lies further along the upper right corner (with less variance). There exists a trade-off between high similarity to target images vs. text-alignment on new prompts. Keeping in consideration this trade-off, our method is on-par or better than the baselines. For multi-concept both joint training and optimization based method are better than other baselines.

<table><tr><td></td><td>Method</td><td>Text-alignment</td><td>Image-alignment</td><td>KID (validation)</td></tr><tr><td rowspan="4">Single- Concept</td><td>Textual Inversion</td><td>0.670</td><td>0.827</td><td>22.27</td></tr><tr><td>DreamBooth</td><td>0.781</td><td>0.776</td><td>32.53</td></tr><tr><td>ÖOurs (w/ fine-tune all)</td><td>0.795</td><td>0.748</td><td>19.27</td></tr><tr><td>Ours</td><td>0.795</td><td>0.775</td><td>20.96</td></tr><tr><td rowspan="6">Multi- Concept</td><td>Textual Inversion</td><td>0.544</td><td>0.630</td><td></td></tr><tr><td>DreamBooth</td><td>0.783</td><td>0.695</td><td></td></tr><tr><td>Öurs (w fine-tune all)</td><td>0.787</td><td>0.6“9</td><td></td></tr><tr><td>Ours (Sequential)</td><td>0.797</td><td>0.700</td><td></td></tr><tr><td>Ours (Optimization)</td><td>0.800</td><td>0.695</td><td></td></tr><tr><td>Ours (Joint)</td><td>0.801</td><td>0.706</td><td></td></tr></table>

Table 1. Quantitative comparisons. Top row: single-concept finetuning evaluation averaged across datasets. The last column shows the KID $( \times 1 0 ^ { 3 } )$ between real validation set images and generated images with the same caption. Since our method uses a regularization set of real images, it achieves lower KID and even improves slightly over the pretrained model. Textual Inversion has the same KID as the pretrained model, as it does not update the model. Bottom row: evaluation on multi-concept averaged across the five composition pairs. We show individual scores for all in Table 7 and 8 in the Appendix. We also evaluate single-concept fine-tuned models on FID [28] (MS-COCO [43]) in Table 6 and show the trend of image-, text-alignment with training steps in Figure 20.

<table><tr><td rowspan="2">Ours</td><td colspan="2">Textual Inversion</td><td colspan="2">DreamBooth</td><td colspan="2">Ours (w/ fine-tune all)</td></tr><tr><td>Text Alignment</td><td>Image Alignment</td><td>Text Alignment</td><td>Image Alignment</td><td>Text Alignment</td><td>Image Alignment</td></tr><tr><td>Single-concept</td><td>72.62 ± 2.38%</td><td>51.62 ± 2.62%</td><td>53.50 ± 2.64%</td><td>56.62 ± 2.44%</td><td>55.17 ± 2.55%</td><td>53.99 ± 2.44%</td></tr><tr><td>Multi-concept (Joint)</td><td>86.65 ± 2.25 %</td><td>81.89 ± 2.09 %</td><td>56.39 ± 2.46 %</td><td>61.80 ± 2.59%</td><td>59.00 ± 2.61 %</td><td>59.12 ± 2.72%</td></tr><tr><td>Multi-concept (Optimization)</td><td>81.2 ± 2.72%</td><td>83.11 ± 2.18%</td><td>57.0 ± 2.62 %</td><td>61.75 ± 2.68%</td><td>57.60 ± 2.43 %</td><td>53.49 ± 2.71%</td></tr></table>

Table 2. Human preference study. For each paired comparison, our method is preferred (over $\ge 5 0 \%$ ) over the baseline in both image- and text-alignment. Textual Inversion seems to overfit to target images and thus has a similar image-alignment as ours but performs worse on text-alignment in the single-concept setting.

For text-alignment, we show the two generations from each method (ours vs. baseline) on the same prompt with the question – “Which image is more consistent with the text?”. In image-alignment, we show 2-4 training samples of the

<table><tr><td>Method</td><td>Text-alignment</td><td>Image-alignment</td><td>KID (validation)</td></tr><tr><td>Ours</td><td>0.795</td><td>0.775</td><td>20.96</td></tr><tr><td>Ours w/o Aug</td><td>0.800</td><td>0.736</td><td>20.67</td></tr><tr><td>Ours w/o Reg</td><td>0.799</td><td>0.756</td><td>32.64</td></tr><tr><td>Ours w/ Gen</td><td>0.791</td><td>0.768</td><td>34.70</td></tr></table>

Table 3. Ablation Study. No augmentation during training leads to lower image-alignment. No regularization dataset or using generated images as regularization produces much worse KID.

relevant concept along with the generated images (same as in text-alignment study) with the question – “Which image better represents the objects as shown in target images?”. We collect 800 responses for each questionnaire. As shown in Table 2, our method is preferred over baselines in both single-concept and multi-concept, even compared to Ours (w/ fine-tune all) method, which shows the importance of only updating cross-attention parameters.

# 4.4. Ablation and Applications

In this section, we ablate various components of our method to show its contribution. We evaluate each experiment on the same setup as in Section 4.1. We show sample generations for ablation experiments on our website.

Generated images as regularization (Ours w/ Gen). As detailed in Section 3.1, we retrieve similar category real images and captions to use as regularization during fine-tuning. Another way of creating the regularization dataset is to generate images from the pretrained model [65]. We compare our method with this setup, i.e., for the target concept of a “category” generate images using the prompt, photo of a {category}, and show results in Table 3. Using generated images results in a similar performance on the target concept. However, this shows signs of overfitting, as measured by KID on a validation set of similar category real images.

Without regularization dataset (Ours w/o Reg). We show results when no regularization dataset is used. We train the model for half the number of iterations (the same number of target images seen during training). Table 3 shows that the model has slightly lower image-alignment and tends to forget existing concepts, as evident from high KID on the validation set.

![](images/ab5e08a184fe4d4f8d663b0bf80884f706d69c5e0e9b4bc238c2f6b5625d40e6.jpg)  
Figure 9. Custom Diffusion with artistic styles. Our method can also be used to learn new styles. We finetune our model on 13 and 15 images for the top and bottom rows, respectively. The last column shows the composition of style with the new “wooden pot” concept (target images shown in Figure 7) using the optimization-based method. Style image credits: Mia Tang (top row), Aaron Hertzmann (bottom row).V1 dog and a V2 cat  dog and a cat

![](images/25ad910400b9ef7e049f3ec0d409dbb4fdf77b4e45ad20f16c66dd2820e2bab4.jpg)  
Figure 10. Image editing with our models using Prompt-to-Prompt [26]. Left: replacement edit to only change the cap. Right: 8899image stylization while preserving the image content.

![](images/4fbb391cb68991d739713b27e94ed2901819d0c65b1fdc41d425e846ccca9e50.jpg)  
Figure 11. Failure cases on multi-concept fine-tuning. Our method fails at difficult compositions like a cat and dog together in a scene or similar category objects like teddybear and tortoise plushy as shown on the right. Though as shown on the left, the pretrained model also struggles with similar compositions.

Without data augmentation (Ours w/o Aug). As mentioned in Section 3.2, we augment by randomly resizing the target images during training and append size-related prompts (e.g., “very small”) to the text. Here, we show the effect of not using these augmentations. The model is trained for the same number of steps. Table 3 shows that no augmentation leads to lower visual similarity with target images.

Fine-tuning on a style. We show in Figure 9 that our method can also be used for fine-tuning with specific styles. We change the text prompt for target images to A painting in the style of $\boldsymbol { \nabla } ^ { * }$ art [21]. The regularization images are retrieved with captions having high similarity with “art”. Image editing with fine-tuned models Similar to the pretrained model, our fine-tuned models can be used by existing image-editing methods. We show an example of using the Prompt-to-prompt [26] method in Figure 10.

# 5. Discussion and Limitations

In conclusion, we have proposed a method for fine-tuning large-scale text-to-image diffusion models on new concepts, categories, personal objects, or artistic styles, using just a few image examples. Our computationally efficient method can generate novel variations of the fine-tuned concept in new contexts while preserving the visual similarity with the target images. Moreover, we only need to save a small subset of model weights. Furthermore, our method can coherently compose multiple new concepts in the same scene.

As shown in Figure 11, difficult compositions, e.g., a pet dog and a pet cat, remain challenging. In this case, the pretrained model also faces a similar difficulty, and our model inherits these limitations. Additionally, composing increasing three or more concepts together is also challenging. We show more analysis and visualization in the Appendix C.

Acknowledgment. We are grateful to Nick Kolkin, David Bau, Sheng-Yu Wang, Gaurav Parmar, John Nack, and Sylvain Paris for their helpful comments and discussion and to Allie Chang, Chen Wu, Sumith Kulal, Minguk Kang, Yotam Nitzan, and Taesung Park for proofreading the draft. We also thank Mia Tang and Aaron Hertzmann for sharing their artwork. We also appreciate the help of Sheng-Yu Wang, Songwei Ge, Daohan Lu, Ruihan Gao, Roni Shechtman, Avani Sethi, Yijia Wang, Shagun Uppal, and Zhizhuo Zhou in collecting the dataset, and Nick Kolkin for the feedback regarding it. This work was partly done by Nupur Kumari during the Adobe internship. The work is partly supported by Adobe Inc.

# References

[1] Chatgpt. https://chat.openai.com/chat, 2022. 14   
[2] Stable diffusion. https : / / huggingface . co / CompVis/stable-diffusion-v-1-4-original, 2022. 2, 3, 5   
[3] Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan latent space? In IEEE International Conference on Computer Vision (ICCV), 2019. 2   
[4] Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan $^ { + + }$ : How to edit the embedded images? In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 2   
[5] Rameen Abdal, Peihao Zhu, John Femiani, Niloy Mitra, and Peter Wonka. Clip2stylegan: Unsupervised extraction of stylegan edit directions. In ACM SIGGRAPH, 2022. 2   
[6] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations (ICLR), 2015. 2   
[7] David Bau, Steven Liu, Tongzhou Wang, Jun-Yan Zhu, and Antonio Torralba. Rewriting a deep generative model. In European Conference on Computer Vision (ECCV), 2020. 2   
[8] Mikolaj Binkowski, Danica J Sutherland, Michael Arbel, and ´ Arthur Gretton. Demystifying mmd gans. In International Conference on Learning Representations (ICLR), 2018. 5, 18   
[9] Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004. 5, 14   
[10] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. In International Conference on Learning Representations (ICLR), 2019. 2   
[11] Lucy Chai, David Bau, Ser-Nam Lim, and Phillip Isola. What makes fake images detectable? understanding properties that generalize. In European Conference on Computer Vision (ECCV), 2020. 22   
[12] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Conditioning method for denoising diffusion probabilistic models. In IEEE International Conference on Computer Vision (ICCV), 2021. 2   
[13] Riccardo Corvi, Davide Cozzolino, Giada Zingarini, Giovanni Poggi, Koki Nagano, and Luisa Verdoliva. On the detection of synthetic images generated by diffusion models. arXiv preprint arXiv:2211.00680, 2022. 22   
[14] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In Conference on Neural Information Processing Systems (NeurIPS), 2021. 2   
[15] Yuxuan Ding, Lingqiao Liu, Chunna Tian, Jingyuan Yang, and Haoxuan Ding. Don’t stop learning: Towards continual learning for the clip model. arXiv preprint arXiv:2207.09248, 2022. 2   
[16] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components estimation. In ICLR Workshop, 2015. 2   
[17] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. In International Conference on Learning Representations (ICLR), 2017. 2   
[18] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 2   
[19] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations (ICLR), 2019. 17   
[20] Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3(4):128–135, 1999. 2   
[21] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 2, 5, 9, 18, 20, 21   
[22] Rinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clipguided domain adaptation of image generators. ACM Transactions on Graphics (TOG), 41(4):1–13, 2022. 2   
[23] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clipadapter: Better vision-language models with feature adapters. arXiv preprint arXiv:2110.04544, 2021. 2   
[24] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139–144, 2020. 2   
[25] Zheng Gu, Wenbin Li, Jing Huo, Lei Wang, and Yang Gao. Lofgan: Fusing local representations for few-shot image generation. In IEEE International Conference on Computer Vision (ICCV), 2021. 2   
[26] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 9   
[27] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. In EMNLP, 2021. 5   
[28] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Conference on Neural Information Processing Systems (NeurIPS), 2017. 8, 20, 21   
[29] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Conference on Neural Information Processing Systems (NeurIPS), 2020. 2, 3   
[30] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 2   
[31] Xun Huang, Arun Mallya, Ting-Chun Wang, and Ming-Yu Liu. Multimodal conditional image synthesis with product-ofexperts gans. In European Conference on Computer Vision, pages 91–109. Springer, 2022. 2   
[32] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 2, 3   
[33] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. In Conference on Neural Information Processing Systems (NeurIPS), 2020. 2   
[34] Tero Karras, Miika Aittala, Samuli Laine, Erik Hark ¨ onen, ¨ Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. In Conference on Neural Information Processing Systems (NeurIPS), 2021. 2   
[35] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2   
[36] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference on Learning Representations (ICLR), 2014. 2, 3   
[37] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. 2   
[38] Nupur Kumari, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Ensembling off-the-shelf models for gan training. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2   
[39] Jason Lee, Kyun ghyun Cho, and Douwe Kiela. Countering language drift via visual grounding. In EMNLP, 2019. 2, 4   
[40] Dingcheng Li, Zheng Chen, Eunah Cho, Jie Hao, Xiaohu Liu, Fan Xing, Chenlei Guo, and Yang Liu. Overcoming catastrophic forgetting during domain adaptation of seq2seq language generation. In NAACL, 2022. 2   
[41] Yijun Li, Richard Zhang, Jingwan Lu, and Eli Shechtman. Few-shot image generation with elastic weight consolidation. In Conference on Neural Information Processing Systems (NeurIPS), 2020. 2, 3   
[42] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017. 2   
[43] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence ´ Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision (ECCV), 2014. 2, 8, 21   
[44] Bingchen Liu, Yizhe Zhu, Kunpeng Song, and Ahmed Elgammal. Towards faster and stabilized gan training for highfidelity few-shot image synthesis. In International Conference on Learning Representations (ICLR), 2021. 2, 20, 21   
[45] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual generation with composable diffusion models. In ECCV, 2022. 2   
[46] Yuchen Lu, Soumye Singhal, Florian Strub, Aaron Courville, and Olivier Pietquin. Countering language drift with seeded iterated learning. In International Conference on Machine Learning (ICML), 2020. 2, 4   
[47] Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Generating images from captions with attention. arXiv preprint arXiv:1511.02793, 2015. 2   
[48] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations (ICLR), 2022. 2   
[49] Sangwoo Mo, Minsu Cho, and Jinwoo Shin. Freeze the discriminator: a simple baseline for fine-tuning gans. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop, 2020. 2   
[50] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning (ICML), 2022. 2   
[51] Yotam Nitzan, Kfir Aberman, Qiurui He, Orly Liba, Michal Yarom, Yossi Gandelsman, Inbar Mosseri, Yael Pritch, and Daniel Cohen-Or. Mystyle: A personalized generative prior. In SIGGRAPH ASIA, 2022. 2   
[52] Atsuhiro Noguchi and Tatsuya Harada. Image generation from small datasets via batch statistics adaptation. In IEEE International Conference on Computer Vision (ICCV), 2019. 2   
[53] Utkarsh Ojha, Yijun Li, Jingwan Lu, Alexei A Efros, Yong Jae Lee, Eli Shechtman, and Richard Zhang. Few-shot image generation via cross-domain correspondence. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 2   
[54] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On buggy resizing libraries and surprising subtleties in fid calculation. arXiv preprint arXiv:2104.11222, 2021. 20   
[55] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Text-driven manipulation of stylegan imagery. In IEEE International Conference on Computer Vision (ICCV), 2021. 2   
[56] Jonas Pfeiffer, Ivan Vulic, Iryna Gurevych, and Sebastian ´ Ruder. Mad-x: An adapter-based framework for multi-task cross-lingual transfer. In EMNLP, 2020. 2   
[57] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML),   
2021. 4 [58] Vinay Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan Dyer. Effect of scale on catastrophic forgetting in neural networks. In International Conference on Learning Representations (ICLR), 2021. 2 [59] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.   
1, 2 [60] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning (ICML), 2021. 2 [61] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. In NeurIPS,   
2019. 2 [62] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. In International Conference on Machine Learning (ICML), 2016. 2 [63] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image ¨ synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.   
1, 2, 3 [64] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234–241. Springer,   
2015. 5 [65] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022. 2, 5, 8, 18,   
20, 21 [66] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. In NeurIPS, 2022. 1, 2 [67] Axel Sauer, Kashyap Chitta, Jens Muller, and Andreas Geiger. ¨ Projected gans converge faster. In Conference on Neural Information Processing Systems (NeurIPS), 2021. 2 [68] Axel Sauer, Katja Schwarz, and Andreas Geiger. Styleganxl: Scaling stylegan to large diverse datasets. In ACM SIG-GRAPH, 2022. 2 [69] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion- $. 4 0 0 \mathrm { m }$ : Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. 4 [70] Gabriel Skantze and Bram Willemsen. Collie: Continual learning of language grounding from language-image embeddings. Journal of Artificial Intelligence Research, 74:1201–1223,   
2022. 2 [71] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning (ICML), 2015. 2, 3 [72] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations (ICLR), 2021. 3 [73] Ming Tao, Hao Tang, Songsong Wu, Nicu Sebe, Xiao-Yuan Jing, Fei Wu, and Bingkun Bao. Df-gan: Deep fusion generative adversarial networks for text-to-image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2 [74] Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, Trung-Kien Nguyen, and Ngai-Man Cheung. Towards good practices for data augmentation in gan training. arXiv preprint arXiv:2006.05338, 2, 2020. 2 [75] Aaron Van Den Oord, Nal Kalchbrenner, and Koray ¨ Kavukcuoglu. Pixel recurrent neural networks. In International Conference on Machine Learning (ICML), 2016.   
2 [76] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 2, 3 [77] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset.   
2011. 2 [78] Sheng-Yu Wang, David Bau, and Jun-Yan Zhu. Rewriting geometric rules of a gan. ACM SIGGRAPH, 2022. 2 [79] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A Efros. Cnn-generated images are surprisingly easy to spot... for now. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 22 [80] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image synthesis and semantic manipulation with conditional gans. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 2 [81] Yaxing Wang, Abel Gonzalez-Garcia, David Berga, Luis Herranz, Fahad Shahbaz Khan, and Joost van de Weijer. Minegan: effective knowledge transfer from gans to target domains with few images. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 2 [82] Yaxing Wang, Chenshen Wu, Luis Herranz, Joost van de Weijer, Abel Gonzalez-Garcia, and Bogdan Raducanu. Transferring gans: generating images from limited data. In European Conference on Computer Vision (ECCV), 2018. 2 [83] Wikipedia. Moon gate. https://en.wikipedia.org/ wiki/Moon_gate, 2022. 1 [84] XavierXiao. Dreambooth on stable diffusion. https:// github.com/XavierXiao/Dreambooth-Stable-Diffusion, 2022. 5, 22 [85] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 2   
[86] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022. 1, 2   
[87] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In IEEE International Conference on Computer Vision (ICCV), 2017. 2   
[88] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 3   
[89] Miaoyun Zhao, Yulai Cong, and Lawrence Carin. On leveraging pretrained gans for generation with limited data. In International Conference on Machine Learning (ICML), 2020. 2   
[90] Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han. Differentiable augmentation for data-efficient gan training. In Conference on Neural Information Processing Systems (NeurIPS), 2020. 2   
[91] Zhengli Zhao, Zizhao Zhang, Ting Chen, Sameer Singh, and Han Zhang. Image augmentations for gan training. arXiv preprint arXiv:2006.02595, 2020. 2   
[92] Jun-Yan Zhu, Philipp Krahenb ¨ uhl, Eli Shechtman, and ¨ Alexei A Efros. Generative visual manipulation on the natural image manifold. In European Conference on Computer Vision (ECCV), 2016. 2   
[93] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In IEEE International Conference on Computer Vision (ICCV), 2017. 2   
[94] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-gan: Dynamic memory generative adversarial networks for textto-image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 2

# Appendix

In Section A, we show the results of our method on the CustomConcept101 dataset, which consists of a significantly larger number of custom concepts from a wide variety of categories. In Section B, we show the derivation of our optimization-based method for merging multiple concepts into a single model. In Section C, we show additional experiment results of our method. We then provide more evaluation results, for example, the trend with training iterations, in Section D and implementations details in Section E. Finally, we discuss the societal implications of our method in Section F.

# A. CustomConcept101

We show the results on our newly released dataset of 101 concepts consisting of a wide variety of categories, including toys, plushies, wearables, pets, scenes, and human faces. Figure 12 shows one image of each concept from the dataset. The images are collected from websites that allow redistribution, e.g., Unsplash, or captured by ourselves. For multi-concept, we propose 101 unique compositions among the concepts, e.g., $\mathtt { V } _ { 1 } ^ { * }$ dog with ${ \boldsymbol { \mathsf { V } } } _ { 2 } ^ { * }$ sunglasses. We also create text prompts for evaluating the customization, 20 prompts each in the case of single-concepts, and 12 prompts for multi-concept composition. We first used ChatGPT [1] to propose 40 prompts consisting of a particular concept or pair of concepts. We instruct it to change the background scene, insert another object in the scene, or create stylistic variations of the main subjects. We then manually filtered and modified the proposals to create a final set of prompts.

Table 4 shows the comparison of our method with Dream-Booth and Textual Inversion. Both ours and DreamBooth models are trained with generated images as regularization on this dataset. In the single-concept case, our method performs on par with DreamBooth with marginally lower image-alignment and higher text-alignment. Textual Inversion overfits the target images and has low text-alignment. For multi-concept customization, both our optimization and joint training method are better than DreamBooth on average. Sample images for each method are shown in Figure 13. For more results and dataset details, please refer to our code and website.

# B. Multi-Concept Optimization Based Method

To compose multiple concepts together, we solved a constrained least squares problem (introduced in Section 3.2, Eqn. 4). We solved the problem in closed form as shown in Eqn. 5 in the main paper. Here, we show the full derivation.

<table><tr><td></td><td>Method</td><td>Text-alignment</td><td>Image-alignment</td></tr><tr><td rowspan="3">Single-</td><td>Textual Inversion</td><td>0.612</td><td>0.752</td></tr><tr><td>DreamBooth</td><td>0.752</td><td>0.752</td></tr><tr><td>Ours</td><td>0.760</td><td>0.744</td></tr><tr><td rowspan="3">Multi- Concept</td><td>DreamBooth</td><td>0.738</td><td>0.662</td></tr><tr><td>Ours (Optimization)</td><td>0.763</td><td>0.658</td></tr><tr><td>Ours (Joint)</td><td>0.757</td><td>0.668</td></tr></table>

Table 4. Quantitative comparison on CustomConcept101. On single-concepts, our method is better on text-alignment and slightly worse on image-ailgnment compared to DreamBooth. For multiconcept, our method outperforms DreamBooth on average. The metrics are calculated over 100 images generated using 20 prompts for single-concept and 120 images generated using 12 prompts for multi-concept and then averaged across all models. We used DDPM sampler with 200 steps in each case.

We first restate the objective below.

$$
\begin{array} { r l } & { \hat { W } = \underset { W } { \arg \operatorname* { m i n } } | | W C _ { \mathrm { r e g } } ^ { \top } { - } W _ { 0 } C _ { \mathrm { r e g } } ^ { \top } | | } \\ & { ~ \mathrm { s . t . } ~ W C ^ { \top } = V , ~ \mathrm { w h e r e } ~ C = [ \mathbf { c } _ { 1 } \cdot \cdot \cdot \mathbf { c } _ { N } ] ^ { \top } ~ } \\ & { ~ \mathrm { a n d } ~ V = [ W _ { 1 } \mathbf { c } _ { 1 } ^ { \top } \cdot \cdot \cdot W _ { N } \mathbf { c } _ { N } ^ { \top } ] ^ { \top } } \end{array}
$$

Here, the matrix norms are the Frobenius norm, $W _ { 0 } \in \mathbb { R } ^ { o \times d }$ is the matrix from the pretrained model, $C ~ \in ~ \mathbb { R } ^ { s \times d }$ is the text features of dimension $d$ . These are compiled of $s$ target words across all $N$ concepts, with all captions for each concept flattened out and concatenated. Similarly, $C _ { \mathrm { r e g } } \in \mathbb { R } ^ { s _ { \mathrm { r e g } } \times d }$ consists of text features of $\sim 1 0 0 0$ randomly sampled captions for regularization.

By using the method of Lagrange multipliers [9], we need to minimize the following objective:

$$
L = \frac { 1 } { 2 } | | W C _ { \mathrm { r e g } } ^ { \top } - W _ { 0 } C _ { \mathrm { r e g } } ^ { \top } | | - \mathrm { t r a c e } ( \mathbf { v } ( W C ^ { \top } - V ) ) ,
$$

here $\mathbf { v } \in \mathbb { R } ^ { s \times o }$ is the Lagrangian multiplier corresponding to the constraints. Differentiating the above objective and equating it to 0, we obtain:

$$
\begin{array} { r } { W C _ { \mathrm { r e g } } ^ { \top } C _ { \mathrm { r e g } } - W _ { 0 } C _ { \mathrm { r e g } } ^ { \top } C _ { \mathrm { r e g } } - \mathbf { v } ^ { \top } C = 0 } \\ { \implies W = W _ { 0 } + \mathbf { v } ^ { \top } C ( C _ { \mathrm { r e g } } ^ { \top } C _ { \mathrm { r e g } } ) ^ { - 1 } . } \end{array}
$$

We assume $C _ { \mathrm { r e g } }$ is non-degenerate. Using the above solution in Eqn. 6, $W C ^ { \top } = V$ , we obtain:

$$
\begin{array} { r l } & { ( W _ { 0 } + \mathbf { v } ^ { \top } C ( C _ { \mathrm { r e g } } ^ { \top } C _ { \mathrm { r e g } } ) ^ { - 1 } ) C ^ { \top } = V } \\ & { \mathrm { L e t } \mathbf { d } = C ( C _ { \mathrm { r e g } } ^ { \top } C _ { \mathrm { r e g } } ) ^ { - 1 } } \\ & { \mathbf { v } ^ { \top } = ( V - W _ { 0 } C ^ { \top } ) ( \mathbf { d } C ^ { \top } ) ^ { - 1 } . } \end{array}
$$

# C. Experiments

In this section, we show more experiments, including ablation and analysis of the limitations of our method.

![](images/1d69012a5cc15e26d7946f97780773ae59328317f24ba0c3d4295940d0dc3413.jpg)  
Figure 12. CustomConcept101 dataset. A example image of each concept in the dataset. Out of 101 categories, 31 are collected from Unsplash, 2 concepts belonging to the flower category are collected from other websites which allow redistribution, and the rest are captured by ourselves. For more details regarding the dataset and license, please refer to our webpage.

# Comparison with DreamBooth and Textual Inversion.

We also show a comparison of our method with DreamBooth and Textual Inversion on the target concept images from the respective works. Figure 17 shows the sample generations for the same input prompts. Our method performs better than baselines in text-alignment while maintaining the visual similarity to target images.

![](images/0cc6deb1f658bec557a12141518016fb35c63374d28b49b474ad51512d7e129c.jpg)  
A ${ \sf V } _ { 1 } { } ^ { * }$ dog lounging on a $\mathsf { V } _ { 2 } ^ { * }$ sofa on a sunny beach   
Figure 13. Qualitative comparison on CustomConcept101. Single-concept top block (first three rows) shows sample comparisons of our method with DreamBooth and Textual Inversion on three concepts from the dataset. Our method shows higher text alignment, whereas DreamBooth retains more image alignment with the target images. Multi-concept: bottom block (last two rows) shows multi-concept composition using our method and DreamBooth. Our joint training method outperforms it on image-alignment with both target images while maintaining text-alignment with the given caption. The optimization method fails to preserve the image similarity in some cases, e.g., the sofa in the last row. We select the best three out of five images for single-concept and the best three out of 10 images for multi-concept.

![](images/0b7b168bb7114f2c73c29400209b5caf3d2d0bc53f366d2eaed41aa6a23e091f.jpg)  
Figure 14. Analysis of singular values of the difference of the key and value projection matrices in the cross-attention layers between pretrained and fine-tuned model. As shown in the plot, the singular values drop to 0, suggesting that we can approximate the difference matrix with a low-rank matrix.

![](images/cbfdd8f2191c446798bbf019fe7e38b776a952695e23ccf1ad5365b52c2f33d8.jpg)  
Figure 15. Quantitative results with compressed models. We save the low-rank approximation of the difference between the pretrained and fine-tuned model updated weights. Even with the top $6 0 \%$ rank $5 \times$ compression), the performance remains similar (overlapping blue and black points). As we increase the compression, the imagealignment score decreases, and the model approaches the pretrained model weights with high text-alignment, as illustrated with qualitative samples in Figure 21.

Model compression. We analyze the singular values of the difference between updated key and value projection matrices and the corresponding pretrained matrices in all cross-attention layers of the model. As shown in Figure 14, the singular values steeply drop. This suggests that the difference matrix can be approximated well with a low-rank decomposition. Thus, we perform SVD and only save the low-rank factorization of the difference matrices. This can reduce the memory storage for each concept further. Figure 15 and 21 shows the quantitative and qualitative results with decreasing compression ratio. Even with an approximation using only the top $6 0 \%$ of singular values $5 \times$ compression in model storage), the performance is similar. But as we increase the compression to only include the top $1 \%$ of singular values, the image-alignment decreases. Top ${ \bf k } \%$ implies singular values till the rank where cumulative sum is ${ \bf k } \%$ of total sum of singular values. We also attempted to enforce a low-rank update to the key, value matrices during fine-tuning but observed the results to be sub-optimal [19]. More sample generations are shown on our website.

![](images/50db54af1faaf90259df1dd5c2e8c44d683c60adb4f8f5e56d0a0d6c4fa5c46b.jpg)  
Figure 16. Three-concept fine-tuning results. We can also compose two objects with a style or three new objects in the same scene for some combination of concepts.

Choice of $\boldsymbol { \nabla } ^ { * }$ . In all experiments, we initialized the unique modifier token with the token-id 42170. During joint training with two concepts, we initialize the other with tokenid 47629 in the pretrained CLIP tokenizer used in Stable Diffusion. We ablate our choice of $\boldsymbol { \nabla } ^ { * }$ with – (1) Random initialization with the mean and standard deviation of existing token embeddings and then optimizing the modifier token, (2) not optimizing the modifier token, once initialized with the rarely occurring token. The quantitative results are shown in Table 5. We observe that not optimizing $\boldsymbol { \nabla } ^ { * }$ leads to significantly lower image-alignment. Compared to random initialization of $\boldsymbol { \nabla } ^ { * }$ , our method has higher image-alignment and lower text-alignment. But we observe that the generated samples with the prompt a {category} shift more towards target image distribution of $\boldsymbol { \nabla } ^ { * }$ category, compared to our final method, as shown in Figure 18.

Three concept fine-tuning. We test the limit of our multiconcept method and show results on three concepts in Figure 16. Our method is able to compose two new objects with a new style or synthesize three new objects.

Limitations of our multi-concept fine-tuning. As shown in Figure 11 in the paper, our method fails at difficult compositions like generating personal cat and dog in the same scene. We observe that the pretrained model also struggles with such compositions and hypothesize that our model inherits these limitations. Here, we analyze the attention map of each token on the latent image features in Figure 19. The “dog” and “cat” token attention maps are largely overlapping for both our and pretrained models, which might lead to worse composition.

![](images/ca3b9b585f002b9f67ed30039456501871edef53d6d5a139d7242eec4fdc041e.jpg)  
Figure 17. Results of our method on DreamBooth [65] and Textual Inversion [21] datasets. Our method works similarly or better in some instances, for example, $\boldsymbol { \nabla } ^ { * }$ backpack with the night sky, and for a $\boldsymbol { \nabla } ^ { * }$ cat backpack. The samples are generated with 200 steps of the DDPM sampler, scale 6.

Generated images as regularization (Ours w/ Gen). We showed in Section 4.4 (Table 3) that using generated images as regularization leads to similar performance on the target concept but higher KID [8] on the validation set. We show here that it also leads to artifacts in the generations with category word used to generate images which are used as the regularization set. For example, fine-tuned model on $\boldsymbol { \nabla } ^ { * }$ dog generates images with saturation artifact for the prompt a dog as shown in Figure 18. Since we use a high learning rate compared to DreamBooth (which also uses generated images as regularization). A lower learning rate might mitigate this issue but at the cost of increased training time.

# D. Evaluation

Text- and image-alignment scores with training iterations. There is usually a trade-off between text-alignment and image-alignment. High image-alignment leads to a decrease in text-alignment where sample generations have less variance and are close to input target images. We show the trend of text- and image-alignment in Figure 20, which shows that initially the model has high text-alignment (as pretrained model) but low image-alignment and with training the curves for text-/image-alignment gradually gets worse/better. Table 7 and 8 shows the individual text- and image-alignment scores for the single-concept and multi-concept fine-tuning. To measure text-alignment, we remove the modifier token $\boldsymbol { \nabla } ^ { * }$ from the text prompt for extracting CLIP text features.

![](images/20982e4be2beef0d497482f9718708ca105afa87b31ea5cb92b7ad1c615cc4c0.jpg)  
Figure 18. Qualitative analysis of Choice of $\boldsymbol { \nabla } ^ { * }$ (Ours w/ random init $\boldsymbol { \nabla } ^ { * }$ ) and generated images as regularization (Ours w/ Gen). We show the generated samples for the original category word, for e.g., images generated on the prompt a dog for models fine-tuned on $\boldsymbol { \nabla } ^ { * }$ dog. We also show the sample generated by pretrained model in column (d) for comparison. Column (b) shows that initializing $\boldsymbol { \nabla } ^ { * }$ randomly from the normal distribution of existing token embeddings and then optimizing leads to more shifts in original category words getting mapped to target images compared to our method in column (a). Similarly, with generated images as regularization, the quality of samples gets worse for the original category as shown in column (c).

![](images/96bfb2e081a04c6cddc61806193813afd6f48c65fb6073d7fa38f93252bbecf1.jpg)  
Figure 19. Attention map visualization of failed compositions. We show the average attention across timestep and layers for each word (token). For both our and pretrained models, the attention map of “cat” and “dog” overlap more often. This can be one of the reasons for the failed compositions.

MS-COCO FID evaluation for all models. We also evaluate MS-COCO FID for our models and DreamBooth. As

Table 5. Quantitative results of different choices for modifier token $\boldsymbol { \nabla } ^ { * }$ . We ablate our method with two settings – (1) Ours (w/ $\boldsymbol { \nabla } ^ { * }$ init normal dist.), where we initialize the modifier token randomly with mean and standard deviation of the existing token embeddings, and then optimize during training. (2) Ours (w/o $\boldsymbol { \nabla } ^ { * }$ opt), i.e., not optimizing the token once initialized with the rare occurring token. We observe that not optimizing $\boldsymbol { \nabla } ^ { * }$ leads to worse results on image-alignment, i.e., the model is not able to learn the target concept. Similarly, random initialization with the normal distribution also results in lower image-alignment and higher text-alignment, but as shown in Figure 18, the category word mapping shifts to target images.   

<table><tr><td></td><td>Methods</td><td>Barn (7)</td><td>Tortoise plushy (12)</td><td>Teddy- Bear (7)</td><td>Wooden Pot (4)</td><td>Dog (10)</td><td>Cat (5)</td><td>Flower (10)</td><td>Table (4)</td><td>Chair (4)</td><td>Mean</td></tr><tr><td rowspan="3">Text-alignment</td><td>Ours</td><td>0.791</td><td>0.827</td><td>0.849</td><td>0.796</td><td>0.764</td><td>0.768</td><td>0.800</td><td>0.788</td><td>0.771 0.766</td><td>0.795</td></tr><tr><td>Ours (w v* int normal ist.)</td><td>0.817</td><td>0.809</td><td>0.853</td><td>0.807</td><td>0.781</td><td>0.805</td><td>−0.82</td><td>0.793</td><td></td><td>0.805</td></tr><tr><td>Ours (w/o v* opt)</td><td>0.847</td><td>0.824</td><td>0.864</td><td>0.830</td><td>0.769</td><td>0.801</td><td>0.823</td><td>0.787</td><td>0.807</td><td>0.816</td></tr><tr><td rowspan="3">Image-alignment</td><td>Ours</td><td>0.744</td><td>0.783</td><td>0.829</td><td>0.769</td><td>0.684</td><td>0.848</td><td>0.734</td><td>0.768</td><td>0.814</td><td>0.774</td></tr><tr><td>Ou t oma .</td><td>0.747</td><td>0.780</td><td>0.829</td><td>0.787</td><td>0.670</td><td>0.785</td><td>0.709</td><td>0..72</td><td>0.811</td><td>0.765</td></tr><tr><td>Ours (w/o V* opt)</td><td>0.730</td><td>0.755</td><td>0.784</td><td>0.786</td><td>0.663</td><td>0.757</td><td>0.683</td><td>0.688</td><td>0.757</td><td>0.733</td></tr><tr><td rowspan="3">KID (×103) (Validation)</td><td>Ours</td><td>09.00</td><td>26.82</td><td>40.33</td><td>08.77</td><td>19.46</td><td>27.39</td><td>36.47</td><td>15.77</td><td>17.94</td><td>22.43</td></tr><tr><td>Ours w v it normal st.</td><td>09.99</td><td>21.76</td><td>44.68</td><td>12.27</td><td>15..9</td><td>25.26</td><td>32.97</td><td>15.26</td><td>21.28</td><td>22.0</td></tr><tr><td>Ours (w/o v* opt)</td><td>10.22</td><td>23.75</td><td>41.64</td><td>11.97</td><td>18.19</td><td>26.41</td><td>28.83</td><td>16.56</td><td>19.20</td><td>21.86</td></tr></table>

<table><tr><td></td><td>Methods</td><td>Moongate (135) [44]</td><td>Barn (7)</td><td>Tortoise plushy (12)</td><td>Teddy- Bear (7)</td><td>Wooden Pot (4)</td><td>Dog (10)</td><td>Cat (5)</td><td>Flower (10)</td><td>Table (4)</td><td>Chair (4)</td></tr><tr><td rowspan="2">MS-COCO FID</td><td>Ours</td><td>16.05</td><td>17.21</td><td>16.27</td><td>16.71</td><td>16.70</td><td>16.26</td><td>16.95</td><td>16.71</td><td>16.25</td><td>16.99</td></tr><tr><td>DreamBooth</td><td>17.35</td><td>20.36</td><td>19.61</td><td>19.45</td><td>20.01</td><td>19.10</td><td>20.57</td><td>18.57</td><td>19.39</td><td>19.35</td></tr></table>

Table 6. MS-COCO FID evaluation with fine-tuned models is a standard evaluation metric for text-to-image models. The pretrained stable diffusion model has 16.35 FID with the same setting of 50 DDPM sampling steps, scale 6. Our method for most datasets has a similar FID, which shows that the fine-tuned models are similar to the pretrained model on other unrelated concepts. Since Textual Inversion does not update the model, it has the same FID as the pretrained model.

![](images/52f624c5ca5c85ef3392aced46dd7951f01232100d4a1ec547f326a08207fc0f.jpg)  
Figure 20. Text- and image-alignment scores with training steps (mean across datasets). The text-alignment score gradually decreases as we fine-tune the model on the target images. In the case of Textual Inversion, we observe that the new token embedding introduced in the method results in high image-alignment but the text-alignment remains significantly lower across training iterations. Compared to DreamBooth, our method at convergence has higher image text- and image-alignment.

shown in Table 6, our method has lower FID and only slightly worse occasionally compared to the 16.35 FID of the pretrained model. This suggests our method doesn’t change the generated image distribution on unrelated concepts. We measure FID [28] using clean-fid library [54].

# E. Implementation and Experiment Details

We describe additional training details for our method and baselines [21, 65].

Datasets. All datasets in the paper were captured manually or downloaded from Unsplash except Moongate [44].

Custom Diffusion (ours). As mentioned in Section 3.2, we train with a batch size of 8 and learning rate $1 0 ^ { - 5 }$ , which is scaled by batch size for an effective learning rate of $8 \times 1 0 ^ { - 5 }$ We train for 250 steps for single-concept experiments and 500 steps for multi-concept. During training, we randomly resize the target images to $1 . 2 - 1 . 4 \times$ every 1 out of 3 times and append zoomed in or close up to the text prompt. The rest of the time the target image is randomly resized to $0 . 4 -$ $1 . 0 \times$ and if the resize ratio is less than 0.6 we append far away or very small to the text prompt, and only propagate the loss in the valid image region. Since fine-tuning is done only for a few iterations, we do not notice any augmentation leaking in the fine-tuned model. We also detach the start token embedding during fine-tuning. For selecting the rare token as the modifier token $\boldsymbol { \nabla } ^ { * }$ , we count the occurrence of the total 49408 tokens in $2 0 0 K$ captions sampled from the LAION-400M dataset. We then select the token with $\sim 5 - 1 0$ occurrences, with alphabetic representation, and not a substring of another token. During training, we oversample the target images to keep the ratio of regularization samples (200) and target samples the same.

Table 7. Quantitative evaluation on single-concept fine-tuning. First and Second row: we show the text- and image-alignment in CLIP feature space (higher is better for both). All metrics are calculated with 1K generated samples across 20 prompts for each dataset. Our method performs better than baselines when averaged across datasets. We show the trend with training steps in Figure 20. Bottom row: KID between real validation set images (500) and generated images (1K) with the same caption. Since our method uses a regularization set of real images, it achieves lower KID than baselines and even improves slightly over the pretrained model except on “Table” and “Chair”. Textual Inversion has the same KID as the pretrained model since the diffusion model is not updated in the method. We also evaluate our models with FID [28] on MS-COCO [43] in Table 6 in the Appendix. We use the DDPM sampler with 50 steps and scale 6 for both metrics. The number of training images is shown in brackets.   

<table><tr><td></td><td>Methods</td><td>Moongate (135) [44]</td><td>Barn (7)</td><td>Tortoise plushy (12)</td><td>Teddy- Bear (7)</td><td>Wooden Pot (4)</td><td>Dog (10)</td><td>Cat (5)</td><td>Flower (10)</td><td>Table (4)</td><td>Chair (4)</td><td>Mean</td></tr><tr><td rowspan="4">Text-alignment</td><td>Textual Inversion DreamBooth</td><td>0.658 0.778</td><td>0.739 0.839</td><td>0.646 0.789</td><td>0.713 0.850</td><td>0.726 0.791</td><td>0.613 0.764</td><td>0.643 0.789</td><td>0.648 0.824</td><td>0.651 0.715</td><td>0.658 0.676</td><td>0.670 0.781</td></tr><tr><td>Ours (w/ fine-tune all)</td><td>0.762</td><td>0.782</td><td></td><td></td><td>0.820</td><td>0.752</td><td>0.792</td><td>0.803</td><td>0.763</td><td>0.767</td><td>0.795</td></tr><tr><td>Ours</td><td></td><td>0.791</td><td>0.822 0.828</td><td>0.854 0.848</td><td>0.798</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>0.772</td><td></td><td></td><td></td><td></td><td>0.764</td><td>0.768</td><td>0.800</td><td>0.787</td><td>0.771</td><td>0.795</td></tr><tr><td rowspan="4">Image-alignment</td><td>Textual Inversion DreamBooth</td><td>0.740</td><td>0.753</td><td>0.755</td><td>0.885</td><td>0.802</td><td>0.760</td><td>0.909</td><td>0.815</td><td>0.891</td><td>0.872</td><td>0.827</td></tr><tr><td>Ours (w/ fine-tune all)</td><td>0.608</td><td>0.679</td><td>0.847</td><td>0.822</td><td>0.788</td><td>0.674</td><td>0.786</td><td>0.688</td><td>0.846</td><td>0.863</td><td>0.776</td></tr><tr><td>Ours</td><td>0.662</td><td>0.6999</td><td>0.822</td><td>0.804</td><td>0.67</td><td>0.669</td><td>0.810</td><td>0.670</td><td>0.725</td><td>0.772</td><td>0.748</td></tr><tr><td></td><td>0.665</td><td>0.744</td><td>0.784</td><td>0.829</td><td>0.769</td><td>0.684</td><td>0.849</td><td>0.735</td><td>0.767</td><td>0.814</td><td>0.775</td></tr><tr><td rowspan="4">KID (×103) (Validation)</td><td>Pretrained Model</td><td>11.33</td><td>11.18</td><td>33.51</td><td>39.82</td><td>12.88</td><td>25.40</td><td>35.01</td><td>30.96</td><td>13.35</td><td>9.26</td><td>22.27</td></tr><tr><td>Textual Inversion</td><td>11.33</td><td>11.18</td><td>33.51</td><td>39.82</td><td>12.88</td><td>25.40</td><td>35.01</td><td>30.96</td><td>13.35</td><td>9.26</td><td>22.27</td></tr><tr><td>DreamBooth</td><td>20.80</td><td>12.975</td><td>36.45</td><td>42.93</td><td>15.80</td><td>44.14</td><td>67.41</td><td>37.55</td><td>21.07</td><td>26.16</td><td>32.53</td></tr><tr><td>Ours (w/ fine-tune all) Ours</td><td>9.04 7.65</td><td>9.35 9.00</td><td>28.55 26.82</td><td>42.78 40.33</td><td>10.60 8.77</td><td>19.58 19.46</td><td>14.79 27.39</td><td>22.95 36.47</td><td>18.69 15.77</td><td>16.34 17.94</td><td>19.27 20.96</td></tr></table>

Table 8. Text-alignment and image-alignment with the two target concepts in CLIP feature space on multi-concept fine-tuning. We evaluate each composition pair on 400 images generated using 8 prompts with 200 steps of DDPM sampler and scale ${ = } 6$ . A high score on one metric at the cost of worse performance on other metrics leads to overall worse results, as our qualitative samples show as well. Our method performs better than concurrent methods on the average metric in all settings except Table $^ +$ Chair, where most methods perform comparably.   

<table><tr><td colspan="2">Methods</td><td>Moongate + Dog</td><td>Cat + Chair</td><td></td><td>- Wooden Pot + Cat Wooden Pot + Flower</td><td></td><td>Table + Chair</td><td>Mean</td></tr><tr><td rowspan="6">gnment</td><td colspan="2">DreamBooth</td><td>0.767</td><td>0.783</td><td>0.774</td><td>0.860</td><td>0.732</td><td>0.783</td></tr><tr><td colspan="2">Textual Inversion</td><td>0.538</td><td>0.445</td><td>0.542</td><td>0.552</td><td>0.644</td><td>0.544</td></tr><tr><td rowspan="3"></td><td>w/ fine-tune all</td><td>0.797</td><td>0.742</td><td>0.800</td><td>0.856</td><td>0.745</td><td>0.787</td></tr><tr><td>Sequential Ours</td><td>0.785</td><td>0.736</td><td>0.863</td><td>0.862</td><td>0.740</td><td>0.797</td></tr><tr><td>Optimization</td><td>0.786</td><td>0.774</td><td>0.806</td><td>0.870</td><td>0.766</td><td>0.800</td></tr><tr><td rowspan="6"></td><td colspan="2">Joint</td><td>0.793</td><td>0.798</td><td>0.833</td><td>0.845</td><td>0.737</td><td>0.801</td></tr><tr><td colspan="2">DreamBooth</td><td>0.492</td><td>0.715</td><td>0.719</td><td>0.697</td><td>0.817</td><td>0.688</td></tr><tr><td rowspan="3"></td><td>Textual Inversion</td><td>0.675</td><td>0.571</td><td>0.539</td><td>0.531</td><td>0.822</td><td>0.627</td></tr><tr><td>w/ fine-tune all</td><td>0.584</td><td>0.736</td><td>0.59</td><td>0.699</td><td>0.817</td><td>0.686</td></tr><tr><td>Sequential Ours</td><td>0.534</td><td>0.696</td><td>0.636</td><td>0.671</td><td>0.833</td><td>0.674</td></tr><tr><td rowspan="3"></td><td>Optimization</td><td>0.583 0.592</td><td>0.792 0.674</td><td>0.580</td><td>0.683</td><td></td><td>0.786</td><td>0.684</td></tr><tr><td>Joint</td><td></td><td></td><td>0.654</td><td></td><td>0.758</td><td>0.837</td><td>0.702</td></tr><tr><td colspan="2">DreamBooth</td><td>0.656</td><td>0.737</td><td>0.633</td><td>0.633</td><td>0.839</td><td>0.699</td></tr><tr><td rowspan="5">Image-alignment, (target2)</td><td rowspan="5"></td><td>Textual Inversion</td><td>0.473</td><td>0.614</td><td>0.673</td><td>0.580</td><td>0.831</td><td>0.634</td></tr><tr><td>w/ fine-tune all</td><td>0.592</td><td>0.646</td><td>0.815</td><td>0.644</td><td>0.783</td><td>0.695</td></tr><tr><td>Sequential Ours</td><td>0.641</td><td>0.762</td><td>0.748</td><td>0.660</td><td>0.819</td><td>0.725</td></tr><tr><td>Optimization</td><td>0.598</td><td>0.639</td><td>0.819</td><td>0.675</td><td>0.803</td><td>0.706</td></tr><tr><td>Joint</td><td>0.582</td><td>0.767</td><td>0.757</td><td>0.640</td><td>0.807</td><td>0.710</td></tr></table>

Ours (w/ fine-tune all). When fine-tuning all parameters, we reduce the learning rate to $8 \times 1 0 ^ { - 6 }$ , which works better than $8 \times 1 0 ^ { - 5 }$ and train for 500 steps with a batch size of 8. For multi-concept, we train for 1000 iterations. The rest of the settings are the same as our final method.

Textual Inversion [21] We train with the recommended batch size of 8, a learning rate of 0.005 (scaled by batch size for an effective learning rate of 0.04) for 5000 steps. The new token is initialized with the category word, e.g., “dog”. In cases when the category word is represented by multiple tokens, e.g., “tortoise plushy”, we use a single word approximation like “plush”, similarly “gate” for “moongate”, “pot” for “wooden pot”, and “bear” for “teddybear”.

DreamBooth [65] We use the third-party implementation [84] of DreamBooth. Training is done with the frozen text transformer and fine-tuning the U-net diffusion model with a batch size of 8 and learning rate $1 0 ^ { - 6 }$ (without scaling with batch size and $\# \mathrm { G P U s }$ ). The text prompt used for target images is photo of [V] category where we initialize [V] with the same rare occurring token-id 42170 as ours. The regularization images are generated with 50 steps of the DDPM sampler with the text prompt photo of a {category}. We train for 2500 steps for single-concept. For multi-concept, we train for 5000 steps but pick the best checkpoint at 3000 iterations. For results on CustomConcept101 dataset which we updated later in Appendix A, we trained DreamBooth with the learning rate of $5 \times 1 0 ^ { - 6 }$ as suggested in their paper and batch-size 4. The training was done for 1000 iterations for single-concept and 2000 iterations for multi-concept.

# F. Societal Impact

While training massive-scale diffusion models is inaccessible to most people, our method of fine-tuning pretrained models can help democratize such models to everyday users. Users can customize these models according to their own personal images, artworks, and objects of interest. Being compute and memory efficient, it will increase the accessibility and usage of large-scale models by individual users as well as enable easy collaboration for sharing millions of finetuned concepts and their compositions. At the same time, the dangers of generative technology accompany our method as well. Possible ways to mitigate this is the reliable detection of fake generated data, which has been studied in the context of GANs [11, 79] and, recently, diffusion models [13].

# G. Change log

v1: Original draft.

v2: CustomConcept101 dataset details and results in Appendix A, Figure 12, 13, and Table 4. Updated citations and results with three concept compositions in Figure 16.

![](images/621e27852d963e46369c6a789d02d01fe3cacea96a06d6c519a1c6fb734b014a.jpg)  
Figure 21. Qualitative results on fine-tuned model’s compression for reduced storage requirements. We save the low-rank approximation of the difference between the pretrained model and fine-tuned model updated weights. The storage requirements of models from left to right are 75MB, 15MB, 5MB, 1MB, 0.1MB, and 0.08MB (to save the optimized $\boldsymbol { \nabla } ^ { * }$ ). Even with $5 \times$ compression with top $6 0 \%$ singular values, the performance remains similar. Top ${ \bf k } \%$ implies singular values till the rank where cumulative sum is ${ \bf k } \%$ of total sum of singular values. As we increase the compression, the image-alignment score decreases, as evident from sample generations not being similar to target images, especially in the case of tortoise plushy, teddybear, and cat.

![](images/21612b72644a2bfd205ba589cc32708d450b1ac33d385ef0d1b037a97f03a022.jpg)  
Figure 22. Multi-concept composition using Textual Inversion. We observe that Textual Inversion struggles with the composition of two fine-tuned objects as shown in the above sample generations as well.

![](images/6e4a5e5c17cfc372a020069fc7c3bcac3260ee2ffe7afdc1ed6fb24c34b4a45b.jpg)  
Figure 23. Generating target images with long text prompts in the pretrained model. We show that even with long text descriptions the pretrained model struggles to generate exact target images. Thus, to generate the target images, we need model fine-tuning.

![](images/fad8ea948b62b6ac86dc831f24285482caee55f9f2f4f87419c6b310f3f90972.jpg)  
Figure 24. Overfitting on the training prompt template. Since during fine-tuning, target images are trained with the text prompt, photo of a $\boldsymbol { \nabla } ^ { * }$ {category}, we show here random sample generations for the text prompt, photo of a {category}, in both ours and ours (w/o reg) case. As shown, the generations shift towards the target images and have less diversity compared to the pretrained model. Between ours and ours (w/o reg), our method has less shift and is more diverse on average.