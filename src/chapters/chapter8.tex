% Chapter 8: Hardness of Optimality

The runtime of \autoref{thm:aqo-runtime},
\[
T = O\!\left(\frac{1}{\varepsilon}\cdot\frac{\sqrt{A_2}}{\Delta^2\, A_1(A_1+1)}\cdot\sqrt{\frac{N}{d_0}}\right),
\]
is achieved by an adaptive schedule that concentrates evolution time near the avoided crossing and accelerates elsewhere. The schedule places a slow phase in the window $[s^* - \delta_s,\, s^*)$ centered at the crossing position $s^* = A_1/(A_1+1)$, where the spectral gap reaches its minimum. The parameters $A_2$ and $d_0$ enter only through the ratio $\sqrt{A_2/d_0}$ and can be replaced by conservative bounds ($A_2 \geq 1$, $d_0 = 1$) at the cost of polynomial slowdown. The critical parameter is $A_1$: it determines where the crossing occurs, and the window width $\delta_s = O(\sqrt{d_0/(NA_2)}) = O(2^{-n/2})$ sets the required precision. An error larger than $\delta_s$ in the crossing position causes the algorithm to evolve rapidly through the gap minimum, destroying the ground-state fidelity.

Computing $A_1$ from the problem Hamiltonian $H_z$ is a classical pre-computation. Given the $N = 2^n$ diagonal entries of $H_z$, the task is to evaluate $A_1 = (1/N)\sum_{k=1}^{M-1} d_k/(E_k - E_0)$ to additive accuracy $O(2^{-n/2})$. The brute-force approach --- enumerating all eigenvalues, sorting, and summing --- takes $O(N)$ time, precisely the cost of classical unstructured search. If the pre-computation is as expensive as the problem itself, the quadratic speedup becomes conditional: the adiabatic algorithm is fast, provided someone has already done the slow part. Throughout this chapter, we write $A_1(H)$ to make the dependence on the Hamiltonian explicit when multiple Hamiltonians are under consideration.

Estimating $A_1$ to the much coarser precision $1/\text{poly}(n)$ is already NP-hard: two queries to an $A_1$-oracle suffice to solve 3-SAT (\autoref{sec:np-hard-A1}). Computing $A_1$ exactly, or to exponentially small precision $O(2^{-\text{poly}(n)})$, is $\#$P-hard: polynomial interpolation extracts all degeneracies $d_k$ from polynomially many queries (\autoref{sec:sharp-P-hard-A1}). At the algorithmically relevant precision $2^{-n/2}$, the interpolation technique breaks down, but a quantum algorithm achieves $O(2^{n/2})$ queries while any classical algorithm requires $\Omega(2^n)$, yielding a Grover-type quadratic separation (\autoref{sec:intermediate-A1}).


\section{NP-Hardness of Estimating $A_1$}
\label{sec:np-hard-A1}

The Hamiltonian $H_z$ encodes an optimization problem whose ground energy $E_0$ determines whether a solution exists. For a 3-SAT instance, $E_0 = 0$ when a satisfying assignment exists and $E_0 \geq 1/\text{poly}(n)$ otherwise. Distinguishing these two cases is the local Hamiltonian promise problem, known to be NP-hard \cite{kempe2006complexity}. The spectral parameter $A_1$ is not obviously related to this decision problem --- it aggregates information about all energy levels, not just the ground energy. A modified Hamiltonian $H'$ creates a bridge: comparing $A_1(H')$ with $A_1(H)$ reveals whether $E_0$ vanishes.

Define the $(n+1)$-qubit Hamiltonian
\begin{equation}
\label{eq:modified-ham}
H' = H \otimes \frac{I + \sigma_z}{2}.
\end{equation}
The operator $(I+\sigma_z)/2$ is the projector onto $\ket{0}$ for the ancilla qubit: it has eigenvalue $1$ on $\ket{0}$ and eigenvalue $0$ on $\ket{1}$. On the $\ket{0}$ branch, $H'$ has the same spectrum as $H$: eigenvalues $E_k$ with degeneracies $d_k$. On the $\ket{1}$ branch, $H'$ annihilates every state, contributing $2^n$ eigenvalues at energy $0$. The ground energy of $H'$ is therefore always zero, regardless of $E_0(H)$. This invariance is the mechanism: $A_1(H')$ measures the spectrum from a fixed reference point $E_0' = 0$, while $A_1(H)$ measures from the variable reference $E_0(H)$. When $E_0(H) > 0$, the two measurements diverge, and the divergence is detectable.

\begin{lemma}[Disambiguation {\cite{braida2024unstructured}}]
\label{lem:disambiguation}
Let $\varepsilon, \mu_1, \mu_2 \in (0,1)$. Suppose $\mathcal{C}_\varepsilon$ is a procedure that accepts the description of a Hamiltonian $H$ and outputs $\widetilde{A}_1(H)$ with $|\widetilde{A}_1(H) - A_1(H)| \leq \varepsilon$. Let $H$ be an $n$-qubit diagonal Hamiltonian with eigenvalues $0 \leq E_0 < E_1 < \cdots < E_{M-1} \leq 1$ and $M \in \text{poly}(n)$, such that either \textnormal{(i)} $E_0 = 0$ or \textnormal{(ii)} $\mu_1 \leq E_0 \leq 1 - \mu_2$. Then two calls to $\mathcal{C}_\varepsilon$ suffice to decide between \textnormal{(i)} and \textnormal{(ii)}, provided
\begin{equation}
\label{eq:disambiguation-bound}
\varepsilon < \frac{\mu_1}{6(1-\mu_1)} - \frac{d_0}{6N} \cdot \frac{1}{\mu_1 \mu_2}.
\end{equation}
\end{lemma}

\begin{proof}
Call $\mathcal{C}_\varepsilon$ on $H$ and on $H'$ defined by Eq.~\eqref{eq:modified-ham}, obtaining estimates $\widetilde{A}_1(H)$ and $\widetilde{A}_1(H')$. The test statistic is $\widetilde{A}_1(H) - 2\widetilde{A}_1(H')$, where the factor $2$ compensates for the doubling of the Hilbert space ($H'$ acts on $2^{n+1}$ states, so $A_1(H')$ carries a normalization factor $1/2^{n+1}$ instead of $1/2^n$).

\textbf{Case (i): $E_0 = 0$.} The ground energy of $H'$ is $0$ with degeneracy $d_0 + 2^n$, and the excited levels of $H'$ are $E_1, \ldots, E_{M-1}$ with degeneracies $d_1, \ldots, d_{M-1}$. Since $E_0 = 0$, both $A_1(H)$ and $A_1(H')$ sum over the same gaps $E_k - 0 = E_k$:
\[
A_1(H) = \frac{1}{2^n}\sum_{k=1}^{M-1}\frac{d_k}{E_k}, \qquad A_1(H') = \frac{1}{2^{n+1}}\sum_{k=1}^{M-1}\frac{d_k}{E_k}.
\]
Therefore $A_1(H) - 2A_1(H') = 0$, and the test statistic satisfies $|\widetilde{A}_1(H) - 2\widetilde{A}_1(H')| \leq 3\varepsilon$.

\textbf{Case (ii): $\mu_1 \leq E_0 \leq 1 - \mu_2$.} The ground energy of $H'$ is still $0$ (from the $\ket{1}$ branch), but now $E_0, E_1, \ldots, E_{M-1}$ are all excited levels. Thus
\[
A_1(H') = \frac{1}{2^{n+1}}\sum_{k=0}^{M-1}\frac{d_k}{E_k}.
\]
Decompose $A_1(H)$ using the partial fraction identity $d_k/(E_k - E_0) = d_k/E_k + d_k E_0/(E_k(E_k - E_0))$:
\begin{align}
A_1(H) &= \frac{1}{2^n}\sum_{k=1}^{M-1}\frac{d_k}{E_k - E_0} = \frac{1}{2^n}\sum_{k=1}^{M-1}\frac{d_k}{E_k} + \frac{E_0}{2^n}\sum_{k=1}^{M-1}\frac{d_k}{E_k(E_k - E_0)} \nonumber \\
&= \frac{1}{2^n}\sum_{k=0}^{M-1}\frac{d_k}{E_k} - \frac{d_0}{2^n E_0} + \frac{E_0}{2^n}\sum_{k=1}^{M-1}\frac{d_k}{E_k(E_k - E_0)}.
\label{eq:A1-decomposition}
\end{align}
The first sum equals $2A_1(H')$. For the remainder sum, bound each denominator from above: $E_k(E_k - E_0) \leq 1 \cdot (1 - E_0)$ since $E_k \leq E_{M-1} \leq 1$ by normalization and $E_k - E_0 \leq 1 - E_0$ follows. This gives
\[
\frac{E_0}{2^n}\sum_{k=1}^{M-1}\frac{d_k}{E_k(E_k - E_0)} \geq \frac{E_0}{1 - E_0}\cdot\frac{1}{2^n}\sum_{k=1}^{M-1}d_k = \frac{E_0}{1 - E_0}\left(1 - \frac{d_0}{N}\right).
\]
Combining with Eq.~\eqref{eq:A1-decomposition}:
\begin{align}
A_1(H) - 2A_1(H') &\geq \frac{E_0}{1-E_0}\left(1 - \frac{d_0}{N}\right) - \frac{d_0}{NE_0} \nonumber \\
&= \frac{E_0}{1-E_0} - \frac{d_0}{N}\cdot\frac{1 - E_0 + E_0^2}{E_0(1-E_0)}.
\label{eq:disambiguation-gap}
\end{align}
Since $1 - E_0 + E_0^2 \leq 1$ and $E_0(1-E_0) \geq \mu_1\mu_2$ on the given range, the fraction $(1 - E_0 + E_0^2)/(E_0(1-E_0))$ is at most $1/(\mu_1\mu_2)$. The first term $E_0/(1-E_0)$ is increasing in $E_0$, so it is at least $\mu_1/(1-\mu_1)$. Therefore
\[
A_1(H) - 2A_1(H') \geq \frac{\mu_1}{1-\mu_1} - \frac{d_0}{N}\cdot\frac{1}{\mu_1\mu_2},
\]
and the test statistic satisfies
\[
\widetilde{A}_1(H) - 2\widetilde{A}_1(H') \geq \frac{\mu_1}{1-\mu_1} - \frac{d_0}{N\mu_1\mu_2} - 3\varepsilon.
\]
The two cases are distinguished when $3\varepsilon$ from case (i) is separated from the lower bound in case (ii), requiring $6\varepsilon < \mu_1/(1-\mu_1) - d_0/(N\mu_1\mu_2)$.
\end{proof}

The disambiguation succeeds whenever the positive correction $E_0/(1-E_0)$ from the partial fraction identity dominates the negative term $-d_0/(NE_0)$, which happens as long as $d_0/N$ is small relative to $\mu_1^2\mu_2$. For the Ising Hamiltonians of interest, $d_0/N$ is exponentially small in $n$, so the condition is easily satisfied.

\begin{theorem}[NP-hardness of $A_1$ estimation {\cite{braida2024unstructured}}]
\label{thm:np-hard-A1}
Computing $A_1(H)$ to additive accuracy
\[
\varepsilon < \frac{1}{72(n-1)}
\]
for a $3$-local Hamiltonian $H$ on $n$ qubits is NP-hard.
\end{theorem}

\begin{proof}
We reduce 3-SAT to ground-energy disambiguation, following the construction of \cite{garey1976simplified, braida2024unstructured}. Let $\varphi$ be a 3-SAT formula on $n_\text{var}$ Boolean variables $x_0, \ldots, x_{n_\text{var}-1}$ with $m$ clauses, each of the form $a_k \lor b_k \lor c_k$ where each literal is some $x_l$ or $\bar{x}_l$. If $n_\text{var} + m < 15$, solve by brute force. Otherwise, define the single-qubit projectors
\[
P_{x_l} = \frac{I - \sigma_z^{(l)}}{2}, \qquad P_{\bar{x}_l} = \frac{I + \sigma_z^{(l)}}{2},
\]
which project onto the $\ket{1}$ and $\ket{0}$ states of qubit $l$, respectively. For each clause $k$ ($0 \leq k < m$), introduce an auxiliary qubit at index $n_\text{var} + k$ and define
\begin{align}
H_k &= P_{\bar{a}_k} + P_{\bar{b}_k} + P_{\bar{c}_k} + P_{\bar{x}_{n_\text{var}+k}} \nonumber \\
&\quad + P_{a_k}P_{b_k} + P_{a_k}P_{c_k} + P_{b_k}P_{c_k} \nonumber \\
&\quad + P_{\bar{a}_k}P_{x_{n_\text{var}+k}} + P_{\bar{b}_k}P_{x_{n_\text{var}+k}} + P_{\bar{c}_k}P_{x_{n_\text{var}+k}}.
\label{eq:clause-ham}
\end{align}
Direct computation on the computational basis shows that the minimum eigenvalue of $H_k$ is $3$ when clause $k$ is satisfied and $4$ when it is not; the maximum eigenvalue is $6$. The combined Hamiltonian on $2n_\text{var} + 2m$ qubits is
\begin{equation}
\label{eq:3sat-ham}
H = \frac{1}{6m}\sum_{k=0}^{m-1}H_k + \frac{1}{2n_\text{var}+2m}\sum_{j=n_\text{var}+m}^{2n_\text{var}+2m-1}P_{x_j} - \frac{1}{2}I.
\end{equation}
The first sum normalizes the clause energies to $[1/2, 1]$; the second sum adds $n_\text{var} + m$ free qubits whose projectors prefer $\ket{0}$; the identity shift places the eigenvalues in $[0, 1]$. When all clauses are satisfied, there exists an assignment making every $H_k$ achieve its minimum, giving $E_0 = 0$. When some clause is unsatisfied, the minimum of $\sum H_k/(6m)$ increases by at least $1/(6m)$, giving $E_0 \geq 1/(6m)$.

Apply \autoref{lem:disambiguation} with $\mu_1 = 1/(6m)$ and $\mu_2 = 1/2$. The number of eigenvalues is $N = 2^{2n_\text{var}+2m}$ and the ground-state degeneracy satisfies $d_0 \leq 2^{n_\text{var}+m}$, so $d_0/N \leq 2^{-(n_\text{var}+m)}$. The precision requirement in Eq.~\eqref{eq:disambiguation-bound} becomes
\begin{align}
\varepsilon &< \frac{1}{6}\cdot\frac{1}{6m-1} - \frac{12m}{6}\cdot\frac{d_0}{N} \nonumber \\
&\leq \frac{1}{36(n_\text{var}+m-1)} - \frac{2m}{2^{n_\text{var}+m}}.
\label{eq:precision-arithmetic}
\end{align}
For $n_\text{var}+m \geq 15$, the second term satisfies $2m/2^{n_\text{var}+m} \leq 1/(72(n_\text{var}+m-1))$, so
\[
\varepsilon < \frac{1}{72(n_\text{var}+m-1)}.
\]
The Hamiltonian $H'$ from Eq.~\eqref{eq:modified-ham} acts on $n = 2n_\text{var}+2m+1$ qubits and is $3$-local (since $H$ is $2$-local and the tensor product with $(I+\sigma_z)/2$ adds one ancilla). Since $n_\text{var}+m \leq n$, the precision bound $\varepsilon < 1/(72(n-1))$ follows.
\end{proof}

For the running example ($M = 2$, Grover search), the spectral parameter $A_1 = (N-1)/N$ is trivially known from the problem description: there are only two energy levels, and the degeneracies are determined by the number of marked items. The NP-hardness arises from Hamiltonians encoding combinatorial problems with polynomially many energy levels and exponentially small ground-energy gaps, where $A_1$ depends on the full degeneracy structure in a non-trivial way.

\textbf{Remark.} The disambiguation technique extends beyond 3-SAT. The MaxCut decision problem --- given a graph $G = (V,E)$ and integer $k$, does $G$ have a cut of size at least $k$? --- also reduces to $A_1$ estimation. The construction adds a weighted edge to $G$, creating an auxiliary Hamiltonian $H'$ whose $A_1$ value differs from a reference by at least $1/(|E|(|E|-1))$ between the two cases. This yields NP-hardness at precision $2/(5n^4)$ with a $2$-local Hamiltonian, sharpening the locality requirement from $3$-local to $2$-local at the cost of a slightly tighter precision bound.


\section{$\#$P-Hardness of Computing $A_1$ Exactly}
\label{sec:sharp-P-hard-A1}

NP-hardness captures the decision problem: is $E_0 = 0$? But $A_1$ encodes more than a single bit. The spectral parameter is a weighted sum over all energy levels, and its exact value determines every degeneracy $d_k$. Extracting these degeneracies solves counting problems --- $d_0$ for an NP-complete Hamiltonian counts the number of satisfying assignments --- and counting is harder than deciding: it is $\#$P-complete \cite{valiant1979complexity}.

The extraction uses a parametrized family of Hamiltonians that shifts the spectrum continuously, turning $A_1$ into a rational function whose poles carry the degeneracies as residues. For a parameter $x > 0$, define the $(n+1)$-qubit Hamiltonian
\begin{equation}
\label{eq:param-ham}
H'(x) = H \otimes I - \frac{x}{2}\, I \otimes \frac{I + \sigma_z^{(n+1)}}{2}.
\end{equation}
On the $\ket{0}$ branch of the ancilla, the eigenvalues are $E_k - x/2$ with degeneracies $d_k$. On the $\ket{1}$ branch, the eigenvalues are $E_k$ with degeneracies $d_k$. The ground energy is $E_0 - x/2$ (from the $\ket{0}$ branch, for $x > 0$). The gaps relative to this ground energy are $\Delta_k = E_k - E_0$ for the $\ket{0}$ branch and $\Delta_k + x/2$ for the $\ket{1}$ branch.

Computing $A_1(H'(x))$ from these gaps and defining $f(x) = 2A_1(H'(x)) - A_1(H)$ isolates the $\ket{1}$-branch contribution \cite{braida2024unstructured}:
\begin{equation}
\label{eq:f-function}
f(x) = \frac{1}{N}\sum_{k=0}^{M-1}\frac{d_k}{\Delta_k + x/2}.
\end{equation}
This function is a sum of $M$ simple poles at $x = -2\Delta_k$. Each pole has residue $2d_k/N$, encoding the degeneracy of the corresponding energy level. The function $f$ is a partial-fraction decomposition of the entire degeneracy spectrum. The extraction problem reduces to recovering these residues from evaluations of $f$.

\begin{lemma}[Exact degeneracy extraction {\cite{braida2024unstructured}}]
\label{lem:exact-degeneracy}
Suppose $\mathcal{C}$ is a procedure that computes $A_1(H)$ exactly for any $n$-qubit diagonal Hamiltonian $H$. Let $H_\sigma$ be an Ising Hamiltonian (\autoref{eq:Ising-Ham}) with integer eigenvalues and known spectral gaps $\Delta_k = E_k - E_0$. Then $O(\text{poly}(n))$ calls to $\mathcal{C}$ suffice to compute all degeneracies $d_0, d_1, \ldots, d_{M-1}$.
\end{lemma}

\begin{proof}
Each evaluation of $f(x_i)$ requires two calls to $\mathcal{C}$: one for $A_1(H)$ and one for $A_1(H'(x_i))$. Evaluate $f$ at $M$ distinct positive odd integers $x_i \in \{1, 3, \ldots, 2M-1\}$. These values avoid the poles: for each $k$, $\Delta_k + x_i/2 \geq 0 + 1/2 > 0$ since $\Delta_k \geq 0$ and $x_i \geq 1$. The total cost is $2M = O(\text{poly}(n))$ oracle calls.

Define the reconstruction polynomial
\begin{equation}
\label{eq:recon-poly}
P(x) = \prod_{k=0}^{M-1}\left(\Delta_k + \frac{x}{2}\right) f(x) = \frac{1}{N}\sum_{k=0}^{M-1} d_k \prod_{\ell \neq k}\left(\Delta_\ell + \frac{x}{2}\right).
\end{equation}
Multiplying $f(x)$ by the product of all denominators clears the poles, yielding a polynomial of degree at most $M - 1$ in $x$. Since the gaps $\Delta_k$ are known integers, the values $P(x_i) = \prod_k(\Delta_k + x_i/2) \cdot f(x_i)$ are computable from the oracle outputs. The $M$ values $P(x_1), \ldots, P(x_M)$ determine $P$ uniquely by Lagrange interpolation \cite{phillips2003interpolation}: a polynomial of degree at most $M - 1$ is determined by $M$ distinct evaluations.

The degeneracies are recovered by evaluating $P$ at the poles. Setting $x = -2\Delta_k$ kills every factor $(\Delta_\ell + x/2)$ except the $k$-th, giving
\begin{equation}
\label{eq:degeneracy-extraction}
d_k = \frac{N \cdot P(-2\Delta_k)}{\displaystyle\prod_{\ell \neq k}(\Delta_\ell - \Delta_k)}, \qquad k \in \{0, \ldots, M-1\}.
\end{equation}
The denominator is nonzero because the eigenvalues are distinct. The entire computation (oracle calls, Lagrange interpolation, pole evaluation) runs in $O(\text{poly}(n))$ time.
\end{proof}

Extracting $d_0$ from an Ising Hamiltonian encoding a 3-SAT formula counts the number of satisfying assignments, solving $\#$3-SAT. Since $\#$3-SAT is $\#$P-complete \cite{valiant1979complexity}, an exact $A_1$ oracle would solve every problem in $\#$P in polynomial time. The degeneracies also determine the output probability of an IQP circuit \cite{movassagh2023hardness}: from the $d_k$ and $\Delta_k$, one computes $|\langle 0^n | C_\text{IQP} | 0^n \rangle|^2 = |N^{-1}\sum_k d_k\, e^{i\Delta_k}|^2$, which is itself $\#$P-hard. The NP-hardness of \autoref{sec:np-hard-A1} uses a $3$-local Hamiltonian (the ancilla qubit raises the locality by one). The $\#$P-hardness holds for $2$-local Ising Hamiltonians, since the parametrized construction in Eq.~\eqref{eq:param-ham} preserves $2$-locality when $H$ is $2$-local.

The exact oracle is unrealistic. A robust version of \autoref{lem:exact-degeneracy} must tolerate additive noise $\varepsilon$ in the oracle outputs. Paturi's amplification lemma controls how pointwise bounds on a polynomial propagate across an interval.

\begin{lemma}[Paturi {\cite{paturi1992}}]
\label{lem:paturi}
Let $P(x)$ be a polynomial of degree at most $M$ satisfying $|P(i)| \leq c$ for all integers $i \in \{0, 1, \ldots, M\}$. Then $|P(x)| \leq c \cdot 2^M$ for all $x \in [0, M]$.
\end{lemma}

Paturi's lemma bounds the growth of a polynomial between its sample points: a polynomial bounded by $c$ at $M+1$ integer points can exceed $c$ by at most a factor $2^M$ on the interval. When applied to the difference between the exact and approximate reconstruction polynomials, it yields a controlled error on the interpolation interval. The oracle noise $\varepsilon$ propagates to $f$ as $|\tilde{f}(x_i) - f(x_i)| \leq 3\varepsilon$ (three oracle calls contribute), then to the polynomial samples as $|\tilde{P}(x_i) - P(x_i)| \leq 3\varepsilon \prod_k(\Delta_k + x_i/2)$. The product is at most $B^M$ where $B = \Delta_{\max} + M = \text{poly}(n)$, so each sample has error at most $3\varepsilon B^M$.

\begin{lemma}[Approximate degeneracy extraction {\cite{braida2024unstructured}}]
\label{lem:approx-degeneracy}
Under the same hypotheses as \autoref{lem:exact-degeneracy}, but with an oracle $\mathcal{C}_\varepsilon$ satisfying $|\widetilde{A}_1(H) - A_1(H)| \leq \varepsilon$: for sufficiently small $\varepsilon \in O(2^{-\text{poly}(n)})$, all degeneracies $d_k$ can be computed exactly by $O(\text{poly}(n))$ calls to $\mathcal{C}_\varepsilon$.
\end{lemma}

\begin{proof}[Proof sketch]
The approximate polynomial $\tilde{P}$ is the Lagrange interpolant through the noisy values $(\tilde{P}(x_1), \ldots, \tilde{P}(x_M))$. Its difference $D = \tilde{P} - P$ is a polynomial of degree at most $M - 1$ bounded by $3\varepsilon B^M$ at the sample points. By Paturi's lemma (\autoref{lem:paturi}), $|D(x)| \leq 3\varepsilon B^M \cdot 2^{M-1}$ on the interpolation interval. At the pole evaluation points $x^* = -2\Delta_k$, which lie outside the interval $[1, 2M-1]$, the error is bounded by the Lagrange basis amplification:
\[
|D(x^*)| \leq 3\varepsilon B^M \cdot \Lambda_M(x^*),
\]
where $\Lambda_M(x^*) = \sum_{j} \prod_{i \neq j} |x^* - x_i|/|x_j - x_i|$ is the Lebesgue function. For extrapolation outside the interval, $\Lambda_M(x^*)$ grows exponentially in $M$, but since $M = \text{poly}(n)$, the total amplification is $2^{\text{poly}(n)}$. Dividing by $\prod_{\ell \neq k}|\Delta_\ell - \Delta_k|$ (also at most $2^{\text{poly}(n)}$ for integer gaps) and multiplying by $N = 2^n$, the degeneracy error satisfies
\[
|d_k - \tilde{d}_k| \leq 3\varepsilon \cdot 2^{\text{poly}(n)}.
\]
For $\varepsilon = O(2^{-\text{poly}(n)})$ with a sufficiently large polynomial, this is less than $1/2$. Since degeneracies are positive integers, rounding $\tilde{d}_k$ to the nearest integer recovers $d_k$ exactly.
\end{proof}

The proof extends to probabilistic oracles. If $\mathcal{C}_\varepsilon$ succeeds with probability at least $3/4$, then $O(\text{poly}(n))$ queries produce enough correct sample points to reconstruct $P$ despite corrupted evaluations. The Berlekamp-Welch algorithm recovers a polynomial of degree $d$ from $k$ partially corrupted evaluations, provided at least $\max\{d+1, (k+d)/2\}$ evaluations are correct \cite{movassagh2023hardness}. By the Chernoff bound, querying $k = O(\text{poly}(n))$ times ensures that at least $(k + M - 2)/2$ evaluations are correct with high probability. Combining this with \autoref{lem:approx-degeneracy}:

\begin{theorem}[$\#$P-hardness of $A_1$ estimation {\cite{braida2024unstructured}}]
\label{thm:sharp-P-hard-A1}
Estimating $A_1(H)$ to additive accuracy $\varepsilon = O(2^{-\text{poly}(n)})$ is $\#$P-hard, even for $2$-local Ising Hamiltonians. The result holds for both deterministic and probabilistic estimation algorithms.
\end{theorem}

For the running example ($M = 2$), the reconstruction polynomial $P(x) = (d_0/N)(1 + x/2) + (d_1/N)(x/2)$ is linear. Two evaluations determine $d_0$ and $d_1$ exactly, and the Lagrange interpolation is trivial: a line through two points. The $\#$P-hardness arises from Hamiltonians with $M = O(n^2)$ levels, where the reconstruction polynomial has high degree and small errors amplify through the exponential Paturi factor. The error amplification from oracle noise to degeneracy error grows as $2^{O(M \log n)}$, a factor that the next section analyzes precisely.


\section{The Intermediate Regime}
\label{sec:intermediate-A1}

The adiabatic algorithm requires $A_1$ to precision $O(2^{-n/2})$. NP-hardness holds at $1/\text{poly}(n)$ (\autoref{thm:np-hard-A1}), and $\#$P-hardness holds at $2^{-\text{poly}(n)}$ (\autoref{thm:sharp-P-hard-A1}). The algorithmically relevant precision $2^{-n/2}$ sits strictly between these regimes. The paper identifies this gap explicitly: ``these proof techniques based on polynomial interpolation do not allow us to conclude anything about the hardness of the approximation of $A_1(H)$ up to the additive error tolerated by the adiabatic algorithm'' \cite{braida2024unstructured}.

The results of this section are contributions of this thesis, addressing this open problem.

NP-hardness extends to $2^{-n/2}$ by monotonicity: an oracle at precision $2^{-n/2}$ is strictly more powerful than one at $1/\text{poly}(n)$ (since $2^{-n/2} < 1/\text{poly}(n)$ for large $n$), so it also solves 3-SAT. But $\#$P-hardness does not extend upward: an oracle at precision $2^{-n/2}$ is \emph{less} powerful than one at $2^{-\text{poly}(n)}$, and the interpolation technique that established the latter breaks down at the former. The following theorem makes this breakdown precise.

\begin{theorem}[Interpolation barrier]
\label{thm:interpolation-barrier}
The polynomial interpolation technique of \autoref{sec:sharp-P-hard-A1} requires oracle precision $\varepsilon = 2^{-n - O(M\log n)}$ to extract exact degeneracies, where $M = \text{poly}(n)$ is the number of distinct energy levels. At $\varepsilon = 2^{-n/2}$, the amplified error exceeds $1/2$ and rounding fails. The $\#$P-hardness argument does not extend to precision $2^{-n/2}$.
\end{theorem}

\begin{proof}
We trace the error propagation from oracle noise to degeneracy error through the construction of \autoref{lem:exact-degeneracy}. Let $\varepsilon$ denote the oracle accuracy, and let $B = \Delta_{\max} + M = \text{poly}(n)$ bound the denominator factors, where $\Delta_{\max}$ is the largest spectral gap.

\textbf{Sample-point error.} The approximate function values satisfy $|\tilde{f}(x_i) - f(x_i)| \leq 3\varepsilon$. The approximate polynomial samples are $\tilde{P}(x_i) = \prod_k(\Delta_k + x_i/2)\, \tilde{f}(x_i)$, with error
\begin{equation}
\label{eq:sample-error}
|\tilde{P}(x_i) - P(x_i)| \leq 3\varepsilon\prod_{k=0}^{M-1}\left(\Delta_k + \frac{x_i}{2}\right) \leq 3\varepsilon\, B^M.
\end{equation}

\textbf{Degeneracy error.} The approximate degeneracies are computed from Eq.~\eqref{eq:degeneracy-extraction} with $\tilde{P}$ in place of $P$. Since $\tilde{P}$ is the Lagrange interpolant through the noisy samples, its value at any point $x^*$ is $\tilde{P}(x^*) = \sum_j \tilde{P}(x_j) \prod_{i \neq j}(x^* - x_i)/(x_j - x_i)$. The error at $x^* = -2\Delta_k$ satisfies
\begin{equation}
\label{eq:interp-error}
|\tilde{P}(x^*) - P(x^*)| \leq 3\varepsilon\, B^M \sum_{j=0}^{M-1}\prod_{i \neq j}\frac{|x^* - x_i|}{|x_j - x_i|} = 3\varepsilon\, B^M \cdot \Lambda_M(x^*),
\end{equation}
where $\Lambda_M(x^*)$ is the Lebesgue function at $x^*$. For the odd-integer nodes $x_j = 2j+1$ and evaluation point $x^* = -2\Delta_k \leq 0$ (outside the interval $[1, 2M-1]$): each numerator factor $|x^* - x_i| = 2\Delta_k + 2i + 1 \leq 2B + 1$, and each denominator factor $|x_j - x_i| = 2|j - i|$. The sum over $j$ evaluates to
\begin{equation}
\label{eq:lebesgue-bound}
\Lambda_M(x^*) \leq \sum_{j=0}^{M-1}\frac{(2B+1)^{M-1}}{2^{M-1}\, j!\, (M-1-j)!} = \frac{(2B+1)^{M-1}}{(M-1)!},
\end{equation}
using the identity $\sum_j \binom{M-1}{j} = 2^{M-1}$. The denominator in Eq.~\eqref{eq:degeneracy-extraction} satisfies $\prod_{\ell \neq k}|\Delta_\ell - \Delta_k| \geq k!(M-1-k)!$ for integer gaps (since $|\Delta_\ell - \Delta_k| \geq |\ell - k|$), with minimum over $k$ at least $((M-1)/(2e))^{M-1}$ by Stirling's approximation. The total degeneracy error is therefore
\begin{equation}
\label{eq:total-error}
|d_k - \tilde{d}_k| \leq \frac{3\varepsilon\, N\, B^M\, (2B+1)^{M-1}}{(M-1)!\, ((M-1)/(2e))^{M-1}}.
\end{equation}
Since $B = \text{poly}(n)$ and $M = \text{poly}(n)$, the amplification factor is $2^{O(M\log n)}$.

\textbf{Rounding condition.} To extract exact degeneracies by rounding, we need $|d_k - \tilde{d}_k| < 1/2$. This requires
\begin{equation}
\label{eq:rounding-condition}
\varepsilon < \frac{1}{6N \cdot 2^{O(M\log n)}} = 2^{-n - O(M\log n)}.
\end{equation}

\textbf{Evaluation at $\varepsilon = 2^{-n/2}$.} Set $\varepsilon = 2^{-n/2}$ and $M = n^c$ for some constant $c \geq 1$. The degeneracy error is at least
\[
|d_k - \tilde{d}_k| \geq 3 \cdot 2^{-n/2} \cdot 2^n \cdot 2^{O(n^c \log n)} = 3 \cdot 2^{n/2 + O(n^c\log n)} \gg 1.
\]
Even for $c = 1$ (the most favorable case $M = n$), the exponent $n/2 + \Omega(n\log n)$ diverges, and rounding fails.
\end{proof}

The barrier is not an artifact of the paper's specific construction. Any scheme that extracts integer degeneracies by evaluating a polynomial outside its interpolation interval faces exponential amplification. The Lebesgue function $\Lambda_d(x^*)$ satisfies $\Lambda_d(x^*) \geq 2^{d-1}$ whenever $x^*$ lies at distance at least $|b - a|$ from the interpolation interval $[a,b]$, regardless of how the $d$ nodes are placed within $[a,b]$. No rearrangement of interpolation nodes, no alternative polynomial basis, and no change of variables can circumvent this. The same structural obstacle appears in quantum computational advantage proposals: the polynomial interpolation techniques used to prove hardness of boson sampling \cite{aaronson2011bosonsampling} and random circuit sampling \cite{bouland2019complexity} face analogous amplification barriers when extending hardness from exponentially small to moderate error regimes.

The interpolation barrier does not rule out $\#$P-hardness at $2^{-n/2}$ by other means. A proof that avoids polynomial extrapolation entirely --- using direct algebraic reductions or information-theoretic arguments --- might succeed. The barrier identifies where new proof techniques are needed: the challenge is to establish counting hardness without extracting exact integers from approximate real evaluations.

What can be computed at precision $2^{-n/2}$? We analyze the problem in the query model, where each query to a diagonal oracle $O_H\colon \ket{x}\ket{0} \mapsto \ket{x}\ket{E_x}$ reveals one diagonal entry of $H_z$. This framework cleanly separates quantum and classical capabilities.

\begin{theorem}[Quantum algorithm for $A_1$]
\label{thm:quantum-A1}
There exists a quantum algorithm that estimates $A_1(H_z)$ to additive precision $\varepsilon$ using
\begin{equation}
\label{eq:quantum-complexity}
O\!\left(\sqrt{N} + \frac{1}{\varepsilon\, \Delta_1}\right)
\end{equation}
quantum queries to the diagonal oracle $O_H$, where $\Delta_1 = E_1 - E_0$ is the spectral gap of $H_z$.
\end{theorem}

\begin{proof}
The algorithm has two stages.

\textbf{Stage 1: Finding $E_0$.} The Hamiltonian $H_z$ is diagonal in the computational basis, so computing $E_x$ for a given $\ket{x}$ requires one query to $O_H$. Finding the minimum of $E_x$ over all $x \in \{0,1\}^n$ is an instance of quantum minimum finding \cite{durr1996quantum}, which succeeds with high probability in $O(\sqrt{N})$ queries.

\textbf{Stage 2: Amplitude estimation of $A_1$.} Define the function
\[
g(x) = \begin{cases} \displaystyle\frac{1}{E_x - E_0} & \text{if } E_x \neq E_0, \\ 0 & \text{if } E_x = E_0. \end{cases}
\]
The spectral parameter is the mean $A_1 = (1/N)\sum_x g(x)$. Since the eigenvalues lie in $[0,1]$, the values of $g$ on non-ground states are in $[1, 1/\Delta_1]$. Rescaling to $h(x) = \Delta_1 \cdot g(x)$ yields $h(x) \in [0,1]$, and $A_1 = \mu_h/\Delta_1$ where $\mu_h = (1/N)\sum_x h(x)$.

Construct a quantum oracle $U_h$ acting as $U_h\colon \ket{x}\ket{0} \mapsto \ket{x}(\sqrt{1-h(x)}\ket{0} + \sqrt{h(x)}\ket{1})$. The implementation queries $O_H$ once to obtain $E_x$, performs classical arithmetic on an ancilla to compute $h(x) = \Delta_1/(E_x - E_0)$ (or $0$ for ground states), executes a controlled rotation $R_y(2\arcsin\sqrt{h(x)})$ on a flag qubit, and uncomputes the ancilla. Each application uses $O(1)$ queries to $O_H$ and $O(\text{poly}(n))$ auxiliary gates.

Preparing the uniform superposition $\ket{+}^{\otimes n}$ and applying $U_h$, the probability of measuring the flag qubit in $\ket{1}$ is
\[
p = \frac{1}{N}\sum_x h(x) = \mu_h.
\]
Amplitude estimation \cite{BrassardHoyerMoscaEtAl2002} estimates $p$ to additive precision $\delta$ using $O(1/\delta)$ applications of $U_h$ and its inverse. Setting $\delta = \varepsilon\, \Delta_1$ ensures $|A_1 - \tilde{A}_1| = |\mu_h - \tilde{\mu}_h|/\Delta_1 \leq \varepsilon$. The number of $U_h$ applications is $O(1/(\varepsilon\, \Delta_1))$.

Combining both stages: $O(\sqrt{N})$ queries for Stage~1 and $O(1/(\varepsilon\, \Delta_1))$ queries for Stage~2, giving the total in Eq.~\eqref{eq:quantum-complexity}. For $\varepsilon = 2^{-n/2}$ and $\Delta_1 = 1/\text{poly}(n)$: $O(2^{n/2} + 2^{n/2}\, \text{poly}(n)) = O(2^{n/2}\, \text{poly}(n))$.
\end{proof}

\begin{theorem}[Classical lower bound for $A_1$ estimation]
\label{thm:classical-lower-A1}
Any classical randomized algorithm estimating $A_1(H_z)$ to additive precision $\varepsilon$ in the query model requires $\Omega(1/\varepsilon^2)$ queries in the worst case.
\end{theorem}

\begin{proof}
We construct an adversarial pair of instances that are indistinguishable without sufficiently many queries.

\textbf{Instance construction.} Fix $t = \lceil\varepsilon N\rceil$. Instance~$H_0$ has a hidden set $S \subseteq \{0,1\}^n$ with $|S| = N/2$, and eigenvalues $E_x = 0$ for $x \in S$, $E_x = 1$ otherwise. Instance~$H_1$ has $|S'| = N/2 + t$ ground states. The spectral parameters are $A_1(H_0) = 1/2$ and $A_1(H_1) = (N/2 - t)/N = 1/2 - t/N$, differing by $t/N \geq \varepsilon$. An algorithm estimating $A_1$ to precision $\varepsilon/2$ must distinguish the two instances.

\textbf{Information-theoretic bound.} A classical query at string $x$ reveals $E_x \in \{0,1\}$, equivalent to learning whether $x \in S$. Under a uniform prior on $S$ (or $S'$), successive queries follow a hypergeometric sampling model. For the $j$-th query ($j \leq q \ll N$), the conditional per-query KL divergence between the two hypotheses is
\[
D_j = O\!\left(\frac{t^2}{(N-j)^2}\right) = O\!\left(\frac{t^2}{N^2}\right)
\]
when $q \leq N/2$. By the chain rule for KL divergence, the total information from $q$ adaptive queries is
\[
D_\text{KL}^{(q)} \leq \sum_{j=1}^{q} D_j \leq q \cdot O\!\left(\frac{t^2}{N^2}\right).
\]
By Le Cam's two-point method \cite{lecam1973convergence}, reliable hypothesis testing requires $D_\text{KL}^{(q)} \geq \Omega(1)$ (via Pinsker's inequality: total variation distance $\leq \sqrt{D_\text{KL}/2}$, and distinguishing requires total variation $\Omega(1)$). Therefore
\[
q \geq \Omega\!\left(\frac{N^2}{t^2}\right) = \Omega\!\left(\frac{1}{\varepsilon^2}\right).
\]
At $\varepsilon = 2^{-n/2}$: $q \geq \Omega(2^n)$.
\end{proof}

\begin{corollary}[Quadratic quantum-classical separation]
\label{cor:quadratic-separation}
In the query model, estimating $A_1(H_z)$ to precision $\varepsilon = 2^{-n/2}$ exhibits a quadratic quantum-classical separation: quantum complexity $O(2^{n/2}\, \text{poly}(n))$ versus classical complexity $\Omega(2^n)$.
\end{corollary}

\begin{proof}
The upper bound is \autoref{thm:quantum-A1} with $\Delta_1 = 1$ for the adversarial instance (or $\Delta_1 = 1/\text{poly}(n)$ in general). The lower bound is \autoref{thm:classical-lower-A1}. The separation ratio is $\Omega(2^{n/2}/\text{poly}(n))$, matching Grover's quadratic speedup for unstructured search.
\end{proof}

For Grover search with $N = 4$ ($n = 2$), the quantum algorithm uses $O(\sqrt{4} + 2) = O(4)$ queries at precision $\varepsilon = 1/2$, while the classical lower bound gives $\Omega(4)$. The separation is trivial at this scale but grows as $\Omega(2^{n/2}/\text{poly}(n))$ with $n$.

Two complementary frameworks apply: computational complexity for the problem of estimating $A_1$ given an explicit Hamiltonian description, and query complexity for the problem given oracle access to the diagonal entries.

\medskip
\noindent\textbf{Computational complexity} (explicit Hamiltonian):

\medskip
\begin{center}
\begin{tabular}{lll}
\hline
Precision $\varepsilon$ & Hardness & Source \\
\hline
$1/\text{poly}(n)$ & NP-hard & \autoref{thm:np-hard-A1} \\
$2^{-n/2}$ & NP-hard & monotonicity \\
$2^{-\text{poly}(n)}$ & $\#$P-hard & \autoref{thm:sharp-P-hard-A1} \\
\hline
\end{tabular}
\end{center}

\medskip
\noindent\textbf{Query complexity} (diagonal oracle, $\varepsilon = 2^{-n/2}$):

\medskip
\begin{center}
\begin{tabular}{lll}
\hline
Model & Complexity & Source \\
\hline
Quantum & $O(2^{n/2} \cdot \text{poly}(n))$ & \autoref{thm:quantum-A1} \\
Classical & $\Omega(2^n)$ & \autoref{thm:classical-lower-A1} \\
\hline
\end{tabular}
\end{center}

\medskip
The precision $2^{-n/2}$ is structurally significant for three reasons. It is the algorithmic requirement: the adiabatic schedule needs $A_1$ to precision $O(\sqrt{d_0/N})$, which is $O(2^{-n/2})$ in the worst case $d_0 = O(1)$. It is the interpolation barrier: the proof technique that establishes $\#$P-hardness breaks exactly at this threshold (\autoref{thm:interpolation-barrier}), while NP-hardness extends by monotonicity. And it is the query complexity transition: at $2^{-n/2}$, the quantum algorithm achieves $O(2^{n/2})$ queries while classical sampling requires $\Omega(2^n)$, a Grover-type quadratic gap.

Independent of the query-complexity analysis, classical sampling provides direct evidence for the hardness of $A_1$ estimation at the algorithmic precision. Given a procedure that samples eigenvalues $E_x$ according to the distribution $\{d_k/N\}$, estimating the mean $A_1 = \mathbb{E}[1/(E_x - E_0)]$ to precision $\delta_s$ requires $O(1/\delta_s^2) = \widetilde{O}(2^n/d_0)$ samples by Chebyshev's inequality \cite{gurvitz2005mixed}. This matches the formal $\Omega(2^n)$ lower bound of \autoref{thm:classical-lower-A1} up to logarithmic factors, providing a consistency check between the query-complexity result and concrete sampling algorithms.

The results of this chapter create a tension. The adiabatic algorithm of \autoref{thm:aqo-runtime} achieves the Grover speedup $\widetilde{O}(\sqrt{N/d_0})$, matching the lower bound for unstructured search. But the algorithm's schedule requires a spectral parameter whose computation is NP-hard, even at a precision far coarser than what the algorithm needs. In the circuit model, Grover's algorithm achieves the same speedup without pre-computing any spectral parameter: oracle queries gather information adaptively during execution. The adiabatic framework demands the schedule be fixed before evolution begins.

This asymmetry raises a precise question. Does the information cost of the adiabatic approach represent a fundamental limitation, or can it be circumvented? What runtime is achievable by an adiabatic algorithm that knows nothing about the problem Hamiltonian beyond its dimension? The next chapter formalizes this as an information-runtime tradeoff, proving a separation theorem for uninformed schedules and exploring whether adaptive measurements can bypass the classical pre-computation barrier.
