% Chapter 8: Hardness of Optimality

The optimal schedule from Chapter 7 achieves a quadratic speedup over classical
brute-force search, but the schedule must be fixed before evolution begins. It
depends on the spectral parameter $A_1$, the weighted sum of inverse gaps that
determines where the avoided crossing occurs. This parameter must be known to
additive accuracy $O(2^{-n/2})$. Given the $N = 2^n$ diagonal entries of
$H_z$, brute-force computation of
$A_1 = (1/N)\sum_{k=1}^{M-1} d_k/(E_k - E_0)$ by enumerating eigenvalues,
sorting, and summing takes $O(N)$ time. That is the same cost as classical
unstructured search. If pre-computation costs as much as the task itself, the
speedup is only conditional.

The runtime of \autoref{thm:aqo-runtime},
\[
T = O\!\left(\frac{1}{\varepsilon}\cdot\frac{\sqrt{A_2}}{\Delta^2\, A_1(A_1+1)}\cdot\sqrt{\frac{N}{d_0}}\right),
\]
makes this dependence explicit. The paper also states an $A_1^2$ denominator in
its headline theorem form. We keep the proof-level
$A_1(A_1+1)$ expression, which is the tighter form used in the derivation. In
the normalized regime $E_k\in[0,1]$, $A_1\ge 1-d_0/N$, so both forms differ only
by a constant-factor normalization for the asymptotic statements used here. The
adaptive schedule places a slow phase in the
window $[s^* - \delta_s,\, s^*)$ centered at $s^* = A_1/(A_1+1)$, where the
gap is smallest, and accelerates elsewhere. The parameters $A_2$ and $d_0$
enter only through $\sqrt{A_2/d_0}$. Using conservative bounds
($A_2 \geq 1 - d_0/N$ from Eq.~\eqref{eq:A2-lower-bound}, and $d_0 = 1$ for
worst-case search) changes only polynomial prefactors when $d_0 \ll N$. The
decisive quantity is $A_1$. It sets crossing location, while
$\delta_s = O(\sqrt{d_0 A_2/N}) = O(2^{-n/2})$ sets required precision. If the
crossing estimate is wrong by more than $\delta_s$, evolution passes too quickly
through the minimum and loses ground-state fidelity. Throughout this chapter, we
write $A_1(H)$ when dependence on the Hamiltonian matters.

The hardness of computing $A_1$ is not the only obstacle to adiabatic optimization for hard problems. Even if $A_1$ were known exactly, the single-crossing framework of Chapters 5--7 applies only to the rank-one projector $H_0 = -\ket{\psi_0}\bra{\psi_0}$. For the transverse-field driver (Chapter~5), the multi-crossing regime makes this framework inapplicable. Knowing $A_1$ no longer helps there, because the schedule would need to navigate exponentially many crossings rather than one.

In our view, the two barriers are complementary. Computing $A_1$ is hard even in the rank-one setting, and the single-crossing method fails for other drivers even when spectral information is available.

NP-hardness already appears at precision $1/\text{poly}(n)$. Two queries to an
$A_1$-oracle then suffice to solve 3-SAT (\autoref{sec:np-hard-A1}). At
precision $2^{-\text{poly}(n)}$, the problem becomes $\#$P-hard because
polynomial interpolation can recover all degeneracies from polynomially many
queries (\autoref{sec:sharp-P-hard-A1}).

The algorithmically relevant precision $2^{-n/2}$ sits strictly between these
scales. It is fine enough to matter for scheduling, but not fine enough for
either existing hardness proof to transfer directly. At this precision, a
quantum algorithm uses $O(2^{n/2})$ queries, while any classical algorithm needs
$\Omega(2^n)$. This is exactly a Grover-type quadratic separation
(\autoref{sec:intermediate-A1}).


\section{NP-Hardness of Estimating \texorpdfstring{$A_1$}{A1}}
\label{sec:np-hard-A1}

The Hamiltonian $H_z$ encodes an optimization problem whose ground energy $E_0$ determines whether a solution exists. For a 3-SAT instance, $E_0 = 0$ when a satisfying assignment exists and $E_0 \geq 1/\text{poly}(n)$ otherwise. Distinguishing these two cases is the local Hamiltonian promise problem, known to be NP-hard \cite{kempe2006complexity}. The spectral parameter $A_1$ is not obviously tied to this decision problem, because it aggregates information across all energy levels rather than only the ground energy. A modified Hamiltonian $H'$ bridges the two tasks. Comparing $A_1(H')$ with $A_1(H)$ reveals whether $E_0$ vanishes.

Define the $(n+1)$-qubit Hamiltonian
\begin{equation}
\label{eq:modified-ham}
H' = H \otimes \frac{I + \sigma_z}{2}.
\end{equation}
The operator $(I+\sigma_z)/2$ is the projector onto $\ket{0}$ for the ancilla qubit. It has eigenvalue $1$ on $\ket{0}$ and eigenvalue $0$ on $\ket{1}$. On the $\ket{0}$ branch, $H'$ has the same spectrum as $H$, with eigenvalues $E_k$ and degeneracies $d_k$. On the $\ket{1}$ branch, $H'$ annihilates every state, contributing $2^n$ eigenvalues at energy $0$. The ground energy of $H'$ is therefore always zero, regardless of $E_0(H)$.

This is the mechanism behind the reduction. $A_1(H')$ measures from the fixed reference point $E_0' = 0$, while $A_1(H)$ measures from the instance-dependent reference $E_0(H)$. When $E_0(H) > 0$, the two values separate by a detectable amount.

\begin{lemma}[Disambiguation {\cite{braida2024unstructured}}]
\label{lem:disambiguation}
Let $\varepsilon, \mu_1, \mu_2 \in (0,1)$. Suppose $\mathcal{C}_\varepsilon$ is a procedure that accepts the description of a Hamiltonian $H$ and outputs $\widetilde{A}_1(H)$ with $|\widetilde{A}_1(H) - A_1(H)| \leq \varepsilon$. Let $H$ be an $n$-qubit diagonal Hamiltonian with eigenvalues $0 \leq E_0 < E_1 < \cdots < E_{M-1} \leq 1$ and $M \in \text{poly}(n)$, such that either \textnormal{(i)} $E_0 = 0$ or \textnormal{(ii)} $\mu_1 \leq E_0 \leq 1 - \mu_2$. Then two calls to $\mathcal{C}_\varepsilon$ suffice to decide between \textnormal{(i)} and \textnormal{(ii)}, provided
\begin{equation}
\label{eq:disambiguation-bound}
\varepsilon < \frac{\mu_1}{6(1-\mu_1)} - \frac{d_0}{6N} \cdot \frac{1}{\mu_1 \mu_2}.
\end{equation}
\end{lemma}

\begin{proof}
Call $\mathcal{C}_\varepsilon$ on $H$ and on $H'$ defined by Eq.~\eqref{eq:modified-ham}, obtaining estimates $\widetilde{A}_1(H)$ and $\widetilde{A}_1(H')$. The test statistic is $\widetilde{A}_1(H) - 2\widetilde{A}_1(H')$, where the factor $2$ compensates for the doubling of the Hilbert space ($H'$ acts on $2^{n+1}$ states, so $A_1(H')$ carries a normalization factor $1/2^{n+1}$ instead of $1/2^n$).

\textbf{Case (i): $E_0 = 0$.} The ground energy of $H'$ is $0$ with degeneracy $d_0 + 2^n$, and the excited levels of $H'$ are $E_1, \ldots, E_{M-1}$ with degeneracies $d_1, \ldots, d_{M-1}$. Since $E_0 = 0$, both $A_1(H)$ and $A_1(H')$ sum over the same gaps $E_k - 0 = E_k$:
\[
A_1(H) = \frac{1}{2^n}\sum_{k=1}^{M-1}\frac{d_k}{E_k}, \qquad A_1(H') = \frac{1}{2^{n+1}}\sum_{k=1}^{M-1}\frac{d_k}{E_k}.
\]
Therefore $A_1(H) - 2A_1(H') = 0$, and by the triangle inequality the test statistic satisfies $|\widetilde{A}_1(H) - 2\widetilde{A}_1(H')| \leq 3\varepsilon$.

\textbf{Case (ii): $\mu_1 \leq E_0 \leq 1 - \mu_2$.} The ground energy of $H'$ is still $0$ (from the $\ket{1}$ branch), but now $E_0, E_1, \ldots, E_{M-1}$ are all excited levels. Thus
\[
A_1(H') = \frac{1}{2^{n+1}}\sum_{k=0}^{M-1}\frac{d_k}{E_k}.
\]
Decompose $A_1(H)$ using the partial fraction identity $d_k/(E_k - E_0) = d_k/E_k + d_k E_0/(E_k(E_k - E_0))$:
\begin{align}
A_1(H) &= \frac{1}{2^n}\sum_{k=1}^{M-1}\frac{d_k}{E_k - E_0} = \frac{1}{2^n}\sum_{k=1}^{M-1}\frac{d_k}{E_k} + \frac{E_0}{2^n}\sum_{k=1}^{M-1}\frac{d_k}{E_k(E_k - E_0)} \nonumber \\
&= \frac{1}{2^n}\sum_{k=0}^{M-1}\frac{d_k}{E_k} - \frac{d_0}{2^n E_0} + \frac{E_0}{2^n}\sum_{k=1}^{M-1}\frac{d_k}{E_k(E_k - E_0)}.
\label{eq:A1-decomposition}
\end{align}
The first sum equals $2A_1(H')$. For the remainder sum, $E_k \leq 1$ and $E_k - E_0 \leq 1 - E_0$, so the product $E_k(E_k - E_0)$ is at most $1 - E_0$. Each fraction $d_k/(E_k(E_k - E_0))$ is therefore bounded from below:
\[
\frac{E_0}{2^n}\sum_{k=1}^{M-1}\frac{d_k}{E_k(E_k - E_0)} \geq \frac{E_0}{1 - E_0}\cdot\frac{1}{2^n}\sum_{k=1}^{M-1}d_k = \frac{E_0}{1 - E_0}\left(1 - \frac{d_0}{N}\right).
\]
Combining with Eq.~\eqref{eq:A1-decomposition}:
\begin{align}
A_1(H) - 2A_1(H') &\geq \frac{E_0}{1-E_0}\left(1 - \frac{d_0}{N}\right) - \frac{d_0}{NE_0} \nonumber \\
&= \frac{E_0}{1-E_0} - \frac{d_0}{N}\cdot\frac{1 - E_0 + E_0^2}{E_0(1-E_0)}.
\label{eq:disambiguation-gap}
\end{align}
Since $1 - E_0 + E_0^2 \leq 1$ and $E_0(1-E_0) \geq \mu_1\mu_2$ on the given range, the fraction $(1 - E_0 + E_0^2)/(E_0(1-E_0))$ is at most $1/(\mu_1\mu_2)$. The first term $E_0/(1-E_0)$ is increasing in $E_0$, so it is at least $\mu_1/(1-\mu_1)$. Therefore
\[
A_1(H) - 2A_1(H') \geq \frac{\mu_1}{1-\mu_1} - \frac{d_0}{N}\cdot\frac{1}{\mu_1\mu_2},
\]
and the test statistic satisfies
\[
\widetilde{A}_1(H) - 2\widetilde{A}_1(H') \geq \frac{\mu_1}{1-\mu_1} - \frac{d_0}{N\mu_1\mu_2} - 3\varepsilon.
\]
The two cases are distinguished when $3\varepsilon$ from case (i) is separated from the lower bound in case (ii), requiring $6\varepsilon < \mu_1/(1-\mu_1) - d_0/(N\mu_1\mu_2)$.
\end{proof}

The disambiguation succeeds whenever the positive correction $E_0/(1-E_0)$ from the partial fraction identity dominates the negative term $-d_0/(NE_0)$, which happens as long as $d_0/N$ is small relative to $\mu_1^2\mu_2$. For the Ising Hamiltonians of interest, $d_0/N$ is exponentially small in $n$, so the condition is easily satisfied.

\begin{theorem}[NP-hardness of $A_1$ estimation {\cite{braida2024unstructured}}]
\label{thm:np-hard-A1}
Computing $A_1(H)$ to additive accuracy
\[
\varepsilon < \frac{1}{72(n-1)}
\]
for a $3$-local Hamiltonian $H$ on $n$ qubits is NP-hard.
\end{theorem}

\begin{proof}
We reduce 3-SAT to ground-energy disambiguation, following the construction of \cite{garey1976simplified, braida2024unstructured}. Let $\varphi$ be a 3-SAT formula on $n_\text{var}$ Boolean variables $x_0, \ldots, x_{n_\text{var}-1}$ with $m$ clauses, each of the form $a_k \lor b_k \lor c_k$ where each literal is some $x_l$ or $\bar{x}_l$. If $n_\text{var} + m < 15$, solve by brute force. Otherwise, define the single-qubit projectors
\[
P_{x_l} = \frac{I - \sigma_z^{(l)}}{2}, \qquad P_{\bar{x}_l} = \frac{I + \sigma_z^{(l)}}{2},
\]
which project onto the $\ket{1}$ and $\ket{0}$ states of qubit $l$, respectively. For each clause $k$ ($0 \leq k < m$), introduce an auxiliary qubit at index $n_\text{var} + k$ and define
\begin{align}
H_k &= P_{\bar{a}_k} + P_{\bar{b}_k} + P_{\bar{c}_k} + P_{\bar{x}_{n_\text{var}+k}} \nonumber \\
&\quad + P_{a_k}P_{b_k} + P_{a_k}P_{c_k} + P_{b_k}P_{c_k} \nonumber \\
&\quad + P_{\bar{a}_k}P_{x_{n_\text{var}+k}} + P_{\bar{b}_k}P_{x_{n_\text{var}+k}} + P_{\bar{c}_k}P_{x_{n_\text{var}+k}}.
\label{eq:clause-ham}
\end{align}
Direct computation on the computational basis shows that the minimum eigenvalue of $H_k$ is $3$ when clause $k$ is satisfied and $4$ when it is not; the maximum eigenvalue is $6$. The combined Hamiltonian on $2n_\text{var} + 2m$ qubits is
\begin{equation}
\label{eq:3sat-ham}
H = \frac{1}{6m}\sum_{k=0}^{m-1}H_k + \frac{1}{2n_\text{var}+2m}\sum_{j=n_\text{var}+m}^{2n_\text{var}+2m-1}P_{x_j} - \frac{1}{2}I.
\end{equation}
The first sum normalizes the clause energies to $[1/2, 1]$; the second sum adds $n_\text{var} + m$ free qubits whose projectors prefer $\ket{0}$; the identity shift places the eigenvalues in $[0, 1]$. When all clauses are satisfied, there exists an assignment making every $H_k$ achieve its minimum, giving $E_0 = 0$. When some clause is unsatisfied, the minimum of $\sum H_k/(6m)$ increases by at least $1/(6m)$, giving $E_0 \geq 1/(6m)$.

Apply \autoref{lem:disambiguation} with $\mu_1 = 1/(6m)$ and $\mu_2 = 1/2$. The number of eigenvalues is $N = 2^{2n_\text{var}+2m}$ and the ground-state degeneracy satisfies $d_0 \leq 2^{n_\text{var}+m}$, so $d_0/N \leq 2^{-(n_\text{var}+m)}$. Substituting into Eq.~\eqref{eq:disambiguation-bound}, the right-hand side satisfies
\begin{equation}
\label{eq:precision-arithmetic}
\frac{1}{6}\cdot\frac{1}{6m-1} - \frac{12m}{6}\cdot\frac{d_0}{N} \geq \frac{1}{36(n_\text{var}+m-1)} - \frac{2m}{2^{n_\text{var}+m}},
\end{equation}
since $1/(6(6m-1)) \geq 1/(36(n_\text{var}+m-1))$ for $n_\text{var} \geq 1$ and $d_0/N \leq 2^{-(n_\text{var}+m)}$. For $n_\text{var}+m \geq 15$, the second term satisfies $2m/2^{n_\text{var}+m} \leq 1/(72(n_\text{var}+m-1))$, so the disambiguation succeeds whenever
\[
\varepsilon < \frac{1}{72(n_\text{var}+m-1)}.
\]
The Hamiltonian $H'$ from Eq.~\eqref{eq:modified-ham} acts on $n = 2n_\text{var}+2m+1$ qubits and is $3$-local (since $H$ is $2$-local and the tensor product with $(I+\sigma_z)/2$ adds one ancilla). Since $n_\text{var}+m \leq n$, the precision bound $\varepsilon < 1/(72(n-1))$ follows.
\end{proof}

When $M = 2$ (Grover search), the spectral parameter $A_1 = (N-1)/N$ is trivial to obtain from the instance description. There are only two levels, and their degeneracies are fixed by the number of marked items. NP-hardness appears only for Hamiltonians that encode combinatorial structure with polynomially many levels and exponentially small ground-energy gaps. In that regime, $A_1$ depends nontrivially on the full degeneracy profile.

\textbf{Remark.} The disambiguation technique extends beyond 3-SAT. The MaxCut decision problem (given a graph $G = (V,E)$ and integer $k$, does $G$ have a cut of size at least $k$?) also reduces to $A_1$ estimation. The construction adds a weighted edge to $G$, creating an auxiliary Hamiltonian $H'$ whose $A_1$ value differs from a reference by at least $1/(|E|(|E|-1))$ between the two cases. This yields NP-hardness at precision $2/(5n^4)$ with a $2$-local Hamiltonian, sharpening the locality requirement from $3$-local to $2$-local at the cost of a slightly tighter precision bound.


\section{\texorpdfstring{$\#$P}{\#P}-Hardness of Computing \texorpdfstring{$A_1$}{A1} Exactly}
\label{sec:sharp-P-hard-A1}

NP-hardness captures a decision question, namely whether $E_0 = 0$. But $A_1$ carries more than a single decision bit. It is a weighted sum over all energy levels, and its exact value determines every degeneracy $d_k$. Recovering these degeneracies solves counting problems. For NP-complete Hamiltonians, for example, $d_0$ counts satisfying assignments. Counting is harder than deciding, and it is $\#$P-complete \cite{valiant1979complexity}.

The extraction uses a parametrized family of Hamiltonians that shifts the spectrum continuously, turning $A_1$ into a rational function whose poles carry the degeneracies as residues. For a parameter $x > 0$, define the $(n+1)$-qubit Hamiltonian
\begin{equation}
\label{eq:param-ham}
H'(x) = H \otimes I - \frac{x}{2}\, I \otimes \frac{I + \sigma_z^{(n+1)}}{2}.
\end{equation}
On the $\ket{0}$ branch of the ancilla, the eigenvalues are $E_k - x/2$ with degeneracies $d_k$. On the $\ket{1}$ branch, the eigenvalues are $E_k$ with degeneracies $d_k$. The ground energy is $E_0 - x/2$ (from the $\ket{0}$ branch, for $x > 0$). The gaps relative to this ground energy are $\Delta_k = E_k - E_0$ (extending the notation $\Delta = E_1 - E_0$ from earlier chapters to all levels) for the $\ket{0}$ branch and $\Delta_k + x/2$ for the $\ket{1}$ branch.

Computing $A_1(H'(x))$ from these gaps and defining $f(x) = 2A_1(H'(x)) - A_1(H)$ isolates the $\ket{1}$-branch contribution \cite{braida2024unstructured}:
\begin{equation}
\label{eq:f-function}
f(x) = \frac{1}{N}\sum_{k=0}^{M-1}\frac{d_k}{\Delta_k + x/2}.
\end{equation}
This function is a sum of $M$ simple poles at $x = -2\Delta_k$. Each pole has residue $2d_k/N$, encoding the degeneracy of the corresponding energy level. The function $f$ is a partial-fraction decomposition of the entire degeneracy spectrum. The extraction problem reduces to recovering these residues from evaluations of $f$.

\begin{lemma}[Exact degeneracy extraction {\cite{braida2024unstructured}}]
\label{lem:exact-degeneracy}
Suppose $\mathcal{C}$ is a procedure that computes $A_1(H)$ exactly for any $n$-qubit diagonal Hamiltonian $H$. Let $H_\sigma$ be an Ising Hamiltonian (\autoref{eq:Ising-Ham}) with integer eigenvalues and known spectral gaps $\Delta_k = E_k - E_0$. Then $O(\text{poly}(n))$ calls to $\mathcal{C}$ suffice to compute all degeneracies $d_0, d_1, \ldots, d_{M-1}$.
\end{lemma}

\begin{proof}
Each evaluation of $f(x_i)$ requires two calls to $\mathcal{C}$, one for $A_1(H)$ and one for $A_1(H'(x_i))$. Evaluate $f$ at $M$ distinct positive odd integers $x_i \in \{1, 3, \ldots, 2M-1\}$. These values avoid the poles. For each $k$, $\Delta_k + x_i/2 \geq 0 + 1/2 > 0$ because $\Delta_k \geq 0$ and $x_i \geq 1$. The total cost is $2M = O(\text{poly}(n))$ oracle calls.

Define the reconstruction polynomial
\begin{equation}
\label{eq:recon-poly}
P(x) = \prod_{k=0}^{M-1}\left(\Delta_k + \frac{x}{2}\right) f(x) = \frac{1}{N}\sum_{k=0}^{M-1} d_k \prod_{\ell \neq k}\left(\Delta_\ell + \frac{x}{2}\right).
\end{equation}
Multiplying $f(x)$ by the product of all denominators clears the poles, yielding a polynomial of degree at most $M - 1$ in $x$. Since the gaps $\Delta_k$ are known integers, the values $P(x_i) = \prod_k(\Delta_k + x_i/2) \cdot f(x_i)$ are computable from oracle outputs. The $M$ values $P(x_1), \ldots, P(x_M)$ determine $P$ uniquely by Lagrange interpolation \cite{phillips2003interpolation}. A polynomial of degree at most $M - 1$ is fixed by $M$ distinct evaluations.

The degeneracies are recovered by evaluating $P$ at the poles. Setting $x = -2\Delta_k$ kills every factor $(\Delta_\ell + x/2)$ except the $k$-th, giving
\begin{equation}
\label{eq:degeneracy-extraction}
d_k = \frac{N \cdot P(-2\Delta_k)}{\displaystyle\prod_{\ell \neq k}(\Delta_\ell - \Delta_k)}, \qquad k \in \{0, \ldots, M-1\}.
\end{equation}
The denominator is nonzero because the eigenvalues are distinct. The entire computation (oracle calls, Lagrange interpolation, pole evaluation) runs in $O(\text{poly}(n))$ time.
\end{proof}

Extracting $d_0$ from an Ising Hamiltonian encoding a 3-SAT formula counts satisfying assignments and therefore solves $\#$3-SAT. Since $\#$3-SAT is $\#$P-complete \cite{valiant1979complexity}, an exact $A_1$ oracle would solve every problem in $\#$P in polynomial time. The same degeneracy data also determines IQP output probabilities \cite{movassagh2023hardness}. From $d_k$ and $\Delta_k$, one computes $|\langle 0^n | C_\text{IQP} | 0^n \rangle|^2 = |N^{-1}\sum_k d_k\, e^{i\Delta_k}|^2$, which is itself $\#$P-hard. The NP-hardness result of \autoref{sec:np-hard-A1} uses a $3$-local Hamiltonian because the ancilla raises locality by one. The $\#$P-hardness statement already holds for $2$-local Ising Hamiltonians, since Eq.~\eqref{eq:param-ham} preserves $2$-locality when $H$ is $2$-local.

The exact oracle is unrealistic. A robust version of \autoref{lem:exact-degeneracy} must tolerate additive noise $\varepsilon$ in the oracle outputs. Paturi's amplification lemma gives the control we need.

\begin{lemma}[Paturi {\cite{paturi1992}}]
\label{lem:paturi}
Let $P(x)$ be a polynomial of degree at most $M$ satisfying $|P(i)| \leq c$ for all integers $i \in \{0, 1, \ldots, M\}$. Then $|P(x)| \leq c \cdot 2^M$ for all $x \in [0, M]$.
\end{lemma}

Paturi's lemma bounds polynomial growth between sample points. If a polynomial is bounded by $c$ at $M+1$ integer points, it can grow by at most a factor $2^M$ on the interval. Applied to the difference between exact and approximate reconstruction polynomials, this gives explicit control of interpolation error.

Across the proofs below, error propagation is organized into three stages. These stages are sample-point error, extrapolation amplification, and rounding threshold.
Stage~1 itself has three algebraic sub-steps: oracle noise enters $f$ as
$|\tilde{f}(x_i) - f(x_i)| \leq 3\varepsilon$ (three oracle calls contribute),
then enters polynomial samples as
$|\tilde{P}(x_i) - P(x_i)| \leq 3\varepsilon \prod_k(\Delta_k + x_i/2)$, and
the product is bounded by $B^M$ with $B = \Delta_{\max} + M = \text{poly}(n)$.

\begin{lemma}[Approximate degeneracy extraction {\cite{braida2024unstructured}}]
\label{lem:approx-degeneracy}
Under the same hypotheses as \autoref{lem:exact-degeneracy}, but with an oracle $\mathcal{C}_\varepsilon$ satisfying $|\widetilde{A}_1(H) - A_1(H)| \leq \varepsilon$, all degeneracies $d_k$ can be computed exactly by $O(\text{poly}(n))$ calls to $\mathcal{C}_\varepsilon$ for sufficiently small $\varepsilon \in O(2^{-\text{poly}(n)})$.
\end{lemma}

\begin{proof}[Proof sketch]
\textbf{Stage 1 (sample-point error).} The proof follows the same three-stage route. The approximate polynomial $\tilde{P}$ is the Lagrange interpolant through the noisy values $(\tilde{P}(x_1),$ $\ldots,$ $\tilde{P}(x_M))$. Its difference $D = \tilde{P} - P$ is a polynomial of degree at most $M - 1$ bounded by $3\varepsilon B^M$ at the sample points. By Paturi's lemma (\autoref{lem:paturi}), $|D(x)| \leq 3\varepsilon B^M \cdot 2^{M-1}$ on the interpolation interval.

\textbf{Stage 2 (extrapolation amplification).} At the pole evaluation points $x^* = -2\Delta_k$, which lie outside the interval $[1, 2M-1]$, the error is bounded by Lagrange-basis amplification:
\[
|D(x^*)| \leq 3\varepsilon B^M \cdot \Lambda_M(x^*),
\]
where $\Lambda_M(x^*) = \sum_{j} \prod_{i \neq j} |x^* - x_i|/|x_j - x_i|$ is the Lebesgue function. For extrapolation outside the interval, $\Lambda_M(x^*)$ grows exponentially in $M$, but since $M = \text{poly}(n)$, the total amplification is $2^{\text{poly}(n)}$. Dividing by $\prod_{\ell \neq k}|\Delta_\ell - \Delta_k|$ (also at most $2^{\text{poly}(n)}$ for integer gaps) and multiplying by $N = 2^n$, the degeneracy error satisfies
\[
|d_k - \tilde{d}_k| \leq 3\varepsilon \cdot 2^{\text{poly}(n)}.
\]
\textbf{Stage 3 (rounding to exact degeneracies).} For $\varepsilon = O(2^{-\text{poly}(n)})$ with a sufficiently large polynomial, this is less than $1/2$. Since degeneracies are positive integers, rounding $\tilde{d}_k$ to the nearest integer recovers $d_k$ exactly.
\end{proof}

The proof extends to probabilistic oracles. If $\mathcal{C}_\varepsilon$ succeeds with probability at least $3/4$, then $O(\text{poly}(n))$ queries produce enough correct sample points to reconstruct $P$ despite corrupted evaluations. The Berlekamp-Welch algorithm recovers a polynomial of degree $d$ from $k$ partially corrupted evaluations, provided at least $\max\{d+1, (k+d)/2\}$ evaluations are correct \cite{movassagh2023hardness}. By the Chernoff bound, querying $k = O(\text{poly}(n))$ times ensures that at least $(k + M - 2)/2$ evaluations are correct with high probability. Combining this with \autoref{lem:approx-degeneracy}:

\begin{theorem}[$\#$P-hardness of $A_1$ estimation {\cite{braida2024unstructured}}]
\label{thm:sharp-P-hard-A1}
Estimating $A_1(H)$ to additive accuracy $\varepsilon = O(2^{-\text{poly}(n)})$ is $\#$P-hard, even for $2$-local Ising Hamiltonians. The result holds for both deterministic and probabilistic estimation algorithms.
\end{theorem}

With $M = 2$, the reconstruction polynomial $P(x) = (d_0/N)(1 + x/2) + (d_1/N)(x/2)$ is linear. Two evaluations determine $d_0$ and $d_1$ exactly, and interpolation reduces to fitting a line through two points. The $\#$P-hardness comes from Hamiltonians with $M = O(n^2)$ levels, where the reconstruction polynomial has high degree and tiny oracle errors amplify through the exponential Paturi factor. The next section quantifies this amplification as $2^{O(M \log n)}$.


\section{The Intermediate Regime}
\label{sec:intermediate-A1}

The adiabatic algorithm requires $A_1$ to precision $O(2^{-n/2})$. NP-hardness
holds at $1/\text{poly}(n)$ (\autoref{thm:np-hard-A1}), and $\#$P-hardness holds
at $2^{-\text{poly}(n)}$ (\autoref{thm:sharp-P-hard-A1}). The algorithmically
relevant precision $2^{-n/2}$ lies strictly between these regimes. The
interpolation method does not reach this precision. Braida et
al.~\cite{braida2024unstructured} therefore left this intermediate regime open.

The interpolation technique extracts exact integers from approximate real evaluations. At precision $2^{-n/2}$, the error amplification inherent in polynomial extrapolation makes this extraction impossible.

NP-hardness extends to $2^{-n/2}$ by monotonicity. An oracle at precision
$2^{-n/2}$ is strictly more powerful than one at $1/\text{poly}(n)$, so it
still solves 3-SAT. The $\#$P-hardness argument does not extend in the opposite
direction. An oracle at precision $2^{-n/2}$ is \emph{less} powerful than one
at $2^{-\text{poly}(n)}$, and the interpolation proof for that regime fails
here.

The failure has a concrete source. Oracle noise enters polynomial samples at
rate $\varepsilon B^M$. The Lebesgue function then amplifies that error
exponentially in $M$. At $\varepsilon = 2^{-n/2}$, the amplified error already
exceeds the rounding margin.

\begin{theorem}[Interpolation barrier]
\label{thm:interpolation-barrier}
The polynomial interpolation technique of \autoref{sec:sharp-P-hard-A1} requires oracle precision $\varepsilon = 2^{-n - O(M\log n)}$ to extract exact degeneracies, where $M = \text{poly}(n)$ is the number of distinct energy levels. At $\varepsilon = 2^{-n/2}$, the amplified error exceeds $1/2$ and rounding fails. The $\#$P-hardness argument does not extend to precision $2^{-n/2}$.
\end{theorem}

\begin{proof}
We trace the same three-stage propagation from oracle noise to degeneracy error. Let $\varepsilon$ denote the oracle accuracy, and let $B = \Delta_{\max} + M = \text{poly}(n)$ bound the denominator factors, where $\Delta_{\max}$ is the largest spectral gap.

\textbf{Stage 1 (sample-point error).} The approximate function values satisfy $|\tilde{f}(x_i) - f(x_i)| \leq 3\varepsilon$. The approximate polynomial samples are $\tilde{P}(x_i) = \prod_k(\Delta_k + x_i/2)\, \tilde{f}(x_i)$, with error
\begin{equation}
\label{eq:sample-error}
|\tilde{P}(x_i) - P(x_i)| \leq 3\varepsilon\prod_{k=0}^{M-1}\left(\Delta_k + \frac{x_i}{2}\right) \leq 3\varepsilon\, B^M.
\end{equation}

\textbf{Stage 2 (extrapolation amplification).} The approximate degeneracies are computed from Eq.~\eqref{eq:degeneracy-extraction} with $\tilde{P}$ in place of $P$. Since $\tilde{P}$ is the Lagrange interpolant through the noisy samples, its value at any point $x^*$ is $\tilde{P}(x^*) = \sum_j \tilde{P}(x_j) \prod_{i \neq j}(x^* - x_i)/(x_j - x_i)$. At $x^* = -2\Delta_k$,
\begin{equation}
\label{eq:interp-error}
|\tilde{P}(x^*) - P(x^*)| \leq 3\varepsilon\, B^M \sum_{j=0}^{M-1}\prod_{i \neq j}\frac{|x^* - x_i|}{|x_j - x_i|} = 3\varepsilon\, B^M \cdot \Lambda_M(x^*),
\end{equation}
where $\Lambda_M(x^*) = \sum_j \prod_{i \neq j} |x^* - x_i|/|x_j - x_i|$ is the Lebesgue function at $x^*$. It is the amplification factor for pointwise interpolation error: if each sample has error $\delta$, then the extrapolated value at $x^*$ can incur error as large as $\delta \cdot \Lambda_M(x^*)$.

For extrapolation outside the sample interval, this amplification is exponential in $M$. Consider odd-integer nodes $x_j = 2j+1$ and evaluation point $x^* = -2\Delta_k \leq 0$, which lies outside $[1, 2M-1]$. Each numerator factor obeys $|x^* - x_i| = 2\Delta_k + 2i + 1 \leq 2B + 1$. For the denominator,
\[
\prod_{i \neq j}|x_j - x_i| = \prod_{i \neq j} 2|j-i| = 2^{M-1}\, j!\,(M-1-j)!,
\]
because the nodes are equally spaced with spacing $2$. The sum over $j$ becomes
\begin{equation}
\label{eq:lebesgue-bound}
\Lambda_M(x^*) \leq \sum_{j=0}^{M-1}\frac{(2B+1)^{M-1}}{2^{M-1}\, j!\, (M-1-j)!} = \frac{(2B+1)^{M-1}}{(M-1)!},
\end{equation}
using $\sum_j \binom{M-1}{j} = 2^{M-1}$. The denominator in Eq.~\eqref{eq:degeneracy-extraction} satisfies $\prod_{\ell \neq k}|\Delta_\ell - \Delta_k| \geq k!(M-1-k)!$ for integer gaps (since $|\Delta_\ell - \Delta_k| \geq |\ell - k|$), with minimum over $k$ at least $((M-1)/(2e))^{M-1}$ by Stirling's approximation.

Putting these bounds together, the total degeneracy error is
\begin{equation}
\label{eq:total-error}
|d_k - \tilde{d}_k| \leq \frac{3\varepsilon\, N\, B^M\, (2B+1)^{M-1}}{(M-1)!\, ((M-1)/(2e))^{M-1}}.
\end{equation}
Since $B = \text{poly}(n)$ and $M = \text{poly}(n)$, the amplification factor is $2^{O(M\log n)}$.

\textbf{Stage 3 (rounding threshold).} To extract exact degeneracies by rounding, we need $|d_k - \tilde{d}_k| < 1/2$. This requires
\begin{equation}
\label{eq:rounding-condition}
\varepsilon < \frac{1}{6N \cdot 2^{O(M\log n)}} = 2^{-n - O(M\log n)}.
\end{equation}

\textbf{Consequence at $\varepsilon = 2^{-n/2}$.} Set $\varepsilon = 2^{-n/2}$ and $M = n^c$ for some constant $c \geq 1$. The error bound from Eq.~\eqref{eq:total-error} evaluates to
\[
|d_k - \tilde{d}_k| \leq 3 \cdot 2^{-n/2} \cdot 2^n \cdot 2^{O(n^c \log n)} = 3 \cdot 2^{n/2 + O(n^c\log n)} \gg 1.
\]
Even for $c = 1$ (the most favorable case $M = n$), the exponent $n/2 + \Omega(n)$ diverges. The upper bound on the degeneracy error already exceeds $1/2$, so the rounding step cannot be guaranteed to succeed.
\end{proof}

The precision $\varepsilon = 2^{-n/2}$ is too coarse for interpolation but too fine for brute force. It sits in a window that existing techniques cannot reach from either side.

This amplification is not specific to the construction above. Exponential growth is intrinsic to polynomial extrapolation for any $d$ nodes, any interval, and any evaluation point sufficiently far outside that interval.

\begin{theorem}[Generic extrapolation barrier]
\label{thm:generic-barrier}
Let $x_1, \ldots, x_d$ be any $d$ distinct nodes in an interval $[a,b]$, and let $x^*$ satisfy $\mathrm{dist}(x^*, [a,b]) \geq b - a$. The Lebesgue function at $x^*$ satisfies $\Lambda_d(x^*) \geq 2^{d-1}$. Consequently, any polynomial extrapolation scheme that evaluates a degree-$(d-1)$ interpolant at $x^*$ from samples with pointwise error $\delta$ can incur worst-case error at least $\delta \cdot 2^{d-1}$ at $x^*$.
\end{theorem}

\begin{proof}
Assume $x^* \leq a - (b-a)$ (the case $x^* \geq b + (b-a)$ follows by symmetry). Let $x_{(1)} = \min_j x_j \geq a$ be the leftmost node. The corresponding Lagrange basis polynomial satisfies
\[
|\ell_{(1)}(x^*)| = \prod_{i:\, x_i \neq x_{(1)}} \frac{|x_i - x^*|}{|x_i - x_{(1)}|} = \prod_{i:\, x_i \neq x_{(1)}} \left(1 + \frac{x_{(1)} - x^*}{x_i - x_{(1)}}\right).
\]
Each factor has numerator shift $x_{(1)} - x^* \geq a - (a - (b-a)) = b - a$ and denominator $x_i - x_{(1)} \leq b - a$, so every factor is at least $2$. With $d - 1$ such factors, $|\ell_{(1)}(x^*)| \geq 2^{d-1}$. Since $\Lambda_d(x^*) = \sum_j |\ell_j(x^*)| \geq |\ell_{(1)}(x^*)|$, the bound follows. For perturbed samples $y_j = P(x_j) + e_j$ with $|e_j| \leq \delta$, the extrapolation error is
\[
\tilde{P}(x^*) - P(x^*) = \sum_{j=1}^d e_j\,\ell_j(x^*).
\]
Choosing adversarial signs $e_j = \delta\,\mathrm{sign}(\ell_j(x^*))$ gives
\[
|\tilde{P}(x^*) - P(x^*)| = \delta \sum_{j=1}^d |\ell_j(x^*)| = \delta\,\Lambda_d(x^*) \geq \delta \cdot 2^{d-1},
\]
so the worst-case error can be at least $\delta \cdot 2^{d-1}$.
\end{proof}

\autoref{thm:generic-barrier} rules out rescuing the $\#$P-hardness argument by tweaking interpolation details. Reordering nodes (equispaced, Chebyshev, or otherwise), changing polynomial bases, or reparameterizing variables cannot push amplification below $2^{d-1}$. At $d = M = \text{poly}(n)$, the needed precision stays at $\varepsilon = 2^{-\Omega(n)}$, far below $2^{-n/2}$. The same obstacle appears in quantum-advantage hardness arguments. Interpolation-based proofs for boson sampling \cite{aaronson2011bosonsampling}, IQP sampling \cite{bremner2017achievingquantum}, and random circuit sampling \cite{bouland2019complexity} face analogous amplification barriers when moving from exponentially small to moderate error.

The interpolation barrier does not rule out $\#$P-hardness at $2^{-n/2}$ by other methods. A proof that avoids polynomial extrapolation, for example through direct algebraic reductions or information-theoretic arguments, might still work. The barrier identifies the pressure point: we need a way to prove counting hardness without recovering exact integers from approximate real values.

What can be computed at precision $2^{-n/2}$? We answer in the query model,
where each query to a diagonal oracle
$O_H\colon \ket{x}\ket{0} \mapsto \ket{x}\ket{E_x}$ reveals one diagonal entry
of $H_z$ at unit cost. This model cleanly separates quantum and classical
capabilities. The interpolation barrier is a classical obstruction: polynomial
extrapolation cannot recover integers at this precision. A quantum algorithm can
bypass that step entirely by using amplitude estimation rather than polynomial
reconstruction.

\begin{theorem}[Quantum algorithm for $A_1$]
\label{thm:quantum-A1}
There exists a quantum algorithm that estimates $A_1(H_z)$ to additive precision $\varepsilon$ using
\begin{equation}
\label{eq:quantum-complexity}
O\!\left(\sqrt{N} + \frac{1}{\varepsilon\, \Delta_1}\right)
\end{equation}
quantum queries to the diagonal oracle $O_H$, where $\Delta_1 = E_1 - E_0$ is the spectral gap of $H_z$.
\end{theorem}

\begin{proof}
The algorithm has two stages.

\textbf{Stage 1: Finding $E_0$.} The Hamiltonian $H_z$ is diagonal in the computational basis, so computing $E_x$ for a given $\ket{x}$ requires one query to $O_H$. Finding the minimum of $E_x$ over all $x \in \{0,1\}^n$ is an instance of quantum minimum finding \cite{durr1996quantum}, which succeeds with high probability in $O(\sqrt{N})$ queries.

\textbf{Stage 2: Amplitude estimation of $A_1$.} Define the function
\[
g(x) = \begin{cases} \displaystyle\frac{1}{E_x - E_0} & \text{if } E_x \neq E_0, \\ 0 & \text{if } E_x = E_0. \end{cases}
\]
The spectral parameter is the mean $A_1 = (1/N)\sum_x g(x)$. Since the eigenvalues lie in $[0,1]$, the values of $g$ on non-ground states are in $[1, 1/\Delta_1]$. Rescaling to $h(x) = \Delta_1 \cdot g(x)$ yields $h(x) \in [0,1]$, and $A_1 = \mu_h/\Delta_1$ where $\mu_h = (1/N)\sum_x h(x)$.

Construct a quantum oracle $U_h$ acting as $U_h\colon \ket{x}\ket{0} \mapsto \ket{x}(\sqrt{1-h(x)}\ket{0} + \sqrt{h(x)}\ket{1})$. The implementation queries $O_H$ once to obtain $E_x$, performs classical arithmetic on an ancilla to compute $h(x) = \Delta_1/(E_x - E_0)$ (or $0$ for ground states), executes a controlled rotation $R_y(2\arcsin\sqrt{h(x)})$ on a flag qubit, and uncomputes the ancilla. Each application uses $O(1)$ queries to $O_H$ and $O(\text{poly}(n))$ auxiliary gates.

Preparing the uniform superposition $\ket{+}^{\otimes n}$ and applying $U_h$, the probability of measuring the flag qubit in $\ket{1}$ is
\[
p = \frac{1}{N}\sum_x h(x) = \mu_h.
\]
Amplitude estimation \cite{BrassardHoyerMoscaEtAl2002} estimates $p$ to additive precision $\delta$ using $O(1/\delta)$ applications of $U_h$ and its inverse. Setting $\delta = \varepsilon\, \Delta_1$ ensures $|A_1 - \tilde{A}_1| = |\mu_h - \tilde{\mu}_h|/\Delta_1 \leq \varepsilon$. The number of $U_h$ applications is $O(1/(\varepsilon\, \Delta_1))$.

Combining both stages gives $O(\sqrt{N})$ queries for Stage~1 and $O(1/(\varepsilon\, \Delta_1))$ queries for Stage~2, yielding Eq.~\eqref{eq:quantum-complexity}. For $\varepsilon = 2^{-n/2}$ and $\Delta_1 = 1/\text{poly}(n)$, this becomes $O(2^{n/2} + 2^{n/2}\, \text{poly}(n)) = O(2^{n/2}\, \text{poly}(n))$.
\end{proof}

To confirm that the quantum algorithm's $O(2^{n/2})$ query count is a genuine advantage, we need a classical lower bound. The natural route is information-theoretic. We ask how many samples a classical algorithm needs to distinguish two carefully chosen instances whose $A_1$ values differ by $\varepsilon$.

\begin{theorem}[Classical lower bound for $A_1$ estimation]
\label{thm:classical-lower-A1}
Any classical randomized algorithm estimating $A_1(H_z)$ to additive precision $\varepsilon$ in the query model requires $\Omega(1/\varepsilon^2)$ queries in the worst case.
\end{theorem}

\begin{proof}
We construct an adversarial pair of instances that are indistinguishable without sufficiently many queries.

\textbf{Instance construction.} Fix $t = \lceil\varepsilon N\rceil$. Instance~$H_0$ has a hidden set $S \subseteq \{0,1\}^n$ with $|S| = N/2$, and eigenvalues $E_x = 0$ for $x \in S$, $E_x = 1$ otherwise. Instance~$H_1$ has $|S'| = N/2 + t$ ground states. The spectral parameters are $A_1(H_0) = 1/2$ and $A_1(H_1) = (N/2 - t)/N = 1/2 - t/N$, differing by $t/N \geq \varepsilon$. An algorithm estimating $A_1$ to precision $\varepsilon/2$ must distinguish the two instances.

\textbf{Information-theoretic bound.} A classical query at string $x$ reveals $E_x \in \{0,1\}$, which is equivalent to learning whether $x \in S$. Under a uniform prior on $S$ (or $S'$), successive queries follow a hypergeometric sampling model. Conditioned on earlier outcomes, the $j$-th query is a Bernoulli trial. Under hypothesis $H_i$, $x$ is a ground state with probability $p_j^{(i)} = (|S_i| - g_{j-1})/(N - j + 1)$, where $g_{j-1}$ counts ground states already found. The difference $p_j^{(1)} - p_j^{(0)} = t/(N - j + 1)$ is independent of $g_{j-1}$. Since both parameters are $\Theta(1)$, a Taylor expansion of binary KL divergence $D(p \| p + \delta) = \delta^2/(p(1-p)) + O(\delta^3)$ with $\delta = t/(N-j+1)$ gives the conditional per-query divergence
\[
D_j = O\!\left(\frac{t^2}{(N-j)^2}\right) = O\!\left(\frac{t^2}{N^2}\right)
\]
when $q \leq N/2$. By the chain rule for KL divergence, the total information from $q$ adaptive queries is
\[
D_\text{KL}^{(q)} \leq \sum_{j=1}^{q} D_j \leq q \cdot O\!\left(\frac{t^2}{N^2}\right).
\]
By Le Cam's two-point method \cite{lecam1973convergence}, reliable hypothesis testing requires $D_\text{KL}^{(q)} \geq \Omega(1)$. Via Pinsker's inequality, total variation distance is at most $\sqrt{D_\text{KL}/2}$, and reliable distinguishing needs total variation $\Omega(1)$. Therefore
\[
q \geq \Omega\!\left(\frac{N^2}{t^2}\right) = \Omega\!\left(\frac{1}{\varepsilon^2}\right).
\]
At $\varepsilon = 2^{-n/2}$, this gives $q \geq \Omega(2^n)$.
\end{proof}

\begin{corollary}[Quadratic quantum-classical separation]
\label{cor:quadratic-separation}
In the query model, estimating $A_1(H_z)$ to precision $\varepsilon = 2^{-n/2}$ exhibits a quadratic quantum-classical separation, with quantum complexity $O(2^{n/2}\, \text{poly}(n))$ and classical complexity $\Omega(2^n)$.
\end{corollary}

\begin{proof}
The upper bound is \autoref{thm:quantum-A1} with $\Delta_1 = 1$ for the adversarial instance (or $\Delta_1 = 1/\text{poly}(n)$ in general). The lower bound is \autoref{thm:classical-lower-A1}. The separation ratio is $\Omega(2^{n/2}/\text{poly}(n))$, matching Grover's quadratic speedup for unstructured search.
\end{proof}

At the precision the adiabatic algorithm needs, quantum computation offers exactly the speedup the algorithm achieves.

The quantum bound in \autoref{thm:quantum-A1} is not only an upper bound; it is tight. The tightness already appears on the simplest family, namely $M = 2$ with $\Delta_1 = 1$. There, estimating $A_1 = (N - d_0)/N$ to precision $\varepsilon$ is exactly approximate counting at precision $\varepsilon$.

Write the Grover iterate as $G = (2\ket{+}\!\bra{+} - I)(I - 2\Pi_S)$, where $\Pi_S$ projects onto the $d_0$ ground states. Its eigenphases are $\pm 2\theta$ with $\sin^2\theta = d_0/N$. For $d_0 \approx N/2$, we have $dp/d\theta = \sin 2\theta = 1$, so precision $\varepsilon$ in $A_1$ requires precision $\varepsilon$ in $\theta$. The Heisenberg limit for quantum phase estimation \cite{giovannetti2006quantum}, together with the quantum Cram\'{e}r-Rao inequality \cite{braunstein1994statistical} and Fisher bound $F_Q \leq 4T^2$, gives $T \geq 1/(2\varepsilon)$ applications of $G$, each with $O(1)$ oracle cost. Combining this with the upper bound yields quantum query complexity $\Theta(2^{n/2})$ at precision $\varepsilon = 2^{-n/2}$. The next chapter formalizes this theorem and ties it to the information cost of the adiabatic approach.

Two complementary frameworks apply here. Computational complexity asks whether a
classical computer can extract $A_1$ from an explicit Hamiltonian description
$(J_{ij}, h_j)$. Query complexity asks how many oracle evaluations are
information-theoretically necessary when diagonal entries are hidden behind
$O_H$. A problem can be easy in one model and hard in the other. For $A_1$
estimation, both frameworks show hardness, which supports the same conclusion:
the pre-computation barrier is structural, not an artifact of one proof style.

In the computational model with an explicit Hamiltonian description, the precision-dependent hardness landscape is summarized below.

\begin{center}
\begin{tabular}{lll}
\hline
Precision $\varepsilon$ & Hardness & Source \\
\hline
$1/\text{poly}(n)$ & NP-hard & \autoref{thm:np-hard-A1} \\
$2^{-n/2}$ & NP-hard & monotonicity \\
$2^{-\text{poly}(n)}$ & $\#$P-hard & \autoref{thm:sharp-P-hard-A1} \\
\hline
\end{tabular}
\end{center}

\noindent In the query model with a diagonal oracle at the algorithmically relevant precision $\varepsilon = 2^{-n/2}$, the comparison is:

\begin{center}
\begin{tabular}{lll}
\hline
Model & Complexity & Source \\
\hline
Quantum & $O(2^{n/2} \cdot \text{poly}(n))$ & \autoref{thm:quantum-A1} \\
Classical & $\Omega(2^n)$ & \autoref{thm:classical-lower-A1} \\
\hline
\end{tabular}
\end{center}

\noindent The precision $2^{-n/2}$ is exactly the algorithmic target. The adiabatic schedule needs $A_1$ to precision $O(\sqrt{d_0/N})$, which is $O(2^{-n/2})$ in the worst case $d_0 = O(1)$. It is also where interpolation-based $\#$P-hardness proofs break (\autoref{thm:interpolation-barrier}), even though NP-hardness still follows by monotonicity. It is also where query complexity separates sharply. The quantum algorithm uses $O(2^{n/2})$ queries, while classical sampling requires $\Omega(2^n)$.

Classical sampling provides independent evidence for the hardness of $A_1$ estimation at the algorithmic precision. Given a procedure that samples eigenvalues $E_x$ according to the distribution $\{d_k/N\}$, estimating the mean $A_1 = \mathbb{E}[1/(E_x - E_0)]$ to precision $\delta_s$ requires $O(1/\delta_s^2) = \widetilde{O}(2^n/d_0)$ samples by Chebyshev's inequality \cite{gurvitz2005mixed}. This matches the formal $\Omega(2^n)$ lower bound of \autoref{thm:classical-lower-A1} up to logarithmic factors, providing a consistency check between the query-complexity result and concrete sampling algorithms.

The information barrier is not specific to adiabatic evolution. Consider the time-independent Hamiltonian $H = -\ket{\psi_0}\bra{\psi_0} + r\,H_\sigma$, where $r > 0$ is fixed and $H_\sigma$ is the problem Hamiltonian. Evolving $\ket{\psi_0}$ under $H$ for time $t$ produces oscillations between $\ket{\psi_0}$ and the ground state of $H_\sigma$, with success probability controlled by $r$.

The oscillation frequency is set by the spectral gap of $H$, and this gap is maximized when $r$ is tuned to the avoided crossing, namely $r = A_1$ (up to normalization). To keep success probability non-negligible, $r$ must lie within $O(2^{-n/2})$ of $A_1$ \cite{braida2024unstructured}. So the same barrier appears in this model as well. One still needs $A_1$ to exponential precision, and computing it remains NP-hard. The obstacle comes from spectral structure, not from adiabatic formalism itself.

The results of this chapter create a tension. The adiabatic algorithm of \autoref{thm:aqo-runtime} achieves Grover scaling $\widetilde{O}(\sqrt{N/d_0})$, matching the unstructured-search lower bound. Yet the schedule depends on a spectral parameter whose computation is NP-hard, even at much coarser precision than the algorithm ultimately needs. In the circuit model, Grover achieves the same speedup without any spectral pre-computation because oracle queries gather information adaptively during execution. In the adiabatic framework, by contrast, the schedule must be fixed before evolution begins.

This asymmetry raises a precise question. Does the information cost of the adiabatic approach represent a fundamental limitation, or can it be circumvented? What runtime is achievable by an adiabatic algorithm that knows nothing about the problem Hamiltonian beyond its dimension? The next chapter formalizes this as an information-runtime tradeoff, proving a separation theorem for uninformed schedules and exploring whether adaptive measurements can bypass the classical pre-computation barrier.
