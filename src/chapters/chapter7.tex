% Chapter 7: Optimal Schedule
% ASSUMES: Chapter 4 defines AQC, adiabatic theorem, spectral gap as
%   computational resource, avoided crossings, local/adaptive schedules,
%   Roland-Cerf construction.
% ASSUMES: Chapter 5 defines H(s), H_z, H_0, |psi_0>, A_p, A_1, A_2,
%   s^*, delta_s, g_min, hat{g}, the three regions, eigenvalue equation,
%   spectral condition (Definition 5.x), grover-gap.
% ASSUMES: Chapter 6 proves gap-left (Lemma 6.1), gap-right (Lemma 6.2),
%   complete-profile (Theorem 6.x), f(s*) = 4, s_0 formula.

The spectral gap of $H(s)$ is now bounded from below on all of $[0,1]$.
\autoref{thm:complete-profile} shows a piecewise form. The gap reaches
$g_{\min}$ at $s^*$, then reopens with slope $A_1(A_1+1)/A_2$ on the left and
$\Delta/30$ on the right. Chapter 5 already identified the key runtime integral,
$\int_0^1 g(s)^{-2}\,ds$ (Eq.~\eqref{eq:runtime-integral-preview}), and showed
that the crossing window dominates. The remaining issue is the adiabatic
theorem itself. Its error bound determines which power of $g$ controls runtime.

With a constant-rate theorem, runtime is controlled by
$\int_0^1 g(s)^{-3}\,ds$. For the profile of \autoref{thm:complete-profile},
the crossing window contributes $\delta_s/g_{\min}^3$. In the two-level case
$M = 2$ with $g_{\min} = 1/\sqrt{N}$, this gives $T = O(N)$ and no speedup over
classical search. An adaptive schedule with $K'(s)$ scaled to the gap replaces
$\int g^{-3}\,ds$ with $\int g^{-p}\,ds$ for $p \in (1,2)$. The resulting
runtime
\[
T = O\!\left(\frac{\sqrt{A_2}}{A_1(A_1+1)\Delta^2}\sqrt{\frac{N}{d_0}}\frac{1}{\varepsilon}\right)
\]
matches Grover scaling up to explicit spectral prefactors.

\section{How Theorem Choice Sets Runtime}
\label{sec:prior-theorems}

The gap profile alone does not determine runtime. Translating spectral data into evolution time requires an adiabatic theorem, and different theorems permit different schedules. That choice changes the gap dependence and, in the running example, is exactly the difference between $O(N)$ and $O(\sqrt{N})$.

The earliest rigorous bounds, due to Jansen, Ruskai, and
Seiler~\cite{jansen2007bounds}, apply to a constant schedule $K'(s) = T$ and
give a transition probability of order $O(1/T^2)$. Their Theorem~3 states that
for a state $\psi \in P(0)$, the probability of leaving the ground space
satisfies
\begin{equation}
\label{eq:jrs-bound}
(\psi, [1-P(s)]U_\tau(s)\psi) \leq A(s)^2,
\end{equation}
where
$A(s) \leq (1/T)(\lVert H' \rVert/g^2)\vert_{\mathrm{bdry}}
+ (1/T)\int_0^s (7\sqrt{m}\,\lVert H'\rVert^2/g^3 + \lVert H'\rVert/g^2)\,ds'$,
with $m$ the multiplicity of the ground eigenvalue and the boundary term
evaluated at $s = 0$ and $s$. Setting $A(s) = \varepsilon$ and solving for $T$
gives
\begin{equation}
\label{eq:jrs-runtime}
T = O\!\left(\frac{1}{\varepsilon}\int_0^1 \frac{\lVert H'\rVert^2}{g(s)^3}\,ds\right).
\end{equation}
With $M = 2$ and $\lVert H'\rVert = O(1)$, the integral $\int_0^1 g^{-3}\,ds$
is dominated by the $O(1/\sqrt{N})$-wide window where $g \approx 1/\sqrt{N}$.
Its contribution is $(1/\sqrt{N})\cdot N^{3/2} = N$. Therefore the JRS bound
gives $T = O(N/\varepsilon)$, which matches classical search. A constant
schedule allocates the same physical time per unit of $s$ whether the gap is
$O(1)$ or $O(1/\sqrt{N})$. That uniform allocation forces the $g^{-3}$
dependence, so the narrow crossing window dominates and the quantum speedup
disappears.

The resolution is to let the schedule depend on the gap. Roland and Cerf~\cite{roland2002quantum} introduced a \emph{local} adiabatic condition. Instead of enforcing adiabaticity with one global timescale $T$, they enforce it infinitesimally on each segment $[s,s+ds]$. The standard criterion gives
\[
\left|\frac{ds}{dt}\right| \leq \frac{\varepsilon\, g(s)^2}{|\langle e_1(s)|H'(s)|e_0(s)\rangle|},
\]
where $e_0,e_1$ are the ground and first excited states. Inverting this inequality yields
\[
K'(s)=\frac{dt}{ds}\geq \frac{|\langle e_1|H'|e_0\rangle|}{\varepsilon\,g(s)^2}.
\]
For the running example, $|\langle e_1|H'|e_0\rangle|=O(1)$ because $H'(s)=\ket{\psi_0}\bra{\psi_0}+H_z$ is constant. Thus $K'(s)\propto 1/g(s)^2$, and the runtime becomes
\begin{equation}
\label{eq:roland-cerf-runtime}
T = \frac{C}{\varepsilon}\int_0^1 g(s)^{-2}\,ds.
\end{equation}
The integral can be evaluated explicitly. Writing $g(s)^2 = (2s-1)^2 + 4s(1-s)/N$ and substituting $u = 2s - 1$:
\begin{equation}
\label{eq:roland-cerf-integral}
\int_0^1 g(s)^{-2}\,ds = \frac{1}{2}\int_{-1}^{1}\frac{du}{u^2 + (1-u^2)/N} = \frac{1}{2}\int_{-1}^{1}\frac{N\,du}{1 + (N-1)u^2}.
\end{equation}
For large $N$, the substitution $v = \sqrt{N-1}\,u$ gives $\frac{N}{2\sqrt{N-1}}\int_{-\sqrt{N-1}}^{\sqrt{N-1}}\frac{dv}{1+v^2} = \frac{N}{2\sqrt{N-1}}\cdot 2\arctan(\sqrt{N-1}) = O(\sqrt{N})$, since $\arctan(\sqrt{N-1}) \to \pi/2$. Therefore $T = O(\sqrt{N}/\varepsilon)$, recovering the Grover speedup from a smooth, continuous-time evolution.

The Roland-Cerf construction requires exact knowledge of $g(s)$ at every point. For the running example with $M=2$, this is feasible because the gap has the closed form in Eq.~\eqref{eq:grover-gap}. For a general $M$-level problem Hamiltonian, the exact gap is unknown and only the piecewise bounds of \autoref{thm:complete-profile} are available. If one substitutes a lower bound $g_0(s)\le g(s)$, the schedule slows down conservatively because $1/g_0^2 \ge 1/g^2$, so the runtime increases only by constants. The difficulty shifts to error control: derivative terms in the adiabatic bound become sensitive to corners in $g_0$. The adaptive schedule of \autoref{sec:adaptive-schedule} addresses this through the exponent $p\in(1,2)$.

Several later results sharpened this picture. Boixo, Knill, and
Somma~\cite{boixo2009eigenpath} introduced eigenpath traversal, which replaces
continuous evolution with projections onto ground states of intermediate
Hamiltonians $H(s_0),H(s_1),\ldots,H(s_L)$. Their key ingredient is phase
randomization between steps. It suppresses coherent accumulation of diabatic
error, which otherwise recreates the usual $O(1/g_{\min}^2)$ behavior. Under
that phase randomization, coherent errors from successive transitions no longer
add constructively.
Under the condition $\int g^{-p}\,ds = O(g_{\min}^{1-p})$, this gives
$O(1/g_{\min})$ scaling. Cunningham and Roland~\cite{cunningham2024eigenpath}
tightened constants and gave a continuous-time version that underlies
\autoref{sec:error-bound}. Elgart and Hagedorn~\cite{ElgartHagedorn2012}
followed a different route using Gevrey-smooth switching, obtaining
superpolynomial suppression with runtime
$T \geq K\, g^{-2}\lvert\ln g\rvert^{6\alpha}$. The schedule used here has a
different practical advantage: it only needs a certified lower bound
$g_0(s)\le g(s)$, not the exact gap.

\section{The Adiabatic Error Bound}
\label{sec:error-bound}

The Schr\"odinger equation $i\,d\ket{\psi}/dt = H(s(t))\ket{\psi}$ governs the evolution under the time-dependent Hamiltonian $H(s)$, with $s:[0,T]\to[0,1]$ parameterizing the interpolation. The density-matrix form $d\rho/dt=-i[H,\rho]$ is more convenient for the error analysis and also covers mixed states. We reparameterize time by $t=K(s)$, where $K:[0,1]\to\mathbb{R}^+$ is differentiable and monotone increasing. The chain rule then gives
\begin{equation}
\label{eq:reparametrized-evolution}
\frac{d\rho}{ds} = -iK'(s)[H(s), \rho(s)],
\end{equation}
where $K'(s) = dK/ds > 0$ controls the instantaneous evolution rate. The total runtime is $T = K(1) = \int_0^1 K'(s)\,ds$. A large $K'(s)$ means slow evolution (long physical time per unit of $s$), allowing the state to track the ground state through a small-gap region. A small $K'(s)$ means fast evolution, appropriate where the gap is large and diabatic transitions are suppressed.

The error of the adiabatic evolution is the probability that the final state does not lie in the ground space of $H(1)$:
\begin{equation}
\label{eq:error-def}
\varepsilon = 1 - \mathrm{Tr}[P(1)\rho(1)],
\end{equation}
where $P(s)$ denotes the projector onto the ground eigenspace of $H(s)$ and $\rho(0) = P(0)$ (the system starts in the ground state of $H(0)$). The projector $P(s)$ and the ground energy $\lambda_0(s)$ are both functions of $s$, varying as the Hamiltonian interpolates from $H_0$ to $H_z$. The operator
\begin{equation}
\label{eq:pseudoinverse-def}
(H(s) - \lambda_0(s))^+ = \sum_{j \geq 1} \frac{1}{\lambda_j(s) - \lambda_0(s)}\,\ket{\phi_j(s)}\bra{\phi_j(s)}
\end{equation}
is the pseudoinverse of $H(s) - \lambda_0(s)$: it acts as zero on the ground space and as $(\lambda_j - \lambda_0)^{-1}$ on the $j$-th excited eigenspace. Its operator norm is $1/g(s)$, so a small spectral gap amplifies the pseudoinverse.

\begin{lemma}[Adiabatic error bound {\cite{braida2024unstructured, cunningham2024eigenpath}}]
\label{lem:error-bound}
Let $H(s)$ be a twice-differentiable path of Hamiltonians with a continuous ground energy $\lambda_0(s)$ and a spectral gap $g(s) > 0$ for all $s \in [0,1]$. Let $K: [0,1] \to \mathbb{R}^+$ be a schedule with absolutely continuous derivative $K'$. Then the evolution~\eqref{eq:reparametrized-evolution} starting from $\rho(0) = P(0)$ satisfies
\begin{equation}
\label{eq:error-bound}
\varepsilon \leq \frac{1}{K'(1)}\left\lVert\left[P'(1),\, (H(1) - \lambda_0(1))^+\right]\right\rVert + \int_0^1 \frac{1}{K'}\left\lVert\left[P',\, (H - \lambda_0)^+\right]'\right\rVert ds + \int_0^1 \left|\left(\frac{1}{K'}\right)'\right|\left\lVert\left[P',\, (H - \lambda_0)^+\right]\right\rVert ds.
\end{equation}
\end{lemma}

\begin{proof}
Since $\rho(0) = P(0)$, the error is $\varepsilon = \mathrm{Tr}[P(0)\rho(0)] - \mathrm{Tr}[P(1)\rho(1)] = \left|\mathrm{Tr}[P\rho]\right|_0^1$, so it suffices to track $\mathrm{Tr}[P(s)\rho(s)]$. Differentiating:
\begin{equation}
\frac{d}{ds}\mathrm{Tr}[P\rho] = \mathrm{Tr}[P'\rho] + \mathrm{Tr}[P\rho'].
\end{equation}
The second term vanishes. Substituting the evolution equation~\eqref{eq:reparametrized-evolution}: $\mathrm{Tr}[P\rho'] = -iK'\,\mathrm{Tr}[P[H,\rho]]$. Since $HP = \lambda_0 P$, the cyclic property gives $\mathrm{Tr}[P[H,\rho]] = \mathrm{Tr}[PH\rho - P\rho H] = \lambda_0\,\mathrm{Tr}[P\rho] - \mathrm{Tr}[HP\rho] = 0$.

For $\mathrm{Tr}[P'\rho]$, write $Q = I - P$ and use the decomposition $P' = PP'Q + QP'P$, which holds because $PP'P = 0$ and $QP'Q = 0$.\footnote{Differentiating $P^2 = P$ gives $P'P + PP' = P'$. Left-multiplying by $P$: $PP'P + PP' = PP'$, so $PP'P = 0$. Then $QP'Q = P' - PP' - P'P + PP'P = P' - P' = 0$.} Inserting $Q = (H-\lambda_0)^+(H-\lambda_0)$ and using the identities $(H-\lambda_0)\rho P = [H,\rho]P$ and $P\rho(H-\lambda_0) = -P[H,\rho]$ (both consequences of $HP = \lambda_0 P$), a cyclic rearrangement under the trace gives
\begin{equation}
\mathrm{Tr}[P'\rho] = \mathrm{Tr}\!\left[PP'(H-\lambda_0)^+[H,\rho]\right] - \mathrm{Tr}\!\left[(H-\lambda_0)^+P'P[H,\rho]\right].
\end{equation}
Since $(H-\lambda_0)^+P = P(H-\lambda_0)^+ = 0$ (the pseudoinverse annihilates the ground space), $PP'(H-\lambda_0)^+$ reduces to $P'(H-\lambda_0)^+$ and $(H-\lambda_0)^+P'P$ reduces to $(H-\lambda_0)^+P'$, so the two terms combine into a commutator:
\begin{equation}
\mathrm{Tr}[P'\rho] = \mathrm{Tr}\!\left[\left[P',\, (H-\lambda_0)^+\right][H,\rho]\right] = i(K')^{-1}\,\mathrm{Tr}\!\left[\left[P',\, (H-\lambda_0)^+\right]\rho'\right],
\end{equation}
where the last equality substitutes $[H,\rho] = i(K')^{-1}\rho'$ from~\eqref{eq:reparametrized-evolution}.

Integrating from $0$ to $1$ gives $\mathrm{Tr}[P\rho]\big|_0^1 = i\int_0^1 (K')^{-1}\,\mathrm{Tr}\!\left[\left[P',\, (H-\lambda_0)^+\right]\rho'\right]ds$. Integration by parts, with $u = (K')^{-1}[P',\,(H-\lambda_0)^+]$ and $dv = \rho'\,ds$, transfers the derivative from $\rho$ onto $u$:
\begin{multline}
\mathrm{Tr}[P\rho]\big|_0^1 = i(K'(1))^{-1}\,\mathrm{Tr}\!\left[\left[P'(1),\, (H(1)-\lambda_0(1))^+\right]\rho(1)\right] \\
- i\int_0^1 \mathrm{Tr}\!\left[\left((K')^{-1}\left[P',\, (H-\lambda_0)^+\right]' + \left((K')^{-1}\right)'\!\left[P',\, (H-\lambda_0)^+\right]\right)\rho\right]ds.
\end{multline}
The boundary term at $s = 0$ vanishes. Since $\rho(0) = P(0)$, the commutator trace expands as
\[
\mathrm{Tr}\!\left[\left[P',\,(H-\lambda_0)^+\right]P\right] = \mathrm{Tr}[P'(H-\lambda_0)^+P] - \mathrm{Tr}[(H-\lambda_0)^+P'P].
\]
For the first summand, $(H-\lambda_0)^+P = 0$ (the pseudoinverse annihilates the ground-space projector), so $\mathrm{Tr}[P'(H-\lambda_0)^+P] = 0$. For the second, cyclicity of the trace gives $\mathrm{Tr}[(H-\lambda_0)^+P'P] = \mathrm{Tr}[P(H-\lambda_0)^+P'] = 0$ by the same identity. Taking absolute values and bounding $|\mathrm{Tr}[A\rho]| \leq \lVert A\rVert$ for any density matrix $\rho$ yields~\eqref{eq:error-bound}.
\end{proof}

The error bound depends on $H(s)$ only through the commutator $[P', (H-\lambda_0)^+]$ and its derivative. The following bounds express these in terms of the Hamiltonian derivatives $H'$, $H''$ and the spectral gap $g$, using the Riesz integral representation of the spectral projector introduced by Kato~\cite{Kato1950}.

\begin{lemma}[Projector derivative bounds {\cite{braida2024unstructured}}]
\label{lem:derivative-bounds}
Under the conditions of \autoref{lem:error-bound}:
\begin{align}
\label{eq:P-prime-bound}
\lVert P'(s) \rVert &\leq \frac{2\lVert H'(s)\rVert}{g(s)}, \\
\label{eq:commutator-bound}
\left\lVert\left[P'(s),\, (H(s) - \lambda_0(s))^+\right]\right\rVert &\leq \frac{4\lVert H'(s)\rVert}{g(s)^2}, \\
\label{eq:commutator-deriv-bound}
\left\lVert\left[P'(s),\, (H(s) - \lambda_0(s))^+\right]'\right\rVert &\leq \frac{40\lVert H'(s)\rVert^2}{g(s)^3} + \frac{4\lVert H''(s)\rVert}{g(s)^2}.
\end{align}
\end{lemma}

\begin{proof}[Proof of~\eqref{eq:P-prime-bound}]
Let $\Gamma$ be a circle in the complex plane centered at $\lambda_0(s)$ with radius $g(s)/2$. The Riesz integral representation gives
\begin{equation}
P(s) = \frac{1}{2\pi i}\oint_\Gamma R_{H(s)}(z)\,dz,
\end{equation}
where $R_{H(s)}(z) = (zI - H(s))^{-1}$ is the resolvent. Differentiating with respect to $s$:
\begin{equation}
P'(s) = \frac{1}{2\pi i}\oint_\Gamma R_{H(s)}(z)\,H'(s)\,R_{H(s)}(z)\,dz,
\end{equation}
using the resolvent identity $R_H' = R_H H' R_H$. On the contour $\Gamma$, every point $z$ lies at distance exactly $g(s)/2$ from $\lambda_0(s)$ and at distance at least $g(s)/2$ from every other eigenvalue (since the nearest eigenvalue is $\lambda_1(s)$ at distance $g(s)$ from $\lambda_0(s)$). Therefore $\lVert R_{H(s)}(z) \rVert = 1/\mathrm{dist}(z, \sigma(H(s))) \leq 2/g(s)$ on $\Gamma$. Bounding the integral:
\begin{equation}
\lVert P'(s) \rVert \leq \frac{1}{2\pi}\oint_\Gamma \lVert R_H(z)\rVert \cdot \lVert H'(s)\rVert \cdot \lVert R_H(z)\rVert\,|dz| \leq \frac{1}{2\pi}\left(\frac{2}{g}\right)^2\lVert H'\rVert \cdot \pi g = \frac{2\lVert H'\rVert}{g}.
\end{equation}
\end{proof}

Bound~\eqref{eq:commutator-bound} follows from~\eqref{eq:P-prime-bound}: $\lVert[A,B]\rVert \leq 2\lVert A\rVert\cdot\lVert B\rVert$ gives $\lVert[P', (H-\lambda_0)^+]\rVert \leq 2 \cdot 2\lVert H'\rVert/g \cdot 1/g = 4\lVert H'\rVert/g^2$.

Bound~\eqref{eq:commutator-deriv-bound} requires two intermediate results. Write $\widetilde{H} = H - \lambda_0$ for the shifted Hamiltonian. Its pseudoinverse satisfies
\begin{equation}
\label{eq:pseudoinverse-derivative}
(\widetilde{H}^+)' = -\widetilde{H}^+\widetilde{H}'\widetilde{H}^+ + P'\widetilde{H}^+ + \widetilde{H}^+P',
\end{equation}
where $\widetilde{H}' = H' - \lambda_0'$. To see this, split the difference quotient $(\widetilde{H}^+(s+h) - \widetilde{H}^+(s))/h$ using $Q = \widetilde{H}^+\widetilde{H}$ and $P = I - Q$. The $Q$-part gives $\lim_{h\to 0}\widetilde{H}^+(s)(\widetilde{H}(s) - \widetilde{H}(s+h))\widetilde{H}^+(s+h)/h = -\widetilde{H}^+\widetilde{H}'\widetilde{H}^+$, while the $P$-part, after adding and subtracting $P(s+h)\widetilde{H}^+(s+h)$ and $\widetilde{H}^+(s)P(s)$, yields $P'\widetilde{H}^+ + \widetilde{H}^+P'$. Bounding the norm and using $|\lambda_0'| = |\langle\phi_0|H'|\phi_0\rangle| \leq \lVert H'\rVert$ (Hellmann-Feynman):
\begin{equation}
\label{eq:pseudoinverse-deriv-bound}
\lVert(\widetilde{H}^+)'\rVert \leq \frac{\lVert H'\rVert + |\lambda_0'|}{g^2} + \frac{4\lVert H'\rVert}{g^2} \leq \frac{6\lVert H'\rVert}{g^2}.
\end{equation}

The second intermediate result bounds $P''$. Differentiating $P' = (2\pi i)^{-1}\oint_\Gamma R_H H' R_H\,dz$ gives
\begin{equation}
P'' = \frac{1}{2\pi i}\oint_\Gamma \left(2R_H H' R_H H' R_H + R_H H'' R_H\right)dz,
\end{equation}
where the two $R_H H' R_H H' R_H$ terms arise from differentiating each resolvent factor. Bounding by $\lVert R_H(z)\rVert \leq 2/g$ on $\Gamma$ and integrating over the contour of length $\pi g$:
\begin{equation}
\label{eq:P-double-prime-bound}
\lVert P''\rVert \leq \frac{1}{2\pi}\left(\frac{2}{g}\right)^{\!3}\!2\lVert H'\rVert^2 \cdot \pi g + \frac{1}{2\pi}\left(\frac{2}{g}\right)^{\!2}\!\lVert H''\rVert \cdot \pi g = \frac{8\lVert H'\rVert^2}{g^2} + \frac{2\lVert H''\rVert}{g}.
\end{equation}

Now expand $[P', (H-\lambda_0)^+]' = [P'', (H-\lambda_0)^+] + [P', ((H-\lambda_0)^+)']$ and bound each commutator:
\begin{equation}
\lVert[P'', (H-\lambda_0)^+]\rVert \leq \frac{2\lVert P''\rVert}{g} \leq \frac{16\lVert H'\rVert^2}{g^3} + \frac{4\lVert H''\rVert}{g^2},
\end{equation}
and, using~\eqref{eq:P-prime-bound} and~\eqref{eq:pseudoinverse-deriv-bound}:
\begin{equation}
\lVert[P', ((H-\lambda_0)^+)']\rVert \leq 2\lVert P'\rVert\cdot\lVert(\widetilde{H}^+)'\rVert \leq 2\cdot\frac{2\lVert H'\rVert}{g}\cdot\frac{6\lVert H'\rVert}{g^2} = \frac{24\lVert H'\rVert^2}{g^3}.
\end{equation}
Summing gives $40\lVert H'\rVert^2/g^3 + 4\lVert H''\rVert/g^2$. A block-matrix decomposition of the commutator with respect to $P$ and $Q = I - P$, tracking cross terms exactly rather than using submultiplicativity, replaces the coefficient $40$ by $\approx 4.77$ \cite{braida2024unstructured}; the asymptotic scaling is unchanged.

The simplest schedule is constant. It sets $K'(s) = T$, so evolution proceeds at a uniform rate regardless of the gap. Substituting the derivative bounds into the error bound~\eqref{eq:error-bound} with $(1/K')' = 0$ gives the constant-rate result.

\begin{theorem}[Constant-rate runtime]
\label{thm:constant-rate}
Under the conditions of \autoref{lem:error-bound}, a constant schedule $K'(s) = T$ achieves error at most $\varepsilon$ provided
\begin{equation}
\label{eq:constant-rate-formula}
T \geq \frac{1}{\varepsilon}\left(\frac{4\lVert H'(1)\rVert}{g(1)^2} + \int_0^1 \frac{40\lVert H'(s)\rVert^2}{g(s)^3}\,ds + \int_0^1\frac{4\lVert H''(s)\rVert}{g(s)^2}\,ds\right).
\end{equation}
\end{theorem}

\begin{proof}
With constant $K'$, the third term in~\eqref{eq:error-bound} vanishes. Substituting bounds~\eqref{eq:commutator-bound} and~\eqref{eq:commutator-deriv-bound} into the remaining two terms:
\begin{equation}
\varepsilon \leq \frac{1}{T}\left(\frac{4\lVert H'(1)\rVert}{g(1)^2} + \int_0^1 \frac{40\lVert H'(s)\rVert^2}{g(s)^3}\,ds + \int_0^1\frac{4\lVert H''(s)\rVert}{g(s)^2}\,ds\right).
\end{equation}
Setting the right side equal to $\varepsilon$ and solving for $T$ gives~\eqref{eq:constant-rate-formula}.
\end{proof}

Since $H'' = 0$ for the linear interpolation $H(s) = -(1-s)\ket{\psi_0}\bra{\psi_0} + sH_z$, and $H'(s) = \ket{\psi_0}\bra{\psi_0} + H_z$ is constant with $\lVert H'\rVert = O(1)$, the dominant term in~\eqref{eq:constant-rate-formula} is $\int_0^1 g(s)^{-3}\,ds$. From the gap profile of \autoref{thm:complete-profile}, the crossing window contributes
\begin{equation}
\label{eq:constant-rate-window}
\int_{s^*-\delta_s}^{s^*} g(s)^{-3}\,ds \leq \frac{\delta_s}{g_{\min}^3} = \frac{A_2}{A_1(A_1+1)}\cdot g_{\min}^{-2},
\end{equation}
using $\delta_s = A_2 g_{\min}/(A_1(A_1+1))$ from Eq.~\eqref{eq:gmin-deltas-relation}. This gives $T_{\mathrm{constant}} = O(\delta_s/(\varepsilon\, g_{\min}^3))$.

Specializing to $M=2$ with $g_{\min}=1/\sqrt{N}$, the exact gap $g(s)=\sqrt{(2s-1)^2+4s(1-s)/N}$ from Eq.~\eqref{eq:grover-gap} gives $\int_0^1 g(s)^{-3}\,ds = O(N)$. The dominant contribution comes from the $O(1/\sqrt{N})$ window where $g\approx 1/\sqrt{N}$. Hence $T_{\mathrm{constant}}=O(N/\varepsilon)$, matching classical search. A constant-rate schedule therefore gives no quantum speedup. It wastes time where the gap is large and still moves too quickly near $s^*$.

\section{The Adaptive Schedule}
\label{sec:adaptive-schedule}

The constant schedule fails because it treats every value of $s$ the same. The
error bound~\eqref{eq:error-bound} suggests the opposite strategy. Choose large
$K'(s)$ where the gap is small and small $K'(s)$ where the gap is large. Since
$K'(s)=dt/ds$, this means spending physical time where diabatic transitions are
dangerous. The natural ansatz is $K'(s)\propto 1/g(s)^p$ for some $p\ge 1$.
Then runtime scales as $T\propto\int_0^1 g(s)^{-p}\,ds$, while the error terms
involve integrals of the form $\int g^{q-3}\,ds$.

The exponent $p$ controls the runtime-error tradeoff. The family
$K'(s)\propto 1/g_0(s)^p$ extends Roland-Cerf ($p=2$) to a continuum of
schedules. At $p=1$, $\int g_0^{-1}\,ds$ is mild but the companion error
integral $\int g_0^{-2}\,ds$ diverges for piecewise linear profiles. At $p=2$,
the scaling is favorable, but the schedule-variation term requires exact gap
information. The useful regime is therefore $p\in(1,2)$. There,
$\int g_0^{-p}\,ds = O(g_{\min}^{1-p})$ and
$\int g_0^{p-3}\,ds = O(g_{\min}^{p-2})$, whose product is always
$O(g_{\min}^{-1})$. This is exactly the range where a certified lower bound
$g_0\le g$ suffices.

The adaptive rate theorem, extending the eigenpath traversal framework of \cite{cunningham2024eigenpath} to the continuous-time setting, formalizes this trade-off.

\begin{theorem}[Adaptive rate {\cite{braida2024unstructured}}]
\label{thm:adaptive-rate}
Let $H(s)$ satisfy the conditions of \autoref{lem:error-bound}, and let $g_0: [0,1] \to \mathbb{R}^+$ be an absolutely continuous function satisfying $g_0(s) \leq g(s)$ for all $s$. Suppose there exist $1 < p < 2$ (the endpoints are excluded: at $p = 1$ the $B_1$ integral diverges logarithmically, and at $p = 2$ the schedule variation term requires the exact gap) and constants $B_1, B_2 \geq 1$ such that
\begin{equation}
\label{eq:B1-condition}
\int_0^1 \frac{ds}{g_0(s)^p} \leq B_1\, g_{\min}^{1-p} \qquad \text{and} \qquad \int_0^1 \frac{ds}{g_0(s)^{3-p}} \leq B_2\, g_{\min}^{p-2}.
\end{equation}
Assume additionally that $g_0(1) \geq g_{\min}$, that there exists $b \in (0,1]$ with $g_0(s) \geq b\, g_{\min}$ for all $s \in [0,1]$, and that $\sup_{s \in [0,1]} |g_0'(s)| < \infty$.
Define
\begin{equation}
\label{eq:c-constant}
c = \sup_{s \in [0,1]}\left(4\lVert H'(s)\rVert + 40\lVert H'(s)\rVert^2 B_2 + 4\lVert H''(s)\rVert + 6p\,|g_0'(s)|\,\lVert H'(s)\rVert\, B_2\right).
\end{equation}
The last term uses $|g_0'(s)|$ rather than $|g'(s)|$: since the schedule is defined in terms of $g_0$, the derivative $(K'^{-1})' \propto (g_0^p)'$ involves $g_0'$. Then the schedule
\begin{equation}
\label{eq:adaptive-schedule}
K'(s) = \frac{1}{\varepsilon}\cdot\frac{c}{g_0(s)^p \cdot g_{\min}^{2-p}}
\end{equation}
achieves error at most $\varepsilon$, with total runtime
\begin{equation}
\label{eq:adaptive-runtime}
T = \int_0^1 K'(s)\,ds \leq \frac{c\, B_1}{\varepsilon\, g_{\min}}.
\end{equation}
\end{theorem}

\begin{proof}
Let $\varepsilon_0$ denote the actual error. Substituting~\eqref{eq:adaptive-schedule} into the error bound~\eqref{eq:error-bound}: $(K')^{-1} = \varepsilon\, g_0^p\, g_{\min}^{2-p}/c$, and $|((K')^{-1})'| = (\varepsilon\, g_{\min}^{2-p}/c)\cdot p\, g_0^{p-1}\,|g_0'|$. The three terms become
\begin{multline}
\label{eq:adaptive-error-expanded}
\varepsilon_0 \leq \frac{\varepsilon}{c}\, g_{\min}^{2-p}\biggl(g_0(1)^p\left\lVert\left[P'(1), (H(1)-\lambda_0(1))^+\right]\right\rVert \\
+ \int_0^1 g_0^p\left\lVert\left[P', (H-\lambda_0)^+\right]'\right\rVert ds + \int_0^1 p\, g_0^{p-1}|g_0'|\left\lVert\left[P', (H-\lambda_0)^+\right]\right\rVert ds\biggr).
\end{multline}

\textbf{Boundary term.} Using bound~\eqref{eq:commutator-bound} with $g_0 \leq g$:
\begin{equation}
g_{\min}^{2-p}\, g_0(1)^p \cdot \frac{4\lVert H'(1)\rVert}{g(1)^2} \leq 4\lVert H'(1)\rVert\, g_{\min}^{2-p}\, g_0(1)^{p-2} \leq 4\lVert H'\rVert,
\end{equation}
since $g_0(1) \geq g_{\min}$ and $p - 2 < 0$ imply $g_0(1)^{p-2} \leq g_{\min}^{p-2}$.

\textbf{Commutator derivative integral.} Using bound~\eqref{eq:commutator-deriv-bound} and splitting:
\begin{align}
g_{\min}^{2-p}\int_0^1 g_0^p \cdot \frac{40\lVert H'\rVert^2}{g^3}\,ds &\leq 40\lVert H'\rVert^2\, g_{\min}^{2-p}\int_0^1 \frac{ds}{g_0^{3-p}} \leq 40\lVert H'\rVert^2 B_2, \label{eq:proof-term-H-prime}
\end{align}
where $g_0^p/g^3 \leq g_0^p/g_0^3 = 1/g_0^{3-p}$ since $g_0 \leq g$, and the $B_2$ condition~\eqref{eq:B1-condition} absorbs $g_{\min}^{2-p}\cdot g_{\min}^{p-2} = 1$. Similarly, the $H''$ sub-term contributes
\begin{equation}
g_{\min}^{2-p}\int_0^1 g_0^p \cdot \frac{4\lVert H''\rVert}{g^2}\,ds \leq 4\lVert H''\rVert\, g_{\min}^{2-p}\int_0^1 \frac{ds}{g_0^{2-p}} \leq 4\lVert H''\rVert, \label{eq:proof-term-H-double-prime}
\end{equation}
since $g_0 \geq b\, g_{\min}$ and $p - 2 < 0$ imply $g_0^{p-2} \leq b^{p-2} g_{\min}^{p-2}$, giving $\int g_0^{p-2}\,ds = O(g_{\min}^{p-2})$ with the constant $b^{p-2}$ absorbed into the $O$-notation.

\textbf{Schedule variation integral.} Using bound~\eqref{eq:commutator-bound}:
\begin{align}
g_{\min}^{2-p}\int_0^1 p\, g_0^{p-1}|g_0'| \cdot \frac{4\lVert H'\rVert}{g^2}\,ds &\leq 4p\,\lVert H'\rVert\, g_{\min}^{2-p}\int_0^1 \frac{g_0^{p-1}\,|g_0'|}{g_0^2}\,ds \nonumber \\
&= 4p\,\lVert H'\rVert\, g_{\min}^{2-p}\int_0^1 g_0^{p-3}\,|g_0'|\,ds. \label{eq:proof-schedule-variation}
\end{align}
Using $\sup|g_0'| < \infty$, we have $\int g_0^{p-3}|g_0'|\,ds \leq \sup|g_0'|\cdot \int g_0^{p-3}\,ds \leq \sup|g_0'|\cdot B_2\, g_{\min}^{p-2}$. The resulting bound is $4p\,\sup|g_0'|\,\lVert H'\rVert\, B_2$. The constant $c$ in~\eqref{eq:c-constant} uses the factor $6p$ rather than $4p$; this is a valid overestimate that simplifies the expression without affecting the asymptotic result.

\textbf{Collecting.} Summing all contributions:
\begin{equation}
\varepsilon_0 \leq \frac{\varepsilon}{c}\left(4\lVert H'\rVert + 40\lVert H'\rVert^2 B_2 + 4\lVert H''\rVert + 6p\,\sup|g_0'|\,\lVert H'\rVert\, B_2\right) \leq \frac{\varepsilon}{c}\cdot c = \varepsilon.
\end{equation}

\textbf{Runtime.} The total evolution time is
\begin{equation}
T = \int_0^1 K'\,ds = \frac{c}{\varepsilon}\, g_{\min}^{p-2}\int_0^1 \frac{ds}{g_0^p} \leq \frac{c}{\varepsilon}\, g_{\min}^{p-2}\cdot B_1\, g_{\min}^{1-p} = \frac{c\, B_1}{\varepsilon\, g_{\min}}. \qedhere
\end{equation}
\end{proof}

Three terms compose the error. The first is a boundary term that depends on $g_0(1)$ and is $O(1)$. The second is an integral that pairs $g_0^p$ from the schedule with $g^{-3}$ from the derivative bounds, producing $\int g_0^{p-3}\,ds$. The third is a schedule-variation term from the non-constant $K'$. The parameter $p$ balances the two integrals. $B_1$ bounds $\int g_0^{-p}\,ds$ (the runtime cost), while $B_2$ bounds $\int g_0^{p-3}\,ds$ (the error cost). Their product with $g_{\min}^{-1}$ gives the final runtime.

\begin{corollary}
\label{cor:ideal-case}
If $\int_0^1 g(s)^{-p}\,ds = O(g_{\min}^{1-p})$ for all $p > 1$, and $\lVert H'\rVert$, $\lVert H''\rVert$, $|\lambda_0'|$, $|g'|$ are all $O(1)$, then $T = O(1/(\varepsilon\, g_{\min}))$.
\end{corollary}

The runtime scales inversely with the minimum gap, which is optimal for quantum search \cite{farhi2008fail}. The running example satisfies these conditions.

The integral $\int_0^1 g(s)^{-p}\,ds$ is dominated by the $O(1/\sqrt{N})$-wide window where $g \approx 1/\sqrt{N}$: the window's contribution is $(1/\sqrt{N})\cdot N^{p/2} = N^{(p-1)/2}$, while outside the window $g = \Omega(|s - 1/2|)$ and the integral converges. For any $p > 1$, this gives $O(g_{\min}^{1-p})$.

\begin{lemma}[Grover gap integral]
\label{lem:grover-integral}
For the exact gap $g(s) = \sqrt{(2s-1)^2 + 4s(1-s)/N}$ of the running example ($M = 2$, $d_0 = 1$, $d_1 = N-1$),
\begin{equation}
\label{eq:grover-integral-bound}
\int_0^1 g(s)^{-p}\,ds = O\left(N^{(p-1)/2}\right) = O\left(g_{\min}^{1-p}\right) \qquad \text{for all } p > 1.
\end{equation}
\end{lemma}

\begin{proof}
The gap is symmetric about $s = 1/2$ and achieves its minimum $g_{\min} = 1/\sqrt{N}$ there. Split the integral at $1/2 - 1/\sqrt{N}$. In the window $[1/2 - 1/\sqrt{N},\, 1/2]$, bound $g \geq g_{\min}$:
\begin{equation}
\int_{1/2-1/\sqrt{N}}^{1/2} g^{-p}\,ds \leq \frac{1}{\sqrt{N}}\cdot N^{p/2} = N^{(p-1)/2}.
\end{equation}
Outside the window, $g(s) \geq c|s - 1/2|$ for a constant $c > 0$ (the gap grows linearly away from the minimum). The change of variable $u = g(s)$, with $|ds/du| = O(1)$ since $|g'(s)| \leq 2$, gives
\begin{equation}
\int_0^{1/2-1/\sqrt{N}} g^{-p}\,ds \leq C\int_{1/\sqrt{N}}^{O(1)} u^{-p}\,du = O\left(N^{(p-1)/2}\right).
\end{equation}
Combining and using the symmetry about $1/2$ gives the result.
\end{proof}

The other conditions of \autoref{cor:ideal-case} are immediate: $\lVert H'\rVert = \lVert\ket{\psi_0}\bra{\psi_0} + H_z\rVert \leq 2$, $H'' = 0$, $|\lambda_0'| \leq \lVert H'\rVert \leq 2$ by the Hellmann-Feynman theorem, and $|g'(s)| \leq 2$ (from $|g'| = |4(1-1/N)(1/2 - s)/g| \leq 2$, since the numerator is at most $2g$). Therefore $T = O(\sqrt{N}/\varepsilon)$ for the running example with an adaptive schedule, compared to $T = O(N/\varepsilon)$ with a constant schedule. The adaptive schedule recovers the full Grover speedup.

The schedule $K'(s) \propto 1/g(s)^p$ concentrates the evolution time near the crossing: at $s = 1/2$, where $g \approx 1/\sqrt{N}$, the schedule rate is $K' \propto N^{p/2}$, while far from $1/2$, where $g = O(1)$, it is $K' = O(1)$. The algorithm spends $O(\sqrt{N})$ physical time traversing the window and $O(1)$ time traversing the rest of $[0,1]$.

\section{Runtime of Adiabatic Quantum Optimization}
\label{sec:aqo-runtime}

Applying \autoref{thm:adaptive-rate} to $H(s)=-(1-s)\ket{\psi_0}\bra{\psi_0}+sH_z$ with the gap profile of \autoref{thm:complete-profile} requires three concrete steps. We construct a continuous lower bound $g_0(s)$ from the piecewise bounds, compute $B_1$ and $B_2$, and then evaluate the constant $c$.

The bounds from \autoref{thm:complete-profile} are valid on their regions but do not meet continuously at $s^*-\delta_s$ and $s^*$. The left bound is too high at $s^*-\delta_s$, and the right bound is too low at $s^*$. Since \autoref{thm:adaptive-rate} requires $g_0$ to be absolutely continuous on $[0,1]$, we shrink the left and window pieces by a constant factor $b$ so the three branches join continuously.

Define
\begin{equation}
\label{eq:g0-def}
g_0(s) = \begin{cases}
b\,\dfrac{A_1(A_1+1)}{A_2}\left(s^* - s\right), & s \in [0,\, s^* - \delta_s), \quad \text{(i.e., } b\,\tfrac{A_1}{A_2}\cdot\tfrac{s^*-s}{1-s^*}\text{)} \\[8pt]
b\, g_{\min}, & s \in [s^* - \delta_s,\, s^*), \\[8pt]
\dfrac{\Delta}{30}\cdot\dfrac{s - s_0}{1 - s_0}, & s \in [s^*,\, 1],
\end{cases}
\end{equation}
where $s_0$ is given by Eq.~\eqref{eq:s0-definition} and the shrinking factor is
\begin{equation}
\label{eq:b-def}
b = k\cdot\frac{2}{1 + f(s^*)} = \frac{1}{4}\cdot\frac{2}{1 + 4} = \frac{1}{10},
\end{equation}
using $k = 1/4$ and $f(s^*) = 4$ from Eq.~\eqref{eq:f-at-sstar}.

Each piece of $g_0$ lies below its corresponding bound from \autoref{thm:complete-profile}. The left and window pieces are scaled by $b=1/10$, while the right piece is unchanged. It remains to check continuity at both junctions. At $s=s^*-\delta_s$, the left branch gives $b\cdot A_1(A_1+1)\delta_s/A_2$. Using $\delta_s=A_2g_{\min}/(A_1(A_1+1))$ from Eq.~\eqref{eq:gmin-deltas-relation}, this equals $b\,g_{\min}=g_{\min}/10$, exactly the window value. At $s=s^*$, the window value is again $b\,g_{\min}=g_{\min}/10$, while the right branch gives $(\Delta/30)(s^*-s_0)/(1-s_0)$. Using $s^*-s_0 = k\, g_{\min}(1-s^*)/(a - k\, g_{\min})$ and $1-s_0 = (1-s^*)a/(a-k\, g_{\min})$ from Eq.~\eqref{eq:s0-definition},
\begin{equation}
\frac{\Delta}{30}\cdot\frac{s^*-s_0}{1-s_0} = \frac{\Delta}{30}\cdot\frac{k\, g_{\min}}{a} = \frac{\Delta}{30}\cdot\frac{g_{\min}/4}{\Delta/12} = \frac{g_{\min}}{10},
\end{equation}
which again matches the window value. The constants $b$, $k$, and $a$ are therefore coupled exactly to enforce continuity. In particular, $b=1/10$ absorbs both the $k=1/4$ factor from the right-side resolvent bound and the value $f(s^*)=4$ from the Chapter 6 monotonicity analysis.

The integral $\int_0^1 g_0^{-p}\,ds$ splits across the three regions. In the left region, $g_0(s) = b\, A_1(A_1+1)(s^*-s)/A_2$, so
\begin{align}
\int_0^{s^*-\delta_s} g_0^{-p}\,ds &= \left(\frac{A_2}{b\, A_1(A_1+1)}\right)^{\!p}\int_{\delta_s}^{s^*}\frac{du}{u^p} = \frac{1}{b^p}\left(\frac{A_2}{A_1(A_1+1)}\right)^{\!p}\cdot\frac{1}{(p-1)\,\delta_s^{p-1}} \nonumber \\
&= \frac{1}{b^p(p-1)}\cdot\frac{A_2}{A_1(A_1+1)}\cdot g_{\min}^{1-p}, \label{eq:B1-left}
\end{align}
where the last step uses $\delta_s^{p-1} = (A_2 g_{\min}/(A_1(A_1+1)))^{p-1}$. In the window, $g_0 = b\, g_{\min}$ is constant:
\begin{equation}
\label{eq:B1-window}
\int_{s^*-\delta_s}^{s^*} g_0^{-p}\,ds = \frac{\delta_s}{b^p\, g_{\min}^p} = \frac{1}{b^p}\cdot\frac{A_2}{A_1(A_1+1)}\cdot g_{\min}^{1-p}.
\end{equation}
Combining the left and window contributions with $b^{-p}=10^p$ gives
\[
\frac{1/(p-1)+1}{b^p}=\frac{p\,10^p}{p-1},
\]
so the combined term is $(p/(p-1))\cdot 10^p \cdot A_2/(A_1(A_1+1))\cdot g_{\min}^{1-p}$.

In the right region, $g_0(s) = (\Delta/30)(s-s_0)/(1-s_0)$, so
\begin{align}
\int_{s^*}^1 g_0^{-p}\,ds &= \left(\frac{30(1-s_0)}{\Delta}\right)^{\!p}\int_{s^*-s_0}^{1-s_0}\frac{du}{u^p} = \left(\frac{30(1-s_0)}{\Delta}\right)^{\!p}\cdot\frac{1}{(p-1)(s^*-s_0)^{p-1}} \nonumber \\
&= \frac{1}{p-1}\left(\frac{30}{\Delta}\right)^{\!p}\left(\frac{a}{k}\right)^{\!p-1}(1-s_0)\cdot g_{\min}^{1-p}, \label{eq:B1-right}
\end{align}
using $s^*-s_0 = k\, g_{\min}(1-s^*)/(a-k\, g_{\min})$ and $1-s_0 = a(1-s^*)/(a-k\, g_{\min})$. Now set $a=(4/3)k^2\Delta$ and $k=1/4$, so $a/k=\Delta/3$. This yields $(30/\Delta)^p(\Delta/3)^{p-1}=30^p/(3\Delta)$, together with $(1-s_0)\le 1/(1+A_1)$. Hence the right contribution is $3\cdot 10^p/((p-1)\Delta(1+A_1))\cdot g_{\min}^{1-p}$.

Since $\Delta A_2 \leq A_1$ (equivalently $A_2 \leq A_1/\Delta$), the left-plus-window term satisfies $A_2/(A_1(1+A_1)) \leq 1/(\Delta(1+A_1))$. Combining all three contributions gives
\begin{equation}
\label{eq:B1-result}
\int_0^1 g_0^{-p}\,ds \leq \frac{(p+3)\cdot 10^p}{(p-1)(1+A_1)\Delta}\cdot g_{\min}^{1-p}, \qquad \text{so} \quad B_1 = O\!\left(\frac{1}{\Delta(1+A_1)}\right).
\end{equation}

The integral $\int_0^1 g_0^{p-3}\,ds$ has the same three-region decomposition, now with exponent $3-p$. Because $p\in(1,2)$ implies $3-p\in(1,2)$, each region converges by the same change of variables used for $B_1$. For example, the window contribution is
\[
\int_{s^*-\delta_s}^{s^*}(b\,g_{\min})^{p-3}\,ds
=\delta_s\,b^{p-3}g_{\min}^{p-3}
=b^{p-3}\frac{A_2}{A_1(A_1+1)}g_{\min}^{p-2}
=O(g_{\min}^{p-2}),
\]
with $b^{p-3}=10^{3-p}$ absorbed into constants. The left and right pieces have the same order, so
\begin{equation}
\label{eq:B2-result}
B_2 = O\!\left(\frac{1}{\Delta(1+A_1)}\right).
\end{equation}

For the adiabatic Hamiltonian $H(s)=-(1-s)\ket{\psi_0}\bra{\psi_0}+sH_z$, we have
\begin{equation}
\lVert H'(s)\rVert = O(1), \qquad \lVert H''(s)\rVert = 0, \qquad |\lambda_0'(s)| = O(1),
\end{equation}
because $H'(s)=\ket{\psi_0}\bra{\psi_0}+H_z$ is constant and $\lambda_0'(s)=\bra{\phi_0(s)}H'(s)\ket{\phi_0(s)}$ is bounded by $\lVert H'\rVert$ through Hellmann-Feynman. The derivative $|g_0'(s)|$ is piecewise bounded: $b\,A_1(A_1+1)/A_2$ on the left, $0$ in the window, and $\Delta/(30(1-s_0))$ on the right. For piecewise linear $g_0$, this keeps $|g_0'|B_2$ bounded. The window contributes nothing because $g_0'=0$ there. On each linear branch, $|g_0'|$ factors out and the substitution $u=g_0(s)$ gives
\[
\int g_0^{p-3}|g_0'|\,ds=\int_{g_{\min}/10}^{O(1)}u^{p-3}\,du=O(g_{\min}^{p-2}),
\]
independently of the slopes. Since $\lVert H''\rVert=0$, the dominant term in \eqref{eq:c-constant} is $40\lVert H'\rVert^2B_2$. Therefore
\begin{equation}
\label{eq:c-result}
c = O(B_2).
\end{equation}

\begin{theorem}[Runtime of AQO: Main Result 1 {\cite{braida2024unstructured}}]
\label{thm:aqo-runtime}
Let $H_z$ satisfy the spectral condition (Definition~\ref{def:spectral-condition}) and assume $A_1 \geq 1/2$ (equivalently $s^* \geq 1/3$). For any $\varepsilon > 0$, the adaptive schedule~\eqref{eq:adaptive-schedule} with the gap lower bound~\eqref{eq:g0-def} prepares the ground state of $H_z$ with fidelity at least $1 - \varepsilon$ in time
\begin{equation}
\label{eq:main-runtime}
T = O\!\left(\frac{1}{\varepsilon}\cdot\frac{\sqrt{A_2}}{\Delta^2\, A_1(A_1+1)}\cdot\sqrt{\frac{N}{d_0}}\right).\footnote{The published paper~\cite{braida2024unstructured} states $A_1^2$ in Theorem~1. The expression $A_1(A_1+1)$ follows from the proof derivation in Appendix~A-IV of the same paper. For Ising Hamiltonians with $A_1 = O(\mathrm{poly}(n))$, the distinction is absorbed by the $O(\cdot)$ notation, since $A_1(A_1+1) = A_1^2 + A_1 = \Theta(A_1^2)$.}
\end{equation}
\end{theorem}

\begin{proof}
By \autoref{thm:adaptive-rate}, $T \leq c\, B_1/(\varepsilon\, g_{\min})$. Substituting $c = O(B_2)$, $B_1 = O(1/(\Delta(1+A_1)))$, $B_2 = O(1/(\Delta(1+A_1)))$, and $g_{\min} = (2A_1/(A_1+1))\sqrt{d_0/(NA_2)}$ from Eq.~\eqref{eq:gmin-formula}:
\begin{equation}
T = O\!\left(\frac{1}{\varepsilon}\cdot\frac{B_1 B_2}{g_{\min}}\right) = O\!\left(\frac{1}{\varepsilon}\cdot\frac{1}{\Delta^2(1+A_1)^2}\cdot\frac{A_1+1}{2A_1}\sqrt{\frac{NA_2}{d_0}}\right) = O\!\left(\frac{1}{\varepsilon}\cdot\frac{\sqrt{A_2}}{\Delta^2\, A_1(A_1+1)}\cdot\sqrt{\frac{N}{d_0}}\right). \qedhere
\end{equation}
\end{proof}

The runtime in \eqref{eq:main-runtime} has five interpretable factors. The $1/\varepsilon$ term is linear in target error. The factor $\sqrt{A_2}$ measures spectral crowding near $E_0$, where larger $A_2=(1/N)\sum d_k/(E_k-E_0)^2$ sharpens the minimum and narrows the crossing window. The denominator $A_1(A_1+1)$ encodes crossing location, since larger $A_1$ pushes $s^*$ toward $1$ and steepens the left arm. The term $1/\Delta^2$ is the cost of right-side control, with one $1/\Delta$ from $B_1$ and one from $B_2$. Finally, $\sqrt{N/d_0}$ is the Grover factor. In short, the speedup is quantum. The remaining terms are explicit spectral overheads of generality.
Compared with the constant-rate theorem, the key gain is exactly here. Time
still scales linearly in the target error $1/\varepsilon$, and the minimum-gap
scaling improves from $T=O(g_{\min}^{-2})$ to $T=O(g_{\min}^{-1})$. At the
proof level, the improvement comes from replacing the $g^{-3}$ corridor with
power-law control centered on $g^{-2}$.

For Ising Hamiltonians $H_\sigma$ (Eq.~\eqref{eq:Ising-Ham}) with $A_1,A_2=O(\mathrm{poly}(n))$ and $\Delta\ge 1/\mathrm{poly}(n)$, this becomes $T=\widetilde{O}(\sqrt{N/d_0})$. That matches the lower bound of \cite{farhi2008fail} up to polylogarithmic factors. When $d_0=O(1)$, the adiabatic algorithm recovers the Grover $\sqrt{N}$ scaling.

For the canonical two-level case $M=2$ with $A_1\approx 1$, $A_2\approx 1$, $\Delta=1$, and $d_0=1$, we obtain
\begin{equation}
T = O\!\left(\frac{1}{\varepsilon}\cdot\frac{1}{1 \cdot 2}\cdot\sqrt{N}\right) = O\!\left(\frac{\sqrt{N}}{\varepsilon}\right),
\end{equation}
matching the circuit-based Grover algorithm. The adaptive adiabatic approach achieves the same quadratic speedup through a smooth interpolation between two Hamiltonians, without relying on the discrete iterate structure of amplitude amplification.

Runtime is controlled by how much spectral information the schedule receives.
The constant-rate schedule (\autoref{thm:constant-rate}) gives
$T=O(N/\varepsilon)$ because the crossing window dominates. The Roland-Cerf
schedule (\autoref{sec:prior-theorems}) sets $K'(s)\propto 1/g(s)^2$ and
improves this to $T=O(\sqrt{N}/\varepsilon)$, but it needs the exact gap at
every point. The adaptive schedule in \autoref{thm:adaptive-rate} keeps the same
$\sqrt{N}$ scaling while using only a certified lower bound
$g_0(s)\le g(s)$, namely the Chapter 6 profile bounds. That change, from exact
gap to lower bound, is what makes the method usable for general
spectral-condition instances.
The discrete-time eigenpath route of Boixo et al.~\cite{boixo2009eigenpath}
reaches the same $O(1/g_{\min})$ scaling through a different mechanism, so this
scaling is not tied to one proof template.

Guo and An~\cite{GuoAn2025} place this in a broader framework. They study
interpolations $H(u(s))=(1-u(s))H_0+u(s)H_1$ under a measure condition. The set
$\{s:\Delta(u(s))\le x\}$ has Lebesgue measure $O(x)$ as $x\to 0$. Under that
condition, the power-law schedule with exponent $p=3/2$ achieves
$O(1/\Delta_*)$, a quadratic improvement over $O(1/\Delta_*^2)$. They also show
that $p=3/2$ is optimal for linear gap profiles. The profile in
\autoref{thm:complete-profile} satisfies their condition because it has one
minimum at $s^*$ and linear reopening on both sides. Equivalently, for small
$x$, the set $\{s : g(s)\le x\}$ is an interval around $s^*$ of width
$\Theta(x)$. Their theorem provides the
existence-and-scaling layer; Chapters 5 and 6 add explicit constants, which is
what turns \eqref{eq:main-runtime} into a concrete runtime bound.
This condition is also the geometric dividing line between narrow, sharp minima
and broad, flat ones: sharp minima satisfy it with a bounded constant, while
broad flat minima force larger overhead.

Running the adaptive schedule still requires knowledge of $g_0(s)$, hence of
$s^*$, $\delta_s$, and $g_{\min}$. These all depend on $A_1$. Inside the window
$[s^*-\delta_s,s^*)$, the schedule is constant,
$K'=c/(\varepsilon\,b^p\,g_{\min}^2)$, so the main issue is not local slope but
window placement. Because $s^*=A_1/(A_1+1)$, we must know $A_1$ to accuracy
$O(\delta_s)=O(2^{-n/2})$ to center the slow segment correctly. Outside the
window, placement errors change only constants. Inside this exponentially narrow
window, the same error causes a real loss of fidelity.

Imprecision in $A_2$ and $d_0$ causes only polynomial slowdown. Replacing $A_2$ by the lower bound $1-d_0/N$ (Eq.~\eqref{eq:A2-lower-bound}) and taking $d_0=1$ changes runtime only by $\mathrm{poly}(n)$ factors, because these parameters appear through $\sqrt{A_2/d_0}$ and $B_1$. The critical quantity is $A_1$. It must be estimated to additive accuracy $O(\delta_s)=O(2^{-n/2})$ before evolution begins. That requirement is severe: the needed precision is exponential in $n$ even though $H_z$ itself has a $\mathrm{poly}(n)$-bit description. Chapter 8 shows that even approximating $A_1$ to much coarser precision $1/\mathrm{poly}(n)$ is NP-hard, while exact computation is $\#$P-hard.
