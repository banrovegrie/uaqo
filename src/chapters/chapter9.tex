% Chapter 9: Information Gap
% ASSUMES: Chapter 5 defines H(s), H_z, H_0, |psi_0>, A_p, A_1, A_2,
%   s^*, delta_s, g_min, hat{g}, eigenvalue equation, spectral condition,
%   symmetric subspace, grover-gap.
% ASSUMES: Chapter 6 proves gap-left, gap-right, complete-profile, f(s*)=4, s_0,
%   c_L = A_1(A_1+1)/A_2, c_R = Delta/30, piecewise linear lower bound.
% ASSUMES: Chapter 7 defines runtime theorem, JRS bound, RC local condition,
%   p=2 schedule, integral T = (C/eps) int g^{-2} ds.
% ASSUMES: Chapter 8 proves NP-hardness (Thm 8.1), #P-hardness (Thm 8.2),
%   interpolation barrier (Thm 8.3), quantum algorithm (Thm 8.4),
%   classical lower bound (Thm 8.5), quadratic separation (Cor 8.1).

The adiabatic algorithm works. Chapter~7 proves it reaches the Grover speedup $\widetilde{O}(\sqrt{N/d_0})$ with an optimal local schedule. But the schedule passes through an avoided crossing at $s^* = A_1/(A_1+1)$, and Chapter~8 proves that computing $A_1$ from a classical description of $H_z$ is NP-hard. Without $s^*$, the schedule designer does not know where the gap reaches its minimum and cannot concentrate the slowdown where it matters. In the circuit model, the D\"urr-H\o yer algorithm~\cite{durr1996quantum} achieves the same speedup without computing $s^*$, without $A_1$, without the gap profile, and without any spectral parameter at all. It does not traverse an interpolation path. It never encounters an avoided crossing.

The discrepancy between these two facts is the subject of this chapter. The adiabatic approach to unstructured search pays an information cost that the circuit approach does not. That cost turns out to have precise mathematical structure. An uninformed fixed schedule loses a factor of $\Omega(2^{n/2})$ in runtime. Each additional bit of $A_1$ knowledge halves the penalty, with no abrupt phase transition. Quantum measurement during evolution recovers the full speedup with $\Theta(n)$ binary probes. No instance-independent modification within the rank-one framework can eliminate the spectrum dependence. And the hardness of $A_1$ is counting hardness, not optimization hardness, connected to partition functions rather than satisfiability. The information gap is a property of the computational model, not of the computational task.

The same phenomenon may arise whenever a quantum algorithm navigates a parametrized energy landscape: the mechanism itself creates an information requirement absent from the raw problem specification. Variational algorithms need good initial parameters, quantum annealing needs an appropriate cooling schedule, and adiabatic algorithms need the crossing position. In each case, the bottleneck is not the hardness of the underlying problem but the cost of learning where, in parameter space, the algorithm must concentrate its effort. The rank-one unstructured search studied here is the setting where this cost admits exact quantification, and the results are sharp enough to yield a complete taxonomy.


\section{The Cost of Ignorance}
\label{sec:cost-of-ignorance}

Throughout this chapter, asymptotic notation ($O$, $\Omega$, $\Theta$) refers to
the limit $N \to \infty$ (equivalently $n \to \infty$ with $N = 2^n$). Unless
stated otherwise, the spectral parameters $d_0$, $M$, $\Delta$, $A_1$, $A_2$
and the target error $\varepsilon$ are treated as fixed positive constants
independent of $n$. When we write ``$O(T_{\mathrm{inf}})$,'' the hidden constant
may depend on these spectral parameters but not on $n$.

A \emph{fixed schedule} is chosen before the instance is revealed. It may depend
on problem size $n$ and target error $\varepsilon$, but not on instance-specific
spectral data. An \emph{instance-independent} algorithm uses the same
Hamiltonian design for all energy assignments with the same degeneracy
structure.

NP-hardness is a statement about worst-case classical computation. It does not directly tell us how much runtime an adiabatic algorithm loses by not knowing $A_1$. If a fixed schedule that ignores $A_1$ still achieved $O(\sqrt{N/d_0})$, the hardness of computing $A_1$ would be academic. It is not. The right way to quantify the loss is through a game. The schedule designer commits to a velocity profile $v(s)$ before seeing the instance. An adversary, knowing the schedule, then places the gap minimum at the worst possible position within an uncertainty interval. The separation between what the informed designer achieves and what the uninformed designer can guarantee is a minimax ratio, and it grows exponentially. To make this game precise, we need a class of gap functions that the adversary is allowed to play. The adversary's freedom is parametrized by three quantities: the endpoints $s_L, s_R$ of the uncertainty interval (encoding what the schedule designer does not know) and the minimum gap value $\Delta_*$ (encoding what is fixed across instances). Requiring a unique minimum avoids degenerate cases where multiple crossings interact, which is the generic situation for the rank-one family studied here.

\begin{definition}[Gap class]
\label{def:gap-class}
The gap class $\mathcal{G}(s_L, s_R, \Delta_*)$ consists of all gap functions $g: [0,1] \to \mathbb{R}_{>0}$ such that the minimum $g(s^*) = \Delta_*$ is achieved at a unique point $s^* \in [s_L, s_R]$, and $g(s) > \Delta_*$ for all $s \neq s^*$.
\end{definition}

\noindent For the running example ($M = 2$, $d_0 = 1$, $N = 4$), $s^* = A_1/(A_1+1) = 3/7$ and $\Delta_* = g_{\min} = 1/\sqrt{4} = 1/2$. Any gap function in $\mathcal{G}(0, 1, 1/2)$ has minimum value $1/2$ somewhere in $[0, 1]$. The adversary controls where that minimum sits.

The schedule designer's constraint is the Roland-Cerf local condition, which ties velocity to the gap at each point. This is the natural admissibility class: the schedule must traverse the gap slowly enough to maintain adiabaticity, and it must do so for every instance simultaneously, because the schedule is fixed before the instance is revealed.

\begin{definition}[RC-admissible fixed schedules]
\label{def:rc-admissible}
Fix a family of Hamiltonian instances. A fixed schedule $u$ with velocity profile $v(s)=|ds/dt|$ is \emph{RC-admissible} on an instance if it satisfies the local Roland-Cerf condition
\[
v(s) \leq \frac{\varepsilon\, g(s)^2}{\|H'(s)\|}\qquad\forall s\in[0,1].
\]
It is \emph{uniformly RC-admissible} on the family if this inequality holds for every instance in the family.
\end{definition}

\noindent The parameter $\Delta_*$ is the minimum of the abstract gap function $g$, distinct from the spectral gap $\Delta = E_1 - E_0$ of $H_z$ and the rank-one minimum $g_{\min}$ of $H(s)$. For rank-one gap profiles, $\Delta_* = g_{\min} = \Theta(\sqrt{d_0/(NA_2)})$.

The schedule induces a velocity profile $v(s) > 0$ on $[0,1]$, with total evolution time $T = \int_0^1 v(s)^{-1}\,ds$.

Within the uniformly RC-admissible class (Definition~\ref{def:rc-admissible}),
the crossing velocity obeys a pointwise bound. Since
$H'(s) = \ket{\psi_0}\!\bra{\psi_0} + H_z$ with $H_z \succeq 0$ and
$\lVert \ket{\psi_0}\!\bra{\psi_0}\rVert = 1$, we have
$\lVert H'(s)\rVert \ge 1$ for all $s$. (Also
$\lVert H'(s)\rVert \le 2$ from $\lVert H_z\rVert \le 1$.) Therefore
RC-admissibility gives
\[
v(s) \leq \varepsilon\,g(s)^2.
\]
At a crossing point where $g=\Delta_*$, this yields $v \leq \varepsilon\,\Delta_*^2$.

The crossing window has width $\delta_s = \Theta(\Delta_*)$. More precisely, Chapter~5 gives $\delta_s = \hat{g}/c_L$ (\autoref{eq:gmin-deltas-relation}), where $\hat{g} = \frac{2A_1}{A_1+1}\sqrt{d_0/(NA_2)}$ and $g_{\min} = (1 \pm O(\eta))\hat{g}$. Since $c_L = A_1(A_1+1)/A_2$ is a positive constant independent of $n$, the proportionality $\delta_s = \Theta(g_{\min}) = \Theta(\Delta_*)$ holds with constants depending only on the fixed spectral parameters $A_1$, $A_2$, $d_0$.

Define the reference scale $v_{\mathrm{slow}} = \varepsilon\,\Delta_*^2$. Since
the family assumption $g_{\min}=\Theta(\Delta_*)$ means $g_{\min}$ and
$\Delta_*$ differ by at most fixed multiplicative constants, all crossing
velocities are comparable to $v_{\mathrm{slow}}$ up to constants independent of
$n$. Both the uninformed and informed runtimes are computed under the same RC
condition, so these constants cancel in the ratio $T_{\mathrm{unf}}/T_{\mathrm{inf}}$.
The separation depends on the geometric factor $(s_R - s_L)/\Delta_*$ alone.

\begin{lemma}[Adversarial gap construction]
\label{lem:adversarial-gap}
For any $s_{\mathrm{adv}} \in [s_L, s_R]$ and $\Delta_* > 0$, the gap function $g_{\mathrm{adv}}(s) = \Delta_* + (s - s_{\mathrm{adv}})^2$ belongs to $\mathcal{G}(s_L, s_R, \Delta_*)$.
\end{lemma}

\begin{proof}
The function satisfies $g_{\mathrm{adv}}(s_{\mathrm{adv}}) = \Delta_*$, $g_{\mathrm{adv}}(s) > \Delta_*$ for $s \neq s_{\mathrm{adv}}$, and $g_{\mathrm{adv}}(s) > 0$ for all $s$.
\end{proof}

\begin{lemma}[Velocity bound for uninformed schedules]
\label{lem:velocity-bound}
Let $u$ be a fixed schedule that is uniformly RC-admissible on the rank-one
family in \autoref{thm:separation}. Then there exists a constant
$C_{\mathrm{vel}}>0$ independent of $n$ such that
$v(s) \leq C_{\mathrm{vel}}\,v_{\mathrm{slow}}$ for all $s \in [s_L, s_R]$,
provided $N$ is sufficiently large that $\Delta_* < \min(1 - s_R, s_L)$
(in particular, $\Delta_* \leq 1-s'$ for all $s' \in [s_L,s_R]$, which makes
the adversary's construction feasible).
\end{lemma}

\begin{proof}
Suppose $v(s') > C_{\mathrm{vel}}\,v_{\mathrm{slow}}$ for some
$s' \in [s_L, s_R]$. We construct a physical Hamiltonian in the rank-one family
whose gap minimum occurs at $s'$, then apply RC-admissibility on that instance.

The two-level family has two free parameters: excitation gap $\Delta > 0$ and
solution fraction $\rho = d_0/N$. Since $s^* = A_1/(A_1+1)$ with
$A_1 = (1-\rho)/\Delta$, fixing the crossing at $s'$ constrains
$\Delta = (1-\rho)(1-s')/s'$, leaving $\rho$ free. Substituting into the gap
formula (\autoref{eq:gmin-formula}), which gives
$\hat{g} = \frac{2A_1}{A_1+1}\sqrt{d_0/(NA_2)}$, and using the two-level
simplifications $A_1 = (1-\rho)/\Delta$, $A_2 = (1-\rho)/\Delta^2$,
$A_1/(A_1+1) = s'$, and $d_0/(NA_2) = \rho\Delta^2/(1-\rho)$, yields
$\hat{g} = 2(1-s')\sqrt{\rho(1-\rho)}$. Choosing $\rho$ to satisfy
$\hat{g} = \Delta_*$ requires
$\rho(1-\rho) = \Delta_*^2/(4(1-s')^2)$, which is feasible for
$\Delta_* \leq 1-s'$ (holding asymptotically since
$\Delta_* = \Theta(2^{-n/2})$ and $s' \leq s_R < 1$). The resulting
Hamiltonian $H_z$ has crossing at $s'$ and $g_{\min} = \Theta(\Delta_*)$.
The adversary in the minimax game selects this physical Hamiltonian, not merely
an abstract gap function.

Because $g_{\min}=\Theta(\Delta_*)$ on the family, there exists
$c_+>0$ (independent of $n$) such that $g_{\min}\le c_+\Delta_*$. Uniform
RC-admissibility on this Hamiltonian gives
\[
v(s') \leq \frac{\varepsilon\,g_{\min}^2}{\lVert H'(s') \rVert}
\leq \varepsilon\,c_+^2\,\Delta_*^2
= c_+^2\,v_{\mathrm{slow}},
\]
using $\lVert H'(s')\rVert\ge 1$. Choosing
$C_{\mathrm{vel}}=c_+^2$ contradicts $v(s')>C_{\mathrm{vel}}v_{\mathrm{slow}}$.
The key point is that the adversary constructs a different physical Hamiltonian
for each probe point $s'$: the schedule must be simultaneously admissible on
all of them. Because $s'$ was arbitrary in $[s_L,s_R]$, the bound holds
throughout the interval.
\end{proof}

\begin{theorem}[Separation (uniformly RC-admissible class)]
\label{thm:separation}
Let $T_{\mathrm{unf}}$ be the minimum time over all fixed schedules that are uniformly RC-admissible for all rank-one instances whose gap minima satisfy $s^* \in [s_L,s_R]$ and $g_{\min} = \Theta(\Delta_*)$, and let $T_{\mathrm{inf}}$ be the corresponding optimal informed runtime (with known $s^*$) on the same rank-one family. Then
\begin{equation}
\label{eq:separation-ratio}
\frac{T_{\mathrm{unf}}}{T_{\mathrm{inf}}} = \Omega\!\left(\frac{s_R - s_L}{\Delta_*}\right).
\end{equation}
\end{theorem}

\begin{proof}
By \autoref{lem:velocity-bound}, $v(s) \leq C_{\mathrm{vel}}v_{\mathrm{slow}}$
for all $s \in [s_L, s_R]$. The uninformed time satisfies
\begin{equation}
T_{\mathrm{unf}} = \int_0^1 \frac{ds}{v(s)} \geq \int_{s_L}^{s_R} \frac{ds}{v(s)} \geq \frac{s_R - s_L}{C_{\mathrm{vel}}\,v_{\mathrm{slow}}}.
\end{equation}
The informed schedule knows $s^*$ exactly and needs to be slow only in the crossing window of width $O(\Delta_*)$. For rank-one profiles, $\delta_s = \hat{g}/c_L = \Theta(g_{\min}) = \Theta(\Delta_*)$ by \autoref{eq:gmin-deltas-relation}, so \autoref{thm:aqo-runtime} gives $T_{\mathrm{inf}} = \Theta(\delta_s/v_{\mathrm{slow}}) = \Theta(\Delta_*/v_{\mathrm{slow}})$. The velocity factors cancel:
\begin{equation}
\frac{T_{\mathrm{unf}}}{T_{\mathrm{inf}}} = \Omega\!\left(\frac{s_R - s_L}{\Delta_*}\right). \qedhere
\end{equation}
\end{proof}

\begin{corollary}[Constant-width uncertainty family]
\label{cor:separation-grover}
For any $n$-qubit rank-one family in which the minimum gap scales as $\Delta_* = \Theta(2^{-n/2})$, the crossing uncertainty interval has constant width $s_R - s_L = \Theta(1)$, and the endpoints are bounded away from the boundaries ($\exists\,\gamma>0$ independent of $n$ with $\gamma \le s_L < s_R \le 1-\gamma$), the minimax separation satisfies
\[
\frac{T_{\mathrm{unf}}}{T_{\mathrm{inf}}} = \Omega(2^{n/2}).
\]
\end{corollary}

The ratio $(s_R - s_L)/\Delta_*$ has a direct geometric reading. The numerator measures how much of the schedule the designer must keep slow, because the crossing might be anywhere in $[s_L, s_R]$. The denominator measures the width of the region that actually needs to be slow around the true crossing. An informed designer slows down over a window of width $O(\Delta_*)$. An uninformed designer must slow down over the entire interval $[s_L, s_R]$. The ratio of the two windows is the overhead.

For the running example ($N = 4$, $d_0 = 1$), consider the toy geometry with $s_R - s_L = 1$ and $\Delta_* = 1/2$. The separation ratio is $2 = \sqrt{N}$, consistent with the general corollary. The corollary itself is a worst-case uncertainty statement. It does not apply to the standard Grover setting, where $s^* = 1/2 + O(1/N)$ is known a priori.

If classical preprocessing runs in polynomial time, NP-hardness forces a gap-uninformed model for fixed schedules. Inside the uniformly RC-admissible class, that model pays an $\Omega(2^{n/2})$ overhead from the adversarial geometry of \autoref{lem:adversarial-gap}. The penalty is geometric, not a byproduct of the hardness reduction.


\section{Partial Knowledge and Hedging}
\label{sec:partial-knowledge}

The separation theorem gives the extremes: perfect knowledge yields $T_{\mathrm{inf}}$, and total ignorance yields $\Omega(2^{n/2}) \cdot T_{\mathrm{inf}}$. But NP-hardness does not mean total ignorance. A polynomial-time heuristic might approximate $A_1$ to some finite precision. A brute-force search over a fraction of the state space might narrow the uncertainty. The natural question is: what is partial knowledge worth? Is there a threshold precision below which the penalty suddenly drops, or does each additional digit buy a proportional improvement?

Throughout this section, $\varepsilon$ denotes precision in $A_1$ estimation (distinct from the adiabatic target error $\varepsilon$ in Definition~\ref{def:rc-admissible}, which is fixed and absorbed into the implicit constants of Section~\ref{sec:cost-of-ignorance}).

Suppose an algorithm has access to an estimate $A_{1,\mathrm{est}}$ satisfying $|A_{1,\mathrm{est}} - A_1| \leq \varepsilon$. The uncertainty propagates to the crossing position through the map $f(x) = x/(x+1)$, whose derivative is $f'(x) = 1/(x+1)^2$.

\begin{lemma}[$A_1$-to-$s^*$ precision propagation]
\label{lem:precision-propagation}
If $|A_{1,\mathrm{est}} - A_1| \leq \varepsilon$ with $|\varepsilon| \leq (1+A_1)/2$, then $|s^*_{\mathrm{est}} - s^*| \leq 2|\varepsilon|/(A_1+1)^2$.
\end{lemma}

\begin{proof}
Direct computation gives the exact identity
\begin{equation}
\label{eq:precision-exact}
s^*_{\mathrm{est}} - s^* = \frac{A_1 + \varepsilon}{1 + A_1 + \varepsilon} - \frac{A_1}{1 + A_1} = \frac{\varepsilon}{(1+A_1)(1+A_1+\varepsilon)}.
\end{equation}
Under $|\varepsilon| \leq (1+A_1)/2$, the denominator satisfies $1 + A_1 + \varepsilon \geq (1+A_1)/2$, so
\begin{equation}
|s^*_{\mathrm{est}} - s^*| \leq \frac{|\varepsilon|}{(1+A_1) \cdot (1+A_1)/2} = \frac{2|\varepsilon|}{(1+A_1)^2}. \qedhere
\end{equation}
\end{proof}

Given $A_1$ precision $\varepsilon$, \autoref{lem:precision-propagation} places
the true crossing within radius $2\varepsilon/(A_1+1)^2$ of the estimate. The
uncertainty interval therefore has width
$W(\varepsilon) = 4\varepsilon/(A_1+1)^2$. Define the $\varepsilon$-informed
gap class as $\mathcal{G}_\varepsilon = \mathcal{G}(s_L(\varepsilon),
s_R(\varepsilon), \Delta_*)$, where the endpoints come from this interval.
Applying \autoref{thm:separation} gives the lower bound. A matching upper bound
comes from a schedule that stays uniformly slow on this interval and fast
outside it.

\begin{theorem}[Interpolation]
\label{thm:interpolation}
For additive $A_1$ precision
$0<\varepsilon\le (1+A_1)/2$, the optimal adiabatic runtime satisfies
\begin{equation}
\label{eq:interpolation}
T(\varepsilon) = \Theta\!\left(T_{\mathrm{inf}} \cdot \max\!\left(1, \frac{\varepsilon}{\delta_{A_1}}\right)\right),
\end{equation}
where $\delta_{A_1} = 2\sqrt{d_0 A_2/N}$ is the precision threshold for optimality.
\end{theorem}

\begin{proof}
\emph{Lower bound.} For $\varepsilon \geq \delta_{A_1}$, \autoref{thm:separation} applied to $\mathcal{G}_\varepsilon$ gives $T(\varepsilon) \geq W(\varepsilon)/v_{\mathrm{slow}}$. Taking the ratio with $T_{\mathrm{inf}} = \Theta(\delta_s/v_{\mathrm{slow}})$ and using the identity
\begin{equation}
\label{eq:precision-identity}
(A_1+1)^2 \cdot \delta_s = (A_1+1)^2 \cdot \frac{2}{(A_1+1)^2}\sqrt{\frac{d_0 A_2}{N}} = 2\sqrt{\frac{d_0 A_2}{N}} = \delta_{A_1}
\end{equation}
yields $T(\varepsilon)/T_{\mathrm{inf}} \geq \Theta(\varepsilon/\delta_{A_1})$. For $\varepsilon < \delta_{A_1}$, the trivial bound $T(\varepsilon) \geq T_{\mathrm{inf}}$ holds regardless of precision.

\emph{Upper bound.} Let $J=[s_L(\varepsilon),s_R(\varepsilon)]$ with width
$W(\varepsilon)$. For $\varepsilon \ge \delta_{A_1}$, use a fixed schedule that
sets $v=v_{\mathrm{slow}}=\Theta(\Delta_*^2)$ on $J$. Outside $J$, let
$d(s,J)=\mathrm{dist}(s,J)$ and set
\[
v(s)=\kappa\bigl(\Delta_*+c_{\min}d(s,J)\bigr)^2
\]
with $\kappa>0$ chosen small enough for RC-admissibility. This is valid because
for any true crossing $s^*\in J$, the Chapter~6 gap bounds imply
$g(s)\ge \max(g_{\min},c_{\min}|s-s^*|)$ with
$c_{\min}:=\min(c_L,c_R)>0$ and $g_{\min}=\Theta(\Delta_*)$, hence
$g(s)\ge
\widetilde c(\Delta_*+c_{\min}d(s,J))$ with a constant $\widetilde c>0$
independent of $n$, and $\|H'(s)\|\le 2$.

Time on $J$ is
\[
T_J=\Theta\!\left(\frac{W(\varepsilon)}{v_{\mathrm{slow}}}\right).
\]
Time outside $J$ is bounded by
\[
T_{\overline J}\;\le\; \frac{2}{\kappa}
\int_0^1 \frac{dd}{(\Delta_*+c_{\min}d)^2}
\;=\; O(1/\Delta_*) \;=\; O(T_{\mathrm{inf}}),
\]
since $\delta_s=\Theta(\Delta_*)$ and
$T_{\mathrm{inf}}=\Theta(\delta_s/v_{\mathrm{slow}})=\Theta(1/\Delta_*)$.
Therefore
\[
T \;=\; T_J + O(T_{\mathrm{inf}})
\;=\; \Theta\!\left(
T_{\mathrm{inf}}\cdot\max\!\left(1,\frac{\varepsilon}{\delta_{A_1}}\right)
\right),
\]
using $W(\varepsilon)=\Theta((\varepsilon/\delta_{A_1})\delta_s)$.
For $\varepsilon < \delta_{A_1}$, the informed schedule gives
$T=O(T_{\mathrm{inf}})$.
\end{proof}

The structure is explicit: linear interpolation with a smooth crossover at
$\varepsilon \sim \delta_{A_1}$, not a discontinuous phase transition. At
precision $1/\mathrm{poly}(n)$ (NP-hard to achieve), the overhead is
$\Theta(2^{n/2}/\mathrm{poly}(n))$, close to the full exponential penalty. At
precision $2^{-n/2}$ (the algorithmically relevant scale from
\autoref{eq:gmin-deltas-relation}), the overhead is $\Theta(1)$. Each
additional bit of $A_1$ knowledge halves the runtime. The region between the
NP-hard precision and the optimal precision is the ``information gap,'' and the
interpolation theorem maps it quantitatively. For the running example:

\begin{center}
\begin{tabular}{ll}
\hline
Precision $\varepsilon$ & $T(\varepsilon)/T_{\mathrm{inf}}$ \\
\hline
$2^{-n/2}$ & $\Theta(1)$ \\
$2^{-n/4}$ & $\Theta(2^{n/4})$ \\
$1/n$ & $\Theta(2^{n/2}/n)$ \\
$1/\mathrm{poly}(n)$ & $\Theta(2^{n/2}/\mathrm{poly}(n))$ \\
$1$ (no knowledge) & $\Theta(2^{n/2})$ \\
\hline
\end{tabular}
\end{center}

The last row (``$1$ (no knowledge)'') is the fully uninformed limit from
\autoref{cor:separation-grover}; it is included for context alongside the
interpolation regime.

The interpolation theorem treats $A_1$ precision as a continuous resource. A
complementary operational question asks for the best fixed schedule when $s^*$
is only known to lie in an interval $[u_L, u_R]$. A hedging schedule spreads
slowdown across that full interval instead of concentrating it at one point. It
uses velocity $v_{\mathrm{slow}}$ on $[u_L, u_R]$ and $v_{\mathrm{fast}}$
outside. The total evolution time is
$T = (u_R - u_L)/v_{\mathrm{slow}} + (1 - u_R + u_L)/v_{\mathrm{fast}}$;
normalizing to $T = 1$ makes the error a function of velocity allocation alone.

Write $w = u_R - u_L$ for the interval width. The separation theorem used the Roland-Cerf local condition, which bounds error through $\int g^{-2}\,ds$. The JRS framework (\autoref{eq:jrs-bound}) has a stronger $g^{-3}$ weighting and includes boundary/derivative terms (Chapter~7). When the velocity $v(s)=ds/dt$ is piecewise constant, the change of variables $dt=ds/v$ gives $\lVert dH/dt\rVert^2 g^{-3}\,dt = v\,\lVert H'\rVert^2 g^{-3}\,ds$, so $v$ factors out of the JRS integrand on each segment. Isolating the dominant $g^{-3}$ bulk term under this ansatz gives the schedule-allocation proxy
\[
E_{\mathrm{proxy}}
:= v_{\mathrm{slow}} I_{\mathrm{slow}} + v_{\mathrm{fast}} I_{\mathrm{fast}},
\]
where
$I_{\mathrm{slow}} = \int_{u_L}^{u_R} g(u)^{-3}\,du$ and
$I_{\mathrm{fast}} = \int_{[0,1]\setminus[u_L,u_R]} g(u)^{-3}\,du$.
This proxy captures the segment-wise weighting used for allocation, but it is
not asserted to equal the full rigorous transition-probability bound.
Because the crossing lies in the slow region, typically
$I_{\mathrm{slow}} \gg I_{\mathrm{fast}}$.

\begin{theorem}[Hedging (piecewise-constant proxy optimum)]
\label{thm:hedging}
Fix an uncertainty interval $[u_L,u_R]$ with width $w=u_R-u_L$, and restrict to
piecewise-constant schedules with velocity $v_{\mathrm{slow}}$ on $[u_L,u_R]$
and $v_{\mathrm{fast}}$ outside. Under runtime normalization $T=1$, the proxy
objective
$E_{\mathrm{proxy}}=v_{\mathrm{slow}}I_{\mathrm{slow}}+v_{\mathrm{fast}}I_{\mathrm{fast}}$
is minimized at
\[
v_{\mathrm{slow}} = w + \sqrt{(1-w)w/R},\qquad R:=I_{\mathrm{slow}}/I_{\mathrm{fast}}.
\]
Moreover, as $R\to\infty$,
\[
\frac{E_{\mathrm{proxy,opt}}}{E_{\mathrm{proxy,unif}}}\to w=u_R-u_L.
\]
\end{theorem}

\begin{proof}
Write $w = u_R - u_L$ for the interval width. The normalization constraint
$w/v_{\mathrm{slow}} + (1-w)/v_{\mathrm{fast}} = 1$ fixes the total time
$T = 1$, and the proxy objective is
$E_{\mathrm{proxy}} = v_{\mathrm{slow}} I_{\mathrm{slow}} + v_{\mathrm{fast}} I_{\mathrm{fast}}$.
The constraint gives
\begin{equation}
v_{\mathrm{fast}} = \frac{(1-w)\,v_{\mathrm{slow}}}{v_{\mathrm{slow}} - w},
\end{equation}
valid for $v_{\mathrm{slow}} > w$. Substituting into the error gives
\begin{equation}
E_{\mathrm{proxy}}(v_{\mathrm{slow}})
= v_{\mathrm{slow}}\, I_{\mathrm{slow}}
+ \frac{(1-w)\,v_{\mathrm{slow}}}{v_{\mathrm{slow}} - w}\, I_{\mathrm{fast}}.
\end{equation}
Differentiating with respect to $v_{\mathrm{slow}}$ and setting to zero:
\begin{equation}
\frac{dE_{\mathrm{proxy}}}{dv_{\mathrm{slow}}}
= I_{\mathrm{slow}}
- \frac{(1-w)\,w\, I_{\mathrm{fast}}}{(v_{\mathrm{slow}} - w)^2}
= 0.
\end{equation}
Solving yields $(v_{\mathrm{slow}} - w)^2 = (1-w)\,w\,I_{\mathrm{fast}}/I_{\mathrm{slow}} = (1-w)\,w/R$, so
\begin{equation}
v_{\mathrm{slow}} = w + \sqrt{(1-w)\,w/R}.
\end{equation}
At this optimum, $v_{\mathrm{fast}} = (1-w)\,v_{\mathrm{slow}}/\sqrt{(1-w)\,w/R} = \sqrt{R\,w\,(1-w)} + (1-w)$. The optimal proxy value, substituting $v_{\mathrm{slow}} - w = \sqrt{(1-w)\,w/R}$, is
\begin{equation}
E_{\mathrm{proxy,opt}} = \bigl(w + \sqrt{(1-w)\,w/R}\bigr)\,I_{\mathrm{slow}} + \bigl(\sqrt{R\,w\,(1-w)} + (1-w)\bigr)\,I_{\mathrm{fast}}.
\end{equation}
Since $R = I_{\mathrm{slow}}/I_{\mathrm{fast}} \gg 1$, the terms involving $\sqrt{R}$ contribute $2\sqrt{w\,(1-w)\,I_{\mathrm{slow}}\,I_{\mathrm{fast}}} = o(I_{\mathrm{slow}})$, and the dominant term is $w\,I_{\mathrm{slow}}$, while the uniform proxy value is
$E_{\mathrm{proxy,unif}} = I_{\mathrm{slow}} + I_{\mathrm{fast}} \approx I_{\mathrm{slow}}$.
Therefore $E_{\mathrm{proxy,opt}}/E_{\mathrm{proxy,unif}} \to w = u_R - u_L$ as $R \to \infty$.
\end{proof}

For an uncertainty interval $[0.4, 0.8]$ (so $w=0.4$), the hedging ratio
$E_{\mathrm{proxy,opt}}/E_{\mathrm{proxy,unif}}$ approaches $0.4$ in the
large-$R$ regime, i.e.\ a $60\%$ reduction in the allocation proxy.

\begin{corollary}[Conditional fidelity improvement at fixed runtime]
\label{cor:fidelity-improvement}
Assume that in the operating regime and schedule class under comparison,
transition error is asymptotically proportional to the proxy:
$\eta = \kappa E_{\mathrm{proxy}}(1+o(1))$ with the same $\kappa>0$ for hedged
and uniform schedules at fixed $T$. Then
\[
\eta_{\mathrm{hedge}} \sim w\cdot \eta_{\mathrm{unif}}.
\]
Consequently, for success fidelity $\mathcal{F}=1-\eta$,
\[
\mathcal{F}_{\mathrm{hedge}} - \mathcal{F}_{\mathrm{unif}}
\sim \eta_{\mathrm{unif}}(1 - w).
\]
\end{corollary}

\begin{proof}
By hypothesis,
\[
\frac{\eta_{\mathrm{hedge}}}{\eta_{\mathrm{unif}}}
= \frac{E_{\mathrm{proxy,hedge}}}{E_{\mathrm{proxy,unif}}} \cdot (1+o(1)).
\]
Apply \autoref{thm:hedging} to get
$E_{\mathrm{proxy,hedge}}/E_{\mathrm{proxy,unif}} \to w$,
hence $\eta_{\mathrm{hedge}} \sim w\,\eta_{\mathrm{unif}}$.
Then
\[
\mathcal{F}_{\mathrm{hedge}}-\mathcal{F}_{\mathrm{unif}}
=\eta_{\mathrm{unif}}-\eta_{\mathrm{hedge}}
\sim \eta_{\mathrm{unif}}(1-w). \qedhere
\]
\end{proof}

The hedging and separation theorems answer different questions, and the distinction matters. The separation theorem (\autoref{thm:separation}) is a worst-case adversarial statement: the adversary places the gap minimum anywhere in $[s_L, s_R]$ after seeing the schedule, and the resulting overhead is $\Omega((s_R - s_L)/\Delta_*)$, which grows exponentially in $n$. The hedging theorem assumes a fixed (but unknown) crossing position within $[u_L, u_R]$ and optimizes piecewise-constant velocity allocation for the proxy objective above. The exponential factor in the separation ratio comes from the denominator $\Delta_* = \Theta(2^{-n/2})$; the hedging ratio $w = u_R - u_L$ is a constant independent of $n$. Both are correct because they measure different quantities: the separation measures the cost of total ignorance about $\Delta_*$-scale placement, while hedging measures the benefit of knowing the crossing's coarse location.

Several caveats are in order. The hedging theorem establishes the optimal velocity allocation for piecewise-constant schedules with respect to $E_{\mathrm{proxy}}$, but whether piecewise-constant is optimal among all fixed schedules with the same uncertainty interval (and under full JRS bounds) remains open. A smoother velocity profile that transitions gradually between slow and fast regions might achieve a better constant, though the improvement cannot change the $\Theta(w)$ scaling at proxy level. The asymptotic ratio $w$ is valid in the regime $R = I_{\mathrm{slow}}/I_{\mathrm{fast}} \gg 1$, which holds whenever the gap minimum lies well inside the uncertainty interval. For crossings near the boundary of $[u_L, u_R]$, the improvement degrades. Finally, the analysis treats single avoided crossings; systems with multiple gap minima would require a multi-region hedging strategy.

These limitations suggest natural directions. Is $w$ the optimal constant for hedging, or can non-piecewise-constant fixed schedules do better? If the crossing position has a known prior distribution rather than just a support, can distribution-aware hedging improve on worst-case allocation? For multi-crossing systems, does the hedging overhead grow with the number of crossings or can a single slow region cover multiple minima?

Bounded uncertainty about $s^*$ therefore yields a constant-factor reduction of
the hedging proxy at fixed runtime, and (under the proportionality assumption
of \autoref{cor:fidelity-improvement}) a matching fidelity improvement
proportional to interval width, rather than the exponential runtime overhead of
the worst-case adversarial model. In the taxonomy of
Section~\ref{sec:complexity-landscape}, this is the same information setting as
Level~2; however, that table measures runtime overhead $T/T_{\mathrm{inf}}$, not
fixed-runtime fidelity gain.


\section{Quantum Bypass}
\label{sec:quantum-bypass}

The separation and interpolation theorems characterize the cost of ignorance within the fixed-schedule model, where the designer commits to a velocity profile before the evolution begins. But an adiabatic device is a physical system. It can be measured during execution. Measurements yield information about the spectrum, and that information can be used to adjust the schedule on the fly. The question, posed in~\cite{braida2024unstructured}, is whether this adaptive access can close the information gap entirely.

It can. Han, Park, and Choi~\cite{HanParkChoi2025} independently proposed a constant geometric speed (CGS) schedule that traverses the eigenstate path at uniform arc length: the velocity $v(s)$ is set so that the instantaneous eigenstate moves at a constant rate in Hilbert space, which automatically slows the schedule near the crossing where the eigenstate changes rapidly. Overlap estimates from quantum Zeno Monte Carlo measurements provide the needed feedback during evolution. Their method improves the runtime scaling from $O(1/\Delta_*^{2})$ to $O(1/\Delta_*^{1})$ and numerically preserves the quadratic speedup without prior spectral knowledge. The binary-search protocol below uses a different mechanism: rather than adapting continuously, it first locates the crossing through discrete branch probes and then runs the informed schedule. In the ideal decision-probe model, it achieves $O(T_{\mathrm{inf}})$.

The key distinction is between \emph{computing} and \emph{detecting}. Computing $s^*$ from the classical description of $H_z$ is NP-hard. Detecting $s^*$ by probing the quantum system $H(s)$ at selected parameter values can be efficient, because each probe extracts a bit of information about the spectrum at a cost proportional to the inverse gap at the probe point. We model this with a binary decision-probe oracle: a probe at parameter $s$ returns whether $s$ is at or left of the true crossing, and costs $O(1/g(s))$.

\begin{definition}[Binary decision probe]
\label{def:binary-probe}
For an instance $H$, let $D_H:[0,1]\to\{0,1\}$ be the idealized oracle
\[
D_H(s)=
\begin{cases}
0,& s \le s^*(H),\\
1,& s > s^*(H),
\end{cases}
\]
with probe cost $O(1/g_H(s))$.
\end{definition}

\begin{definition}[Adaptive adiabatic protocol]
\label{def:adaptive-protocol}
The protocol operates in two phases.

\emph{Phase 1 (Location).} Initialize $s_{\mathrm{lo}} = 0$, $s_{\mathrm{hi}} = 1$. For $i = 1, \ldots, \lceil n/2 \rceil$:
\begin{enumerate}
\item Set $s_{\mathrm{mid}} = (s_{\mathrm{lo}} + s_{\mathrm{hi}})/2$.
\item Query $D_H(s_{\mathrm{mid}})$.
\item If $D_H(s_{\mathrm{mid}})=0$ (at or left of crossing), set $s_{\mathrm{lo}} = s_{\mathrm{mid}}$.
\item If $D_H(s_{\mathrm{mid}})=1$ (right of crossing), set $s_{\mathrm{hi}} = s_{\mathrm{mid}}$.
\end{enumerate}
After $\lceil n/2 \rceil$ iterations, $s^*$ is located to precision $O(2^{-n/2})$.

\emph{Phase 2 (Execution).} Reset the state to $\ket{\psi_0}$. Evolve from $s = 0$ to $s = 1$ using the informed local schedule of \autoref{thm:aqo-runtime}, with the crossing position estimated in Phase~1.
\end{definition}

Each call to $D_H(s_{\mathrm{mid}})$ costs $O(1/g(s_{\mathrm{mid}}))$ by Definition~\ref{def:binary-probe}, since the branch test resolves a spectral question about $H(s_{\mathrm{mid}})$ and the required evolution time scales inversely with the gap at the probe point.

\begin{lemma}[Phase 1 cost]
\label{lem:phase1-cost}
The total time for Phase 1 is $O(T_{\mathrm{inf}})$.
\end{lemma}

\begin{proof}
Let $d_i = |s_{\mathrm{mid},i} - s^*|$ be the distance from the $i$-th midpoint to the true crossing. Chapter~6 gives two complementary bounds. Outside the crossing window ($|s - s^*| > \delta_s$), the gap satisfies
\[
g(s) \geq c_{\min} |s - s^*|,
\]
where $c_{\min} = \min(c_L, c_R)$ with $c_L = A_1(A_1+1)/A_2$ and $c_R = \Delta/30$. Both constants are positive and independent of $n$. Inside the crossing window ($|s - s^*| \leq \delta_s$), the gap satisfies $g(s) \geq g_{\min}$. Because $g_{\min}$ is the global minimum, these two bounds combine to
\begin{equation}
\label{eq:gap-lower-bound}
g(s_{\mathrm{mid},i}) \geq \max(g_{\min},\; c_{\min} \cdot d_i).
\end{equation}
The probe cost at iteration $i$ is therefore
\begin{equation}
\label{eq:pe-cost}
O\!\left(\frac{1}{g(s_{\mathrm{mid},i})}\right) \leq O\!\left(\min\!\left(\frac{1}{g_{\min}},\; \frac{1}{c_{\min} \cdot d_i}\right)\right).
\end{equation}
The two bounds in~\eqref{eq:pe-cost} cross at $d_i = g_{\min}/c_{\min}$. Since $g_{\min} = c_L \delta_s \cdot (1 - O(\eta))$ and $c_{\min} \leq c_L$, the crossover distance satisfies
\[
d_{\mathrm{cross}} = g_{\min}/c_{\min} = (c_L/c_{\min})\,\delta_s \geq \delta_s.
\]
The ratio $c_L/c_{\min}$ is a positive constant independent of $n$ because $c_L = A_1(A_1+1)/A_2$ and $c_R = \Delta/30$ are fixed by spectral parameters. Therefore $c_{\min} = \min(c_L, c_R) > 0$ is also fixed. At most $O(\log(c_L/c_{\min}) + 1) = O(1)$ binary-search midpoints can fall in the near regime $d_i \leq d_{\mathrm{cross}}$.

Group the $\lceil n/2 \rceil$ iterations by the distance $d_i$ in dyadic shells $S_j = [2^{-j-1}, 2^{-j}]$. Let $L_i = 2^{-i+1}$ be the binary-search interval width at step $i$. Since the next interval is the half containing $s^*$, the midpoint distances obey
\[
d_{i+1} = \left|d_i - \frac{L_i}{4}\right| = \left|d_i - 2^{-i-1}\right|.
\]
This recurrence gives $O(1)$ occupancy per shell: for $i > j+1$, $d_i \leq L_i/2 = 2^{-i} < 2^{-j-1}$, so $d_i \notin S_j$; and for $i \leq j$, if $d_i \in (2^{-j-1}, 2^{-j})$, then $d_{i+1} \notin (2^{-j-1}, 2^{-j})$. Thus the shell interior is visited at most once, with only a dyadic-rational edge case where the boundary value $2^{-j-1}$ can appear twice consecutively. Hence each shell contributes $O(1)$ midpoint queries.

\emph{Far shells} ($j < \log_2(1/\delta_s) \approx n/2$): here $d_i > \delta_s$, so the binding bound in~\eqref{eq:pe-cost} is $O(1/(c_{\min} \cdot d_i)) = O(2^j/c_{\min})$, where $c_{\min}$ enters the implicit constant.

\emph{Near shells} ($j \geq n/2$): here $d_i \leq \delta_s$, so the binding bound is $O(1/g_{\min}) = O(1/\Delta_*) = O(2^{n/2})$.

There are $O(1)$ near shells because at most $O(1)$ midpoints can satisfy $d_i \leq d_{\mathrm{cross}}$ in a binary search. The total cost is
\begin{equation}
\sum_{j=0}^{n/2-1} O(1) \cdot O(2^j) + O(1) \cdot O(2^{n/2}) = O(2^{n/2}) + O(2^{n/2}) = O(2^{n/2}) = O(T_{\mathrm{inf}}).
\end{equation}
The state preparation cost is $O(n)$ per iteration and $O(n)$ iterations, giving $O(n^2) = o(T_{\mathrm{inf}})$.
\end{proof}

\begin{theorem}[Adaptive adiabatic optimality in the decision-probe model]
\label{thm:adaptive}
The adaptive protocol of \autoref{def:adaptive-protocol} achieves runtime $T_{\mathrm{adapt}} = O(T_{\mathrm{inf}})$ with $\Theta(n)$ measurements.
\end{theorem}

\begin{proof}
Phase~1 locates $s^*$ to precision $O(2^{-n/2}) = O(\delta_s)$ using total time $O(T_{\mathrm{inf}})$ by \autoref{lem:phase1-cost}. This precision is within the crossing window width $\delta_s = O(\Delta_*)$. Phase~2 has time $O(T_{\mathrm{inf}})$ by \autoref{thm:aqo-runtime}, since the estimate of $s^*$ is accurate to $O(\delta_s)$. The total is $O(T_{\mathrm{inf}}) + O(T_{\mathrm{inf}}) = O(T_{\mathrm{inf}})$.
\end{proof}

\begin{theorem}[Measurement lower bound]
\label{thm:measurement-lower}
For rank-one families where the crossing location is a priori uncertain in an
interval of width $\Theta(1)$ and $\delta_s=\Theta(2^{-n/2})$ (the same regime
as \autoref{cor:separation-grover}), any adaptive protocol in the binary
decision-probe model that guarantees localization of $s^*$ to precision
$O(\delta_s)$ requires $\Omega(n)$ measurements. In particular, any
locate-then-execute strategy that first localizes $s^*$ and then runs the
informed schedule in $O(T_{\mathrm{inf}})$ has measurement cost $\Omega(n)$.
\end{theorem}

\begin{proof}
The prior uncertainty interval has width $\Theta(1)$, while the required final
uncertainty is $O(\delta_s)=\Theta(2^{-n/2})$. Therefore the protocol must
distinguish among
$K=\Omega\!\bigl(1/\delta_s\bigr)=\Omega(2^{n/2})$ disjoint location bins.
In the binary decision-probe model, each measurement returns one bit, so after
$m$ measurements there are at most $2^m$ possible transcripts. To identify one
of $K$ bins in the worst case we need $2^m\ge K$, hence
$m\ge \log_2 K=\Omega(n)$.
\end{proof}

The three adiabatic regimes are:

\begin{center}
\begin{tabular}{lll}
\hline
Strategy & Runtime & Measurements \\
\hline
Fixed, uninformed & $\Omega(2^{n/2} \cdot T_{\mathrm{inf}})$ & 0 \\
Adaptive & $O(T_{\mathrm{inf}})$ & $\Theta(n)$ \\
Fixed, informed & $O(T_{\mathrm{inf}})$ & 0 \\
\hline
\end{tabular}
\end{center}

\noindent For the running example ($N = 4$, $d_0 = 1$, $n = 2$), Phase~1 performs $\lceil 1 \rceil = 1$ probe at $s_{\mathrm{mid}} = 0.5$, so the location stage already achieves width $1/2 = O(2^{-n/2})$. The gap at that point is $g(0.5)=1/\sqrt{N}=1/2$, which gives probe cost $O(1/g)=O(2)=O(T_{\mathrm{inf}})$.

\noindent \emph{Implementation note.} A physical instantiation of $D_H$ uses phase-estimation-based branch tests on $H(s_{\mathrm{mid}})$, which distinguish the ground and excited branches at cost $O(1/g(s_{\mathrm{mid}}))$ per probe. The theorem above is stated in the ideal decision-probe model; the open question is whether constant-probability probes with standard amplification introduce only polylogarithmic overhead, preserving the $O(T_{\mathrm{inf}})$ scaling. The key observation is that each probe requires only constant-probability correctness, so standard amplification keeps the overhead logarithmic.

The adaptive protocol acquires $A_1$ through measurement, while the circuit model bypasses $A_1$ entirely. The D\"urr-H\o yer minimum-finding algorithm~\cite{durr1996quantum} achieves $\Theta(\sqrt{N/d_0})$ by maintaining a threshold and iteratively lowering it with Grover search. It never traverses an adiabatic path and never encounters an avoided crossing. Its mechanism is amplitude amplification with iterative thresholding and no spectral inputs. It needs no $A_1$, no $s^*$, no $\Delta$, and no gap profile.

\begin{proposition}[$A_1$-blindness]
\label{prop:A1-blindness}
Let $X_{\mathrm{DH}}$ denote the output of the amplified D\"urr-H\o yer algorithm (with $r = \Theta(n)$ repetitions). Then $I(X_{\mathrm{DH}};\,A_1 \mid S_0, E_0) \leq 2^{-\Omega(n)}$. Conditioned on success ($X_{\mathrm{DH}} \in S_0$), the mutual information is exactly zero.
\end{proposition}

\begin{proof}
Two problem Hamiltonians $H_z, H_z'$ are ground-equivalent if they share the same ground energy $E_0$ and ground space $S_0$. By symmetry of Grover's algorithm applied to the uniform initial state, the output distribution conditioned on success is $\mathrm{Uniform}(S_0)$, regardless of the excited spectrum. Since $A_1$ depends only on the excited spectrum (via $\{d_k, E_k\}_{k \geq 1}$), we have
\[
I(X_{\mathrm{DH}};\,A_1 \mid \text{success}, S_0, E_0) = 0.
\]
Let $F$ be the failure indicator of the amplified routine, and fix any prior over ground-equivalent instances conditioned on $(S_0,E_0)$. With $r = \Theta(n)$ repetitions using Boyer-Brassard-H\o yer-Tapp amplification~\cite{BBHT1998}, the per-trial success probability is at least $2/3$, so
\[
p_f := \Pr[F=1] \leq (1/3)^r = 2^{-\Omega(n)}.
\]
Using chain rule and the fact that $F$ is a function of $X_{\mathrm{DH}}$:
\begin{align}
I(X_{\mathrm{DH}};\,A_1 \mid S_0,E_0)
&\le I(X_{\mathrm{DH}},F;\,A_1 \mid S_0,E_0) \nonumber\\
&= I(F;\,A_1 \mid S_0,E_0) + I(X_{\mathrm{DH}};\,A_1 \mid F,S_0,E_0) \nonumber\\
&= I(F;\,A_1 \mid S_0,E_0) + p_f\, I(X_{\mathrm{DH}};\,A_1 \mid F=1,S_0,E_0) \nonumber\\
&\le H(F) + p_f\, H(X_{\mathrm{DH}} \mid F=1,S_0,E_0). \label{eq:A1-blindness-mi}
\end{align}
The $F=0$ term vanishes by the conditional independence established above. Now $H(F) \le h_2(p_f)$, where $h_2(p) = -p\log p - (1-p)\log(1-p)$ is the binary entropy, and $H(X_{\mathrm{DH}} \mid F=1,S_0,E_0) \le \log N = n$, so Eq.~\eqref{eq:A1-blindness-mi} gives
\[
I(X_{\mathrm{DH}};\,A_1 \mid S_0,E_0) \le h_2(p_f) + p_f n = 2^{-\Omega(n)}.
\]
\end{proof}

The circuit model does not merely avoid computing $A_1$. It is provably blind to it. Two problem Hamiltonians that share the same ground space but have entirely different excited spectra, and therefore different $A_1$ values, produce output distributions that are exponentially close in total variation: conditioned on success the distributions coincide exactly (uniform on $S_0$), and all discrepancy is confined to failure events of probability $2^{-\Omega(n)}$, so $\|P_{H_z}-P_{H'_z}\|_{\mathrm{TV}} \le p_f(H_z)+p_f(H'_z)=2^{-\Omega(n)}$. No measurement on the output of D\"urr-H\o yer, no matter how clever, can reliably distinguish two such instances. The fixed adiabatic model has the opposite property: a schedule tuned to one value of $A_1$ performs poorly on ground-equivalent instances with different excited spectra, because the crossing position shifts and the gap minimum moves away from the schedule's slowdown region.

The three models form a hierarchy of spectral information usage. The circuit model is blind to $A_1$ and does not need it. The adaptive adiabatic model acquires $A_1$ through $O(n)$ measurements during execution, paying $O(T_{\mathrm{inf}})$ to do so. The fixed adiabatic model requires $A_1$ as classical input and faces NP-hard recovery. This hierarchy is the operational content of the ``information gap'': the same computational task admits three qualitatively different information profiles depending on the model.

The adaptive protocol's efficiency rests on a structural feature of the rank-one gap profile: the gap grows linearly away from the crossing ($\alpha = 1$), so a probe at distance $d$ costs $O(1/d)$ and the geometric sum converges. But the rank-one case is special. What changes when the gap approaches its minimum more gently?


\section{Gap Geometry and Schedule Optimality}
\label{sec:gap-geometry}

If the gap approaches its minimum more gently than the rank-one linear growth, the danger zone where the schedule must be slow widens, and the runtime degrades. How much depends on the geometry of the approach.

To make the scaling statements precise, this section analyzes the model profile
\[
g_{\alpha,c,\Delta_*}(s) = \Delta_* + c|s-s^*|^\alpha,\qquad c>0,
\]
on $[0,1]$, with $c$ independent of $\Delta_*$. The flatness exponent $\alpha$
parametrizes the gap shape. A V-shaped minimum ($\alpha = 1$) is the rank-one
case. Flatter approaches ($\alpha > 1$) create wider regions where the gap stays
close to its minimum, forcing the schedule to be slow over a wider interval.

Guo and An~\cite{GuoAn2025} identified a measure condition that guarantees $O(1/g_{\min})$ for the $p = 3/2$ power-law schedule, which they show is variationally optimal within the JRS error functional. The condition asks that sublevel sets of the gap grow at most linearly: $\mu(\{s : g(s) \leq x\}) \leq Cx$ for all $x > 0$, with $C$ independent of $\Delta_*$. This section proves three things. First, the measure condition holds if and only if $\alpha \leq 1$. Second, when $\alpha > 1$, the optimal power-law schedule degrades from $T = O(1/\Delta_*)$ to $T = O(1/\Delta_*^{3-2/\alpha})$, forming a continuous spectrum of scaling exponents rather than a binary dichotomy between ``easy'' and ``hard'' gap profiles. Third, the rank-one framework forces $\alpha = 1$, placing it exactly at the boundary where the measure condition holds.

Whether some non-power-law fixed schedule can recover $O(1/\Delta_*)$ when $\alpha > 1$ remains open. The constant geometric speed approach of Han, Park, and Choi~\cite{HanParkChoi2025} achieves $O(1/g_{\min})$ by adaptive gap measurements, but that method uses runtime feedback during evolution and is therefore not a fixed schedule. The open question is specifically: does there exist a fixed (non-adaptive) schedule family, not necessarily a power law, that achieves $O(1/\Delta_*)$ for gap profiles with $\alpha > 1$? No such family is currently known.

\begin{theorem}[Geometric characterization]
\label{thm:geometric-char}
For the model profile $g_{\alpha,c,\Delta_*}(s)$, the measure condition
$\mu(\{s:g(s)\le x\})\le Cx$ with $C$ independent of $\Delta_*$ holds if and
only if $\alpha \leq 1$.
\end{theorem}

\begin{proof}
For $x \ge \Delta_*$, write $y=(x-\Delta_*)/c$. The sublevel set has measure
\[
\mu(x)=\mu(\{s:g(s)\le x\})\le \min(1,\,2y^{1/\alpha}).
\]

\emph{Case $\alpha \leq 1$.} For $x\in[\Delta_*,1]$,
\[
\frac{\mu(x)}{x}
\le \frac{2}{c^{1/\alpha}}\frac{(x-\Delta_*)^{1/\alpha}}{x}
\le \frac{2}{c^{1/\alpha}}x^{1/\alpha-1}
\le \frac{2}{c^{1/\alpha}},
\]
because $1/\alpha-1\ge0$ and $x\le1$. For $x\ge1$, $\mu(x)/x\le1$. Thus
$\sup_{x>0}\mu(x)/x \le \max(1,2/c^{1/\alpha})$, independent of $\Delta_*$.

\emph{Case $\alpha > 1$.} At $x=2\Delta_*$, one side of the sublevel interval
always contributes length at least $(\Delta_*/c)^{1/\alpha}$, so
$\mu(x)\ge (\Delta_*/c)^{1/\alpha}$. Therefore
\[
\frac{\mu(2\Delta_*)}{2\Delta_*}
\ge \frac{1}{2c^{1/\alpha}}\Delta_*^{1/\alpha-1},
\]
which diverges as $\Delta_*\to0$ since $1/\alpha-1<0$. Hence no finite
$C$ independent of $\Delta_*$ can satisfy the measure condition.
\end{proof}

The gap integral $\int_0^1 g(s)^{-\beta}\,ds$ controls the runtime for power-law schedules. A substitution $u = c|s - s^*|^\alpha/\Delta_*$ gives the following scaling.

\begin{lemma}[Gap integral]
\label{lem:gap-integral}
For the model profile $g_{\alpha,c,\Delta_*}$ and $\beta > 1/\alpha$,
\begin{equation}
\label{eq:gap-integral}
\int_0^1 g(s)^{-\beta}\,ds = \Theta(\Delta_*^{1/\alpha - \beta}).
\end{equation}
For $\beta = 1/\alpha$, the integral is $\Theta(\log(1/\Delta_*))$. For $\beta < 1/\alpha$, the integral is $\Theta(1)$.
\end{lemma}

\begin{proof}
The substitution $u = (c|s-s^*|^\alpha)/\Delta_*$ transforms the near-minimum contribution to
\[
\Delta_*^{1/\alpha-\beta}\int_0^{U} u^{1/\alpha-1}(1+u)^{-\beta}\,du,
\]
where $U=\Theta(1/\Delta_*)$. As $u\to\infty$, the integrand behaves like $u^{1/\alpha-1-\beta}$.

If $\beta>1/\alpha$, the exponent is strictly less than $-1$, so the $u$-integral converges to a finite constant, yielding $\Theta(\Delta_*^{1/\alpha-\beta})$.

If $\beta=1/\alpha$, the integrand is asymptotically $u^{-1}$, so the integral contributes $\Theta(\log U)=\Theta(\log(1/\Delta_*))$.

If $\beta<1/\alpha$, the integral grows as $U^{1/\alpha-\beta}$, which cancels the prefactor $\Delta_*^{1/\alpha-\beta}$, giving $\Theta(1)$.

The contribution from outside a neighborhood of $s^*$ is always $O(1)$. For fixed $\delta$, if $|s-s^*|\ge\delta$ then $g(s)\ge g_0>0$ independently of $\Delta_*$, so $\int_{|s-s^*|\ge\delta} g(s)^{-\beta}ds \le g_0^{-\beta}$.
\end{proof}

\begin{theorem}[Scaling spectrum]
\label{thm:scaling-spectrum}
For the model profile $g_{\alpha,c,\Delta_*}$ with flatness exponent
$\alpha > 2/3$, the adiabatic runtime with the $p = 3/2$ power-law schedule
(variationally optimal in the JRS framework~\cite{GuoAn2025}) satisfies
\begin{equation}
\label{eq:scaling-spectrum}
T = \Theta(1/\Delta_*^{3 - 2/\alpha}).
\end{equation}
\end{theorem}

\begin{proof}
The power-law schedule $u'(s) = c_p\,g(u(s))^p$ has normalization constant $c_p = \int_0^1 g(v)^{-p}\,dv$. The JRS error functional becomes
\begin{equation}
\eta \leq \frac{1}{T}\,c_p \int_0^1 g(v)^{p-3}\,dv.
\end{equation}
By \autoref{lem:gap-integral}, $c_p = \Theta(\Delta_*^{1/\alpha - p})$ (requiring $p > 1/\alpha$) and the second integral is $\Theta(\Delta_*^{1/\alpha + p - 3})$ (requiring $3 - p > 1/\alpha$). Together these require $1/\alpha < p < 3 - 1/\alpha$, an interval of width $3 - 2/\alpha$, which is positive if and only if $\alpha > 2/3$. The symmetric choice $p = 3/2$ lies in this interval for all $\alpha > 2/3$. Their product is
\begin{equation}
c_p \int g^{p-3}\,dv = \Theta(\Delta_*^{(1/\alpha - p) + (1/\alpha + p - 3)}) = \Theta(\Delta_*^{2/\alpha - 3}).
\end{equation}
Setting $\eta = O(1)$ gives $T = \Omega(\Delta_*^{-(3 - 2/\alpha)}) = \Omega(1/\Delta_*^{3-2/\alpha})$. The $p = 3/2$ power-law schedule achieves this scaling, giving a matching upper bound $T = O(1/\Delta_*^{3-2/\alpha})$.
\end{proof}

\begin{center}
\begin{tabular}{llll}
\hline
$\alpha$ & Exponent $\gamma = 3 - 2/\alpha$ & Measure condition & Runtime \\
\hline
$1$ & $1$ & Holds & $\Theta(1/\Delta_*)$ \\
$2$ & $2$ & Fails & $\Theta(1/\Delta_*^2)$ \\
$3$ & $7/3$ & Fails & $\Theta(1/\Delta_*^{7/3})$ \\
$\infty$ & $3$ & Fails & $\Theta(1/\Delta_*^3)$ \\
\hline
\end{tabular}
\end{center}

The exponents in the table are specific to the JRS-optimal $p = 3/2$ schedule; other schedule families produce different curves. In particular, the exponent $\gamma = 3$ at $\alpha = \infty$ is not universal: the Roland-Cerf schedule ($p = 2$) gives $T = O(1/\Delta_*^2)$ for flat gaps via a tighter adiabatic condition. The table shows the JRS-optimal scaling as the gap flattens.

The runtime exponent $\gamma = 3 - 2/\alpha$ interpolates continuously from $1$ (V-shaped gap, best case) to $3$ (flat gap, worst case). There is no sharp boundary between ``easy'' and ``hard'' gap profiles, only a gradual degradation as the minimum flattens. For the running example ($M = 2$, $d_0 = 1$, $N = 4$), $\alpha = 1$, and $\gamma = 1$, confirming the optimal $T = \Theta(1/\Delta_*)$ scaling. The formula $\gamma = 3 - 2/\alpha$ also explains why the transition is one-sided: as $\alpha \to \infty$ (perfectly flat gap), $\gamma \to 3$ and the exponent saturates; as $\alpha \to 2/3$ from above, the interval of valid power-law exponents shrinks to zero and the variational framework loses its grip entirely.

\begin{proposition}[Structural $\alpha = 1$]
\label{prop:structural-alpha}
For the rank-one Hamiltonian
$H(s) = -(1-s)\ket{\psi_0}\!\bra{\psi_0} + sH_z$ under the spectral-condition
regime of Chapters~5--6 (with $d_1 \ge 1$ and $\Delta>0$), the effective
crossing geometry is linear: $g(s)=\Theta(|s-s^*|)$ for
$|s-s^*|\gg \delta_s$. Equivalently, the flatness exponent is $\alpha=1$.
\end{proposition}

\begin{proof}
Chapter~5 gives the crossing-window form
\[
g(s)=(1\pm O(\eta))\frac{s(A_1+1)}{A_2(1-s)}
\sqrt{(s-s^*)^2+\Theta(\delta_s^2)}.
\]
In the window, the prefactor $s/(1-s)$ is bounded above and below by positive
constants (depending only on fixed spectral parameters), so for
$|s-s^*|\gg\delta_s$ the square-root term is $\Theta(|s-s^*|)$ and hence
$g(s)=\Theta(|s-s^*|)$. This is exactly $\alpha=1$.
\end{proof}

The linearity is structural: the coupling between the ground and first excited
branches is proportional to $|\!\braket{\psi_0}{\phi_1}\!|^2 = d_1/N > 0$, so
the avoided crossing is simple. A higher-order tangency ($\alpha > 1$) would
require this coupling to vanish, which cannot happen when $d_1 > 0$.
To get different values of $\alpha$, one must change the interpolation scheme.
Examples include quantum phase transitions with $H(s)$ nonlinear in $s$ or
systems with symmetry-enforced higher-order crossings. This structural
$\alpha = 1$ explains why both the Roland-Cerf analysis and the Guo-An
framework achieve the same asymptotic runtime.

Braida et al.~\cite{braida2024unstructured} and Guo and An~\cite{GuoAn2025} are independent works on the same problem class. The former provides the spectral analysis ($A_1$, $s^*$, piecewise gap bounds), while the latter provides the variational optimization (power-law schedule, measure condition).

\begin{theorem}[Measure condition for the rank-one gap profile]
\label{thm:measure-paper}
Under the spectral condition of Chapter~5, for all sufficiently large $n$ the
piecewise-linear gap profile satisfies the measure condition with
\begin{equation}
\label{eq:measure-constant}
C \leq \frac{3A_2}{A_1(A_1+1)} + \frac{30(1 - s_0)}{\Delta},
\end{equation}
where $s_0$ is the right-arm basepoint defined in Chapter~6.
\end{theorem}

\begin{proof}
Fix $x > 0$. For $x < g_{\min}$, the sublevel set is empty. For $x \geq g_{\min}$, bound each region of the piecewise gap profile separately.

The left arm satisfies $g(s) \geq c_L(s^* - s)$, but it contributes only when
$x \ge \hat g$ (because its minimum at the window boundary is $\hat g$). Hence
its contribution is at most $x/c_L$ for $x \ge \hat g$ and $0$ for
$x < \hat g$. The crossing window has width $2\delta_s = 2\hat{g}/c_L$. If
$x \geq \hat{g}$, the window contributes at most $2\hat g/c_L \le 2x/c_L$. If
$g_{\min} \leq x < \hat{g}$, use $g_{\min} \geq (1-2\eta)\hat{g}$ and
$x \geq g_{\min}$ to get $\hat{g} \leq x/(1-2\eta)$. For $\eta \leq 1/6$, this
implies $\hat{g} \leq 3x/2$, so the window contribution is at most
$2\hat{g}/c_L \leq 3x/c_L$. The condition $\eta \leq 1/6$ holds asymptotically
because $\eta = O(\sqrt{d_0/(NA_2)}) \to 0$.

Thus left-arm plus window contribution is at most $3x/c_L$ in both regimes
($x \ge \hat g$ and $g_{\min}\le x<\hat g$). The right arm satisfies
$g(s) \geq c_R(s - s_0)/(1 - s_0)$ and contributes at most
$x \cdot 30(1-s_0)/\Delta$. Adding these contributions and substituting
$c_L = A_1(A_1+1)/A_2$ gives the stated bound.
\end{proof}

\begin{corollary}[Grover measure constant]
\label{cor:grover-C}
For Grover ($M = 2$, $d_0 = 1$, $d_1 = N - 1$, $E_0 = 0$, $E_1 = 1$), the exact measure constant is $C = 1$.
\end{corollary}

\begin{proof}
The exact gap is $g(s)^2 = (2s-1)^2(1 - 1/N) + 1/N$. Solving $g(s) \leq x$ gives
\[
\mu(\{g \leq x\}) = \sqrt{\frac{Nx^2 - 1}{N-1}}
\]
for $x \in [1/\sqrt{N}, 1]$, with $\mu = 1$ for $x > 1$. The ratio $\mu/x$ is increasing on $[1/\sqrt{N}, 1]$ and equals $1$ at $x = 1$.
\end{proof}

For the Grover problem, the exact gap integral is $\int_0^1 g(s)^{-2}\,ds = (N/\sqrt{N-1})\arctan\sqrt{N-1} \to (\pi/2)\sqrt{N}$ as $N \to \infty$. This closed-form evaluation confirms the $O(\sqrt{N})$ runtime from the piecewise analysis and provides the exact constant. For the running example ($N = 4$), $\int_0^1 g(s)^{-2}\,ds = (4/\sqrt{3})\arctan\sqrt{3} = 4\pi/(3\sqrt{3}) \approx 2.42$, consistent with the runtime $T_{\mathrm{inf}} = O(\sqrt{4}) = O(2)$.

Both the Roland-Cerf $p = 2$ schedule and Guo-An's $p = 3/2$ schedule achieve
the same asymptotic runtime
$T = O(\sqrt{N/d_0}/\varepsilon)$. As declared at the start of this chapter, all
spectral parameters $A_1$, $A_2$, and $\Delta$ are absorbed into the implicit
constant. The RC runtime involves the integral
$I = \int_0^1 g(s)^{-2}\,ds$, while Guo-An's bound involves $C^2/g_{\min}$.

\begin{theorem}[Constant comparison]
\label{thm:constant-comparison}
Write $a = 3/c_L$ and $r = 30(1-s_0)/\Delta$. Define the surrogate constants
$C_{\mathrm{up}} := a + r$ (from the measure-constant upper bound) and
$I_{\mathrm{sur}} := a + r^2 c_L$ (the matching two-arm surrogate for the RC
integral). Then
\[
I_{\mathrm{sur}} - C_{\mathrm{up}}^2
= (c_L - 1)r^2 - 2ar + a(1-a).
\]
In the right-arm-dominated regime ($r \gg a$) with $c_L > 1$, we have
$C_{\mathrm{up}}^2/I_{\mathrm{sur}} \to 1/c_L = A_2/(A_1(A_1+1)) < 1$.
\end{theorem}

\begin{proof}
With $C_{\mathrm{up}} = a + r$ and $I_{\mathrm{sur}} = a + r^2 c_L$, we obtain
\[
I_{\mathrm{sur}} - C_{\mathrm{up}}^2
= (c_L - 1)r^2 - 2ar + a(1-a).
\]
For $c_L > 1$ and $r \gg a$, the leading term $(c_L - 1)r^2$ dominates, so
$I_{\mathrm{sur}} > C_{\mathrm{up}}^2$ and
$C_{\mathrm{up}}^2/I_{\mathrm{sur}} \to 1/c_L$.
\end{proof}

The framework comparison extends across gap geometries. For
$1/2<\alpha<1$, the Roland-Cerf integral
$\int g^{-2}\,ds = \Theta(g_{\min}^{1/\alpha - 2})$ grows slower than
$1/g_{\min}$, making the RC analysis tighter. At $\alpha=1/2$, the same
integral is $\Theta(\log(1/g_{\min}))$; for $\alpha<1/2$, it is $\Theta(1)$.
For $\alpha = 1$, both frameworks give $\Theta(1/g_{\min})$, and the surrogate
comparison of
\autoref{thm:constant-comparison} identifies regimes where the JRS prefactor can
be smaller than the RC prefactor. For $\alpha > 1$, the measure constant
$C \to \infty$ as $g_{\min} \to 0$, so the JRS framework degrades and only the
RC analysis applies. The structural $\alpha = 1$
(\autoref{prop:structural-alpha}) sits at the exact boundary where both
frameworks are valid and neither uniformly dominates.

For the Grover problem, $c_L \to 2$ as $N \to \infty$. Using the exact values $C_{\mathrm{exact}} = 1$ and $I_{\mathrm{exact}} \to (\pi/2)\sqrt{N}$ gives $C^2/I \to 2/(\pi\sqrt{N}) \to 0$, so the JRS certification is asymptotically tighter. This comparison is unusually explicit because the Grover gap has a closed form.

For structured Hamiltonians with richer spectra, exact constants are rarely analytic, and \autoref{thm:constant-comparison} becomes the practical tool. Exact numerical diagonalization of the open ferromagnetic Ising chain (\autoref{eq:Ising-Ham}, $J = h = 1$, computed in the thesis experiment pipeline \texttt{src/experiments/11\_schedule\_optimality/}) gives $C^2/I = 0.7273$ at $n = 8$, $C^2/I = 0.7122$ at $n = 10$, and $C^2/I = 0.6908$ at $n = 12$, while the Grover benchmarks at matching $N$ are $0.6042$, $0.6033$, and $0.6031$ respectively. The JRS advantage is stable across system sizes: $C^2 < I$ persists, but the ratio decreases toward the Grover benchmark as $n$ grows. Varying the uniform field strength at fixed $n = 10$ ($h \in \{1, 2, 3, 4\}$) gives $C^2/I = 0.7122$, $0.7098$, $0.7531$, $0.7793$, each above the Grover value and below $1$. The non-monotonicity in $h$ reflects competing effects: increasing the field strength concentrates the spectrum near the ground state (decreasing $C$) but also reshapes the gap profile (changing $I$), with neither effect dominating uniformly.

The two frameworks are best viewed as complementary. The spectral analysis~\cite{braida2024unstructured} identifies $A_1$, $s^*$, and the piecewise gap structure, while the variational analysis~\cite{GuoAn2025} identifies the optimal power-law exponent. Together they give a complete account of the rank-one $\alpha = 1$ case, which sits exactly at the boundary where both frameworks apply and the measure condition remains bounded.

This complementarity persists under partial spectral knowledge. The RC framework ($p = 2$) builds the schedule from crossing position $s^*$, so its runtime degrades on the crossing-localization scale:
\[
T_{\mathrm{RC}}(\varepsilon_{A_1})
= T_{\mathrm{RC},\infty}\cdot \Theta\!\left(\max\!\left(1, \frac{\varepsilon_{A_1}}{\delta_{A_1}}\right)\right),
\]
where $\delta_{A_1} = 2\sqrt{d_0 A_2/N}$ is the threshold from \autoref{thm:interpolation}. The JRS framework ($p = 3/2$) instead uses certified bounds $(C_+, g_-)$ on the measure constant and minimum gap, leading to multiplicative overhead
\[
\frac{(1 + \delta_C/C)^2}{1 - \delta_g/g_{\min}},
\]
where $\delta_C$ and $\delta_g$ are estimation errors. The sensitivity profiles are therefore different: RC needs exponentially precise localization of $s^*$, while JRS needs only constant relative accuracy in $C$ and $g_{\min}$. In the partial-information regime forced by NP-hardness, this can make JRS more robust in practice even though both methods share the same asymptotic scaling.

The gap geometry and optimality analysis above assumes the rank-one
interpolation
\[
H(s) \;=\; -(1-s)\ket{\psi_0}\!\bra{\psi_0} + s\,H_z.
\]
The structural $\alpha = 1$, the crossing at
$s^* = A_1/(A_1+1)$, and the measure condition all follow from this specific
path. The rank-one structure is a design choice, not a physical constraint. A
different initial state, ancilla qubits, a multi-segment path, or a
higher-rank projector might avoid the $A_1$ dependence entirely. The next
section shows that none of these modifications succeed.


\section{Anatomy of the Barrier}
\label{sec:barrier-anatomy}

The barrier theorems above apply to fixed schedules on rank-one Hamiltonians. A natural objection is that the designer has more freedom: ancilla qubits, different initial states, coupled interactions, multi-segment paths, higher-rank projectors. If any of these modifications eliminated the $A_1$ dependence, the information gap would be an artifact of a restrictive model rather than a structural limitation. We now show that none of them succeed, proceeding from the weakest modification (adding uncoupled ancillas) to the most general path redesign (multi-segment interpolation with higher-rank projectors), then combining the four obstructions into a single no-go theorem.

Recall from Chapter~5 that for any initial state
$\ket{\psi} \in \mathbb{C}^N$, the weights
$w_k(\psi) = \sum_{z \in \Omega_k} |\braket{z}{\psi}|^2$
determine
$A_1^{\mathrm{eff}}(\psi) = \sum_{k \geq 1} w_k(\psi)/(E_k - E_0)$
and therefore
$s^*(\psi) = A_1^{\mathrm{eff}}(\psi)/(A_1^{\mathrm{eff}}(\psi) + 1)$.
For the uniform superposition $\ket{\psi_0}$, we recover
$w_k = d_k/N$ and $A_1^{\mathrm{eff}} = A_1$.

\begin{theorem}[Product ancilla invariance]
\label{thm:product-ancilla}
For any product initial state $\ket{\Psi} = \ket{\psi_0} \otimes \ket{\phi}$ and uncoupled final Hamiltonian $H_f = H_z \otimes I_{2^m}$, the extended Hamiltonian $H_{\mathrm{ext}}(s) = -(1-s)\ket{\Psi}\!\bra{\Psi} + s(H_z \otimes I_{2^m})$ has the same crossing position $s^* = A_1/(A_1 + 1)$ as the bare system.
\end{theorem}

\begin{proof}
Decompose the extended Hilbert space $\mathbb{C}^N \otimes \mathbb{C}^{2^m}$ into the subspace $\mathcal{V}_\phi = \mathbb{C}^N \otimes \ket{\phi}$ and its orthogonal complement. States $\ket{z} \otimes \ket{a}$ with $\braket{\phi}{a} = 0$ satisfy $\braket{\Psi}{z,a} = 0$, so they are exact eigenstates of $H_{\mathrm{ext}}(s)$ with eigenvalue $sE(z)$. These $N(2^m - 1)$ states do not participate in the avoided crossing. On $\mathcal{V}_\phi$, the restriction of $H_{\mathrm{ext}}(s)$ is unitarily equivalent to the bare Hamiltonian $H(s)$ via the map $\ket{\psi} \otimes \ket{\phi} \mapsto \ket{\psi}$.
\end{proof}

\begin{remark}[Gap degradation from ancillas]
The crossing position is invariant, but the gap of $H_{\mathrm{ext}}(s)$ is strictly smaller than the bare gap: for $d_0 = 1$, the extra eigenvalues at $sE_0$ (from states $\ket{z} \otimes \ket{a}$ with $z \in \Omega_0$, $a \perp \ket{\phi}$) sit between the ground eigenvalue $\lambda_0(s) < sE_0$ and the crossing branch. Uncoupled ancillas make the gap worse, not better.
\end{remark}

Extra qubits by themselves do not move the bottleneck. They add spectators and can even shrink the usable gap, because the ancilla's excited states crowd the spectrum near the crossing. If the hardware cannot help, perhaps the initial state can. The rank-one framework uses the uniform superposition $\ket{\psi_0}$ because it is the natural starting point for unstructured search. But any state $\ket{\psi}$ defines weights $w_k(\psi)$ and hence a crossing position $s^*(\psi)$. Could a different initial state make $s^*$ independent of the spectrum?

\begin{theorem}[Universality of uniform superposition]
\label{thm:universality}
Among all states $\ket{\psi} \in \mathbb{C}^N$, the uniform superposition $\ket{\psi_0}$ is the unique state (up to per-basis-element phases) for which the weights $w_k(\psi)$ depend only on $\{E_k, d_k\}$ and not on the specific assignment of energies to computational basis states.
\end{theorem}

\begin{proof}
An energy assignment is a function $\sigma: \{0,\ldots,N-1\} \to \{E_0,\ldots,E_{M-1}\}$ with $|\sigma^{-1}(E_k)| = d_k$. The weights under assignment $\sigma$ are $w_k(\psi,\sigma) = \sum_{z:\sigma(z)=E_k} |\braket{z}{\psi}|^2$. We require $w_k(\psi,\sigma) = w_k(\psi,\sigma')$ for all assignments $\sigma, \sigma'$ with the same degeneracies.

Any two such assignments are related by a permutation $\pi$ of $\{0,\ldots,N-1\}$. The condition becomes $\sum_{z \in \Omega_k} |\braket{z}{\psi}|^2 = \sum_{z \in \Omega_k} |\braket{\pi^{-1}(z)}{\psi}|^2$ for all $k$ and all permutations $\pi$.

\emph{Necessity.} Consider two-level spectra with $d_0 = 1$. For any two basis states $z_a, z_b$, the transposition swapping them maps the assignment $\sigma$ (with $\sigma(z_a) = E_0$) to $\sigma'$ (with $\sigma'(z_b) = E_0$). The condition forces $|\braket{z_a}{\psi}|^2 = |\braket{z_b}{\psi}|^2$. Since $z_a, z_b$ are arbitrary, $|\braket{z}{\psi}|^2 = 1/N$ for all $z$.

\emph{Sufficiency.} If $|\braket{z}{\psi}|^2 = 1/N$ for all $z$, then $w_k = d_k/N$ regardless of the assignment.
\end{proof}

\begin{corollary}
\label{cor:universality}
Within the rank-one initial-projector framework above (same Hamiltonian form
for all energy assignments with the same degeneracy structure), any
instance-independent adiabatic algorithm must use the uniform superposition as
initial state, fixing the crossing at $s^* = A_1/(A_1+1)$.
\end{corollary}

The uniform superposition is the only instance-independent choice. Any other state would assign unequal weight to at least two basis elements, and a permutation of energy levels would then produce different weights and a different crossing. Changing the initial state requires giving up instance independence, which amounts to knowing the spectrum in advance.

A more aggressive route is to keep the uniform state but add a fixed ancilla coupling $V$. The coupling might shift energy levels in a way that cancels the instance-dependent contribution to $A_1^{\mathrm{eff}}$.

\begin{theorem}[Coupled ancilla limitation]
\label{thm:coupled-ancilla}
Consider an extended Hamiltonian $H_{\mathrm{ext}}(s) = -(1-s)\ket{\Psi}\!\bra{\Psi} + s(H_z \otimes I + V)$ where $\ket{\Psi} = \ket{\psi_0} \otimes \ket{\phi}$ and $V$ is instance-independent. No fixed $V$ makes $A_1^{\mathrm{eff}}$ constant across all problem instances.
\end{theorem}

\begin{proof}
Consider the two-level family parametrized by $\Delta > 0$, with $E_0 = 0$, $E_1 = \Delta$, $d_0 = 1$, and $d_1 = N - 1$. For $\Delta > 2\lVert V \rVert$, Weyl's inequality implies that each eigenvalue of $H_f(\Delta) = H_z(\Delta) \otimes I + V$ lies within $\lVert V \rVert$ of an eigenvalue of $H_z(\Delta) \otimes I$. The spectrum therefore splits into two separated clusters, one near energy $0$ and one near energy $\Delta$. For each eigenvalue $E_j$ in the excited cluster, $|E_j - \Delta| \leq \lVert V \rVert$. Hence the excited contribution to $A_1^{\mathrm{eff}}$ is
\[
\sum_{j \in \mathrm{excited}} \frac{|\!\braket{\Psi}{\phi_j}\!|^2}{E_j - E_0}
= \frac{1 - d_0/N}{\Delta + O(\lVert V \rVert)}
= \Theta(1/\Delta)
\]
for $\Delta \gg \lVert V \rVert$. Because $\lVert V \rVert$ is fixed independently of $\Delta$, this contribution varies with $\Delta$, so $A_1^{\mathrm{eff}}(\Delta)$ cannot be constant.
\end{proof}

Coupling shifts energy levels but cannot make the excited contribution constant across all spectra. For large excitation energy $\Delta$, the coupling $V$ is a fixed perturbation against a growing gap, and Weyl's inequality bounds its effect to $O(\|V\|/\Delta)$. The last escape route within the rank-one design is to break the path into multiple segments and hope that an intermediate stage can absorb the spectrum dependence before the final segment encounters it.

\begin{theorem}[Multi-segment rigidity]
\label{thm:multi-segment}
Consider a two-segment path where segment 2 has Hamiltonian
$H_2(t) = -(1-t)\ket{\psi_{\mathrm{mid}}}\!\bra{\psi_{\mathrm{mid}}} + tH_z$,
and segment 1 is generated by an instance-independent Hamiltonian that does not
involve $H_z$. Then the intermediate state $\ket{\psi_{\mathrm{mid}}}$ must be
the uniform superposition, giving the same crossing $B_1 = A_1$.
\end{theorem}

\begin{proof}
Segment 2 is a rank-one adiabatic Hamiltonian with initial state
$\ket{\psi_{\mathrm{mid}}}$. Its crossing position is
$t^* = B_1/(B_1+1)$ where
$B_1 = \sum_{k \geq 1} w_k(\psi_{\mathrm{mid}})/(E_k - E_0)$.
Because segment 1 does not involve $H_z$, $\ket{\psi_{\mathrm{mid}}}$ is
determined entirely by an instance-independent Hamiltonian and is therefore the
same for all energy assignments with the same degeneracy structure.
\autoref{thm:universality} then forces $w_k = d_k/N$, so $B_1 = A_1$.
\end{proof}

These four modifications exhaust the natural instance-independent designs where
spectral information is not injected by measurement before the final rank-one
segment: the Hilbert-space dimension (ancillas), the initial state
(universality), fixed ancilla coupling, and path segmentation with an
$H_z$-free preparation stage. Each fails for a different reason: ancillas
decouple and leave the crossing invariant, the uniform superposition is forced
by instance independence, coupling is a finite perturbation against a variable
gap, and segmentation inherits the same constraint through the intermediate
state. Together they close off the space.

\begin{theorem}[No-go]
\label{thm:no-go}
For any adiabatic algorithm in this design class (rank-one initial Hamiltonian,
final Hamiltonian whose ground state encodes the solution, instance-independent
design, and no $H_z$ dependence before the final rank-one segment), the crossing
position cannot be made independent of the problem spectrum.
\end{theorem}

\begin{proof}
Combine Theorems~\ref{thm:product-ancilla}--\ref{thm:multi-segment}.
\autoref{thm:universality} forces the uniform superposition.
\autoref{thm:product-ancilla} shows that uncoupled ancillas preserve $s^*$.
\autoref{thm:coupled-ancilla} shows that coupled ancillas can shift $s^*$ but
cannot make it constant. \autoref{thm:multi-segment} then rules out escape
through multi-segment paths within the class.
\end{proof}

For the running example ($N = 4$, $d_0 = 1$), product ancilla invariance (\autoref{thm:product-ancilla}) implies that appending any number of ancilla qubits in a product state leaves the crossing at $s^* = 3/7$. Universality (\autoref{thm:universality}) means that any other initial state would break instance independence on this family. Coupled ancillas (\autoref{thm:coupled-ancilla}) can perturb the crossing but cannot pin it to a constant, because the perturbation is bounded while the excitation gap varies freely.

The no-go is best understood as a structural constraint rather than an
engineering failure. The crossing position $s^* = A_1/(A_1+1)$ is determined by
the secular equation of $H(s)$, which depends algebraically on the overlap
between the initial state and each energy eigenspace. Instance independence
forces those overlaps to be uniform, and uniform overlaps make $s^*$ a
nontrivial function of the spectrum through the weighted harmonic mean
$A_1 = \sum_{k \geq 1} (d_k/N)/(E_k - E_0)$. Within this design class, no
instance-independent transformation can break this dependence, because the
secular equation encodes spectral data into the crossing position, and no
design that treats all energy assignments equally can undo the encoding.

We have now exhausted this rank-one design class under instance independence.
The natural objection is that rank one may simply be too narrow. We next move
to rank-$k$ projectors and show that the same dependence persists.

For rank-$k$ projectors $P = UU^\dagger$, the secular equation becomes a
$k \times k$ determinant condition
\[
\det(I_k - (1-s)G(\lambda,s)) = 0, \qquad
G(\lambda,s) = U^\dagger(sH_z - \lambda I)^{-1}U.
\]
On the two-level family ($E_0 = 0$, $E_1 = \Delta$), this reduces to
\[
\det(I_k - (x/\Delta)B) = 0, \qquad
B = U_{\mathrm{exc}}^\dagger U_{\mathrm{exc}}, \quad x = (1-s)/s.
\]

Each positive eigenvalue $\mu$ of $B$ gives a crossing branch $s(\Delta) = 1/(1+\Delta/\mu)$, and this branch is non-constant in $\Delta$.

\begin{proposition}[Rank-$k$ two-level obstruction]
\label{prop:rank-k-obstruction}
Fixed rank-$k$ projectors cannot make crossing positions spectrum-independent on fixed-degeneracy two-level families unless the projector has zero support on excited states.
\end{proposition}

\begin{proof}
On the two-level family ($E_0 = 0$, $E_1 = \Delta$), each positive eigenvalue $\mu$ of $B = U_{\mathrm{exc}}^\dagger U_{\mathrm{exc}}$ gives a crossing branch at $s(\Delta) = 1/(1+\Delta/\mu)$, which is non-constant in $\Delta$ whenever $\mu > 0$. If the projector has zero support on excited states, then $B = 0$ and there are no crossing branches, but also no gap closing.
\end{proof}

The two-level obstruction already blocks constant crossings in the simplest
nontrivial family. For general multilevel spectra, a trace argument gives an
even sharper obstruction.

\begin{proposition}[Trace no-go]
\label{prop:trace-nogo}
For a rank-$k$ projector $P = UU^\dagger$ and the multilevel family with gaps $\Delta_1, \ldots, \Delta_{M-1}$, if $B_j = U_j^\dagger U_j \neq 0$ and $\Delta_j$ varies, then at least one crossing position must change with $\Delta_j$.
\end{proposition}

\begin{proof}
Define the reduced matrix $A(\Delta) = \sum_{\ell=1}^{M-1} B_\ell/\Delta_\ell$ where $B_\ell = U_\ell^\dagger U_\ell \succeq 0$ collects the excited-level contributions. The trace $\mathrm{tr}(A(\Delta)) = \sum_\ell \mathrm{tr}(B_\ell)/\Delta_\ell$ is non-constant in $\Delta_j$ because $\mathrm{tr}(B_j) > 0$. By Weyl's eigenvalue monotonicity theorem, each eigenvalue of $A(\Delta)$ is a continuous function of $\Delta_j$, and their sum equals $\mathrm{tr}(A)$. Since the trace changes, at least one positive eigenvalue, and hence at least one crossing position $s_r = G_r/(1+G_r)$, must change with $\Delta_j$.
\end{proof}

\begin{remark}
When the excited blocks commute ($[B_\ell, B_m] = 0$ for all $\ell, m$), the reduced crossing equation can be simultaneously diagonalized. This gives explicit per-branch formulas. For each active branch $r$ with $G_r(\Delta) = \sum_\ell \mu_{\ell r}/\Delta_\ell > 0$, the crossing position is $s_r = G_r/(1 + G_r)$. Varying any gap $\Delta_j$ yields
\[
\frac{\partial s_r}{\partial \Delta_j}
= -\frac{\mu_{jr}}{\Delta_j^2(1 + G_r)^2}
\leq 0,
\]
with strict inequality whenever $\mu_{jr} > 0$. So the commuting case gives explicit quantitative non-constancy for each branch, complementing the trace argument's aggregate statement. Even this most tractable version of the generalized secular equation cannot produce spectrum-independent crossings.
\end{remark}

At this point the pattern is hard to miss. The barrier is structural inside the
rank-one framework and survives the move to higher-rank projectors. It may
still be possible to escape with genuinely different control models, such as
time-dependent couplings or non-rank-one intermediate Hamiltonians, but those
lie outside the present theorem. This distinction matters because once we drop
the monotone-schedule restriction, constant controls already recover the Grover
timescale on the restricted two-level family.

\begin{proposition}[Constant-control optimality on two-level family~\cite{eduardo}]
\label{prop:constant-control}
For $H_z = I - P_0$ where $P_0$ projects onto the $d_0$-dimensional ground space, the continuous-time rank-one Hamiltonian $H = -\ket{\psi_0}\!\bra{\psi_0} + H_z$ with constant controls achieves $p_0(t^*) = 1$ at $t^* = (\pi/2)\sqrt{N/d_0}$, with controls independent of $A_1$.
\end{proposition}

\begin{proof}
Let $\mu = d_0/N$, $\ket{G} = d_0^{-1/2}\sum_{x \in S_0}\ket{x}$, and $\ket{B} = (N - d_0)^{-1/2}\sum_{x \notin S_0}\ket{x}$. The initial state is $\ket{\psi_0} = \sqrt{\mu}\,\ket{G} + \sqrt{1-\mu}\,\ket{B}$. Dropping the global identity term, the effective Hamiltonian in the $(\ket{G}, \ket{B})$ basis is
\begin{equation}
\widetilde{H} = -\begin{pmatrix} \mu & \sqrt{\mu(1-\mu)} \\ \sqrt{\mu(1-\mu)} & -\mu \end{pmatrix},
\end{equation}
which satisfies $\widetilde{H}^2 = \mu\,I_2$. The matrix exponential is $e^{-it\widetilde{H}} = \cos(\sqrt{\mu}\,t)\,I_2 - i\sin(\sqrt{\mu}\,t)\,\widetilde{H}/\sqrt{\mu}$. Applying to $\ket{\psi_0}$ and computing the ground-state probability:
\begin{equation}
p_0(t) = |\!\bra{G}e^{-it\widetilde{H}}\ket{\psi_0}\!|^2 = \mu + (1 - \mu)\sin^2(\sqrt{\mu}\,t).
\end{equation}
At $t^* = (\pi/2)/\sqrt{\mu} = (\pi/2)\sqrt{N/d_0}$, $\sin^2(\sqrt{\mu}\,t^*) = 1$, so $p_0(t^*) = 1$.
\end{proof}

On the two-level family, the Hamiltonian self-calibrates. The dynamics reduce to a Rabi oscillation at frequency $\sqrt{\mu} = \sqrt{d_0/N}$, independent of $A_1$. The time-independent Hamiltonian $H_r = -\ket{\psi_0}\!\bra{\psi_0} + r \cdot H_z$ with $r = 1$ is optimal without explicit knowledge of $A_1$~\cite{eduardo}. The monotone-schedule barrier is a barrier for schedules, not for continuous-time dynamics. Dropping the monotonicity constraint allows a qualitatively different mechanism (resonance instead of adiabatic passage) that does not require the crossing position. For general spectra, the resonance shifts to $r^* = A_1$~\cite{eduardo}, and the calibration problem changes from classical computation of $A_1$ to quantum detection of a resonance.

\begin{remark}[Non-adiabatic barrier extension]
The $A_1$ barrier extends beyond the adiabatic framework. For the
time-independent Hamiltonian
$H_r = -\ket{\psi_0}\!\bra{\psi_0} + r \cdot H_z$, the resonance condition
$r = A_1$ depends on the problem spectrum. Since $A_1$ varies across instances
(\autoref{thm:coupled-ancilla}), no instance-independent choice of $r$ achieves
resonance universally. The same structural ingredients used above---uniform
instance-independent driver weights and secular-equation dependence on spectral
data---apply to the oscillation parameter $r$ exactly as they apply to the
crossing position $s^*$. Thus the barrier governs both adiabatic schedules
(which need $s^* = A_1/(A_1 + 1)$) and non-adiabatic oscillation (which needs
$r \approx A_1$).
\end{remark}

A natural approach is Loschmidt-echo measurement. Evolve under $H_r$ and measure return probability $|\!\braket{\psi_0}{e^{-iH_r t}\psi_0}\!|^2$, which oscillates with large amplitude near resonance and stays close to $1$ away from resonance. On two-level families this works cleanly: binary search over $r$ with $O(n)$ probe measurements, each costing $O(1/g_{\min})$, locates $r^*$ with polynomial overhead (details in~\cite{eduardo}). For general multilevel spectra, additional excited-state frequencies can mask the resonance signal. Whether one can efficiently deconvolve this multilevel echo, or replace it with a better observable, remains open.

The constant-control counterexample applies only to the two-level family $H_z = I - P_0$. Under normalized controls, the barrier reappears.

\begin{proposition}[Normalized-control lower bound]
\label{prop:normalized-control}
Under normalized controls $|g(t)| \leq 1$ and the scaled family
$H_z^{(\delta)} = \delta(I - P_0)$ with minimum excitation
$0<\delta\le 1$, any instance-independent algorithm achieving success
probability $\geq 2/3$ requires $T = \Omega(\sqrt{N/d_0}/\delta)$.
\end{proposition}

\begin{proof}
The oracle-dependent term $g(t)H_z^{(\delta)} = \delta g(t)(I - P_0)$ has instantaneous oracle strength $\delta|g(t)|$. The total oracle action is $\mathcal{A} = \int_0^T \delta|g(t)|\,dt \leq \delta T$. The continuous-time query lower bound for unstructured search with $d_0$ marked items among $N$ gives $\mathcal{A} = \Omega(\sqrt{N/d_0})$~\cite{farhi1998analog}, so $T = \Omega(\sqrt{N/d_0}/\delta)$.
\end{proof}

For $\delta = N^{-1/2}$, this yields $T = \Omega(N/\sqrt{d_0})$, the same exponential penalty as the fixed-schedule adiabatic model. The barrier reappears whenever controls are normalized. The scope of the obstruction is now precise: it comes from bounded, instance-independent controls on rank-one Hamiltonians. It does not come from continuous-time quantum computation itself, as the constant-control counterexample on two-level families demonstrates. The gap between these statements, general spectra with unbounded controls, remains open.


\section{Computational Nature of \texorpdfstring{$A_1$}{A1}}
\label{sec:computational-nature}

The barrier survives every design modification within the rank-one framework and extends to higher-rank projectors. It cannot be engineered away. The next question is what kind of computational object $A_1$ actually is. The NP-hardness of Chapter~8 comes from a reduction that maps satisfiability to spectral estimation. But $A_1 = (1/N)\sum_{k \geq 1} d_k/(E_k - E_0)$ is a weighted sum over the entire excited spectrum, not a decision problem about the ground state. Its hardness turns out to be counting hardness, inherited from the partition-function structure of the energy landscape. This distinction matters because it separates the tractability of $A_1$ from the tractability of the underlying optimization problem.

The quantity $A_1$ aggregates the full spectrum, not just the gap edge. A minimal example makes this concrete. Consider three energy levels with $E_0 = 0$ ($d_0 = 1$), $E_1 = 1/n$ ($d_1 = 1$), and $E_2 = 1$ ($d_2 = N-2$). Then $\Delta = 1/n \to 0$ but
$A_1 = (n + N - 2)/N \to 1$, so $1/\Delta \to \infty$ while $A_1 = \Theta(1)$.
The tail of $N-2$ states at energy $1$ contributes $(N-2)/N \approx 1$ to
$A_1$, while the single state at the gap edge contributes only $n/N \approx 0$.
So the crossing position $s^* = A_1/(A_1+1) \approx 1/2$ is determined by the
bulk of the spectrum, not by the minimum gap. In this sense $A_1$ is a
whole-spectrum quantity that $\Delta$ alone cannot predict.

The distinction between NP-hardness at precision $1/\mathrm{poly}(n)$ (\autoref{thm:np-hard-A1}) and $\#$P-hardness exactly (\autoref{thm:sharp-P-hard-A1}) matters because $A_1$ is fundamentally a counting quantity.

\begin{proposition}[$A_1$ hardness is counting hardness]
\label{prop:counting-hardness}
For Boolean CSPs where counting satisfying assignments is $\#$P-hard (including $k$-SAT for $k \geq 2$), computing $A_1$ of the clause-violation Hamiltonian is $\#$P-hard even restricted to satisfiable instances.
\end{proposition}

\begin{proof}
Encode the CSP as $H_z = \sum_{j=1}^m C_j$ where each $C_j(x) = 1$ if assignment $x$ violates clause $j$. The interpolation argument (\autoref{thm:sharp-P-hard-A1}) recovers all degeneracies $d_k$ from polynomially many evaluations of $A_1$ with shifted parameters, via Lagrange interpolation on the rational function $f(x) = \sum_k d_k/(\Delta_k + x/2)$. For satisfiable CSPs, $d_0$ counts satisfying assignments, and counting is $\#$P-hard by hypothesis.
\end{proof}

The connection between $A_1$ and the partition function makes this precise and reveals why the hardness is counting rather than optimization. Shifting energies so that $E_0 = 0$ and defining the Laplace partition function $Z(\beta) = \sum_x e^{-\beta E(x)}$, the spectral parameter admits the integral representation
\begin{equation}
\label{eq:A1-laplace}
A_1 = \frac{1}{N}\int_0^\infty (Z(\beta) - d_0)\,d\beta.
\end{equation}
For integer spectra with $E(x) \in \{0,1,\ldots,m\}$, the ordinary generating function $Z(t) = \sum_x t^{E(x)}$ gives
\[
A_1 = \frac{1}{N}\int_0^1 \frac{Z(t) - d_0}{t}\,dt.
\]
At first glance both formulas seem to require $d_0$, which is itself counting-hard for many CSPs. For additive approximation, however, one can replace $d_0$ by a single low-temperature sample $Z(\tau)$ with small $\tau > 0$. Define the $\tau$-truncated proxy
\begin{equation}
\label{eq:A1-proxy}
A_1^{(\tau)} = \frac{1}{N}\int_\tau^1 \frac{Z(t) - Z(\tau)}{t}\,dt.
\end{equation}
The additive error satisfies $0 \leq A_1 - A_1^{(\tau)} \leq \tau(1 + \ln(1/\tau))$. To see this, write $Z(t) - d_0 = \sum_{q \geq 1} d_q t^q$ and compute
\[
(A_1 - A_1^{(\tau)})N
= \int_0^{\tau}\sum_{q \geq 1} d_q t^{q-1}\,dt
  + \Bigl(\sum_{q \geq 1} d_q \tau^q\Bigr)\ln\frac{1}{\tau}
= \sum_{q \geq 1} d_q \tau^q \Bigl(\frac{1}{q} + \ln\frac{1}{\tau}\Bigr).
\]
All terms are nonnegative, giving the lower bound. For the upper bound, use $\tau^q \leq \tau$ and $1/q \leq 1$ to get $(A_1 - A_1^{(\tau)})N \leq (N - d_0)\tau(1 + \ln(1/\tau)) \leq N\tau(1 + \ln(1/\tau))$. The bound is tight: for the two-level Grover spectrum, equality holds up to the factor $(N - d_0)/N$. Choosing $\tau = O(\eta/\ln(1/\eta))$ therefore gives an $\eta$-approximation to $A_1$ without direct access to $d_0$.

When $E_0$ is known, this coarse form is already useful. Boltzmann sampling at
inverse temperature $\beta$ gives unbiased estimators of observables under the
Boltzmann law; combining such samples across temperatures (for example via
partition-function ratio estimation / thermodynamic integration) yields
estimates of $Z(\beta)/Z(0)=Z(\beta)/N$. Integrating those estimates through
Eq.~\eqref{eq:A1-laplace} gives an additive approximation to $A_1$ without
explicitly counting $d_0$. In this way, computing $A_1$ becomes a
partition-function task, and tractability of $A_1$ tracks tractability of
counting.

The truncated proxy $A_1^{(\tau)}$ works for integer spectra. For general nonnegative energies, the Laplace identity~\eqref{eq:A1-laplace} requires an analogous truncation. Define the \emph{anchored proxy} at inverse temperature $B > 0$:
\begin{equation}
\label{eq:A1-laplace-proxy}
A_1^{[B]} := \frac{1}{N}\int_0^B \bigl(Z(\beta) - Z(B)\bigr)\,d\beta.
\end{equation}
Anchoring at $Z(B)$ instead of $d_0$ removes the ground-degeneracy dependence, and truncation at $B$ avoids the infinite tail.

\begin{proposition}[Laplace-side proxy error]
\label{prop:laplace-proxy}
For all $B > 0$,
\[
0 \leq A_1 - A_1^{[B]} \leq e^{-B\Delta_{\min}}\!\left(B + \frac{1}{\Delta_{\min}}\right).
\]
\end{proposition}

\begin{proof}
Write $Z(\beta) - d_0 = \sum_{x : E(x) > 0} e^{-\beta E(x)}$ and compute
\[
(A_1 - A_1^{[B]})N = \sum_{x : E(x) > 0} e^{-B E(x)}\!\left(\frac{1}{E(x)} + B\right),
\]
which is nonnegative. Since $E(x) \geq \Delta_{\min}$, we have $e^{-B E(x)} \leq e^{-B\Delta_{\min}}$ and $1/E(x) \leq 1/\Delta_{\min}$, so the sum is at most $N\,e^{-B\Delta_{\min}}(B + 1/\Delta_{\min})$.
\end{proof}

The tail decays as $Be^{-B\Delta_{\min}}$, so additive error $\eta$ requires $B = \Theta((1/\Delta_{\min})\log(1/\eta))$. At schedule-relevant precision $\eta = 2^{-n/2}$, this forces $B = \Theta(n/\Delta_{\min})$: low-temperature partition functions are unavoidable.

When $E_0$ and a lower bound $\Delta_{\min}$ are both known, even the simplest estimator suffices at coarse precision.

\begin{remark}[BPP coarse approximation]
\label{rem:bpp-coarse}
Define $G(x) = 0$ if $E(x) = E_0$ and $G(x) = 1/(E(x) - E_0)$ otherwise. Then $A_1 = \mathbb{E}_{x \sim \mathrm{Unif}}[G(x)]$ with $0 \leq G(x) \leq 1/\Delta_{\min}$. By Hoeffding's inequality~\cite{hoeffding1963probability}, the empirical mean of $T$ i.i.d.\ uniform samples satisfies $\Pr[|\widehat{A}_1 - A_1| > \eta] \leq \delta$ for $T \geq (1/(2\eta^2\Delta_{\min}^2))\ln(2/\delta)$. In particular, additive inverse-polynomial approximation of $A_1$ is in $\mathrm{BPP}$ under the promise that $E_0$ and $\Delta_{\min}$ are known and inverse-polynomially bounded.
\end{remark}

This stands in sharp contrast to the NP-hardness of $A_1$ estimation without the $E_0$ promise (\autoref{thm:np-hard-A1}). The promise boundary is the ground energy itself: once the counting problem is partially solved by revealing $E_0$, the remaining estimation is a standard mean-estimation task.

The proxies above reduce $A_1$ to a weighted integral of partition-function values. An approximate partition-function oracle therefore yields an approximate $A_1$.

\begin{proposition}[Partition-function oracle reduction]
\label{prop:oracle-reduction}
Assume integer energies in $\{0, 1, \ldots, m\}$ with $E_0 = 0$. Fix $\varepsilon \in (0,1)$ and suppose an oracle returns $\widehat{Z}(t)$ with $|\widehat{Z}(t) - Z(t)| \leq \alpha N$ for any $t \in [\varepsilon, 1]$. The midpoint-rule estimator with $K$ quadrature points satisfies
\[
\bigl|\widehat{A}_1^{(\varepsilon)} - A_1^{(\varepsilon)}\bigr|
\leq 2\alpha\ln\frac{1}{\varepsilon} + \frac{m\ln^2(1/\varepsilon)}{2K}.
\]
Choosing $\alpha \leq \eta/(8\ln(1/\varepsilon))$ and $K \geq 4m\ln^2(1/\varepsilon)/\eta$ ensures the right-hand side is at most $\eta/2$, so combined with Eq.~\eqref{eq:A1-proxy} the total error is at most $\eta$.
\end{proposition}

\begin{proof}
Change variables $u = \ln t$, so $A_1^{(\varepsilon)} = (1/N)\int_{\ln\varepsilon}^0 (Z(e^u) - Z(\varepsilon))\,du$. The oracle error at each point is at most $2\alpha N$ (triangle inequality on the difference $\widehat{Z}(t) - \widehat{Z}(\varepsilon)$), contributing $2\alpha\ln(1/\varepsilon)$ after integration and normalization. For the quadrature error, define $F(u) = Z(e^u) - Z(\varepsilon)$. Since $Z'(t) = \sum_{q \geq 1} q\,d_q\,t^{q-1} \leq mN$ for $t \in [0,1]$, we have $|F'(u)| = |e^u Z'(e^u)| \leq mN$. The midpoint rule on $K$ intervals of width $h = \ln(1/\varepsilon)/K$ incurs total error at most $(mN/2) \cdot h \cdot \ln(1/\varepsilon)$. Dividing by $N$ gives the quadrature term.
\end{proof}

Whenever a model family admits a polynomial-time partition-function approximation scheme, the oracle reduction converts it into a polynomial-time $A_1$ estimator at the same precision. The ferromagnetic Ising model provides a concrete instantiation.

\begin{proposition}[Ferromagnetic Ising tractability at coarse precision]
\label{prop:ising-fpras}
Consider a ferromagnetic Ising model with nonnegative couplings $J_{ij}$ and fields $h_i$, energy range $R \leq 1$, and minimum excitation $\Delta_{\min} \geq 1/W$ where $W = \sum J_{ij} + \sum h_i$. If a multiplicative FPRAS for $Z(\beta)$ is available on $\beta \in [0, B]$~\cite{jerrum1993polynomial}, then $A_1$ admits an additive-$\eta$ approximation in time $\mathrm{poly}(n, W, 1/\eta, \log(1/\delta))$ for any inverse-polynomial $\eta$.
\end{proposition}

\begin{proof}
Apply \autoref{prop:laplace-proxy} with $B = O(W\log(W/\eta))$ to make the tail at most $\eta/3$. The midpoint-rule quadrature with $K = \Theta(RB^2/\eta)$ points introduces error at most $\eta/3$. At each point, the FPRAS provides multiplicative accuracy $\mu = O(\eta/B)$, contributing at most $2\mu B \leq \eta/3$. The FPRAS runtime per query is $\mathrm{poly}(n, 1/\mu, \log(1/\delta_q))$, and union-bounding over $K + 1$ queries gives overall success probability $1 - \delta$.
\end{proof}

The tractability does not survive at schedule-relevant precision. At $\eta = \Theta(2^{-n/2})$, the required multiplicative accuracy $\mu = O(2^{-n/2}/B)$ forces FPRAS runtime $\mathrm{poly}(1/\mu) = 2^{\Omega(n)}$, restoring the exponential cost. This is a precision barrier, not a model barrier: the algorithmic machinery works at every precision, but the cost crosses the polynomial-to-exponential boundary exactly at the schedule-relevant scale.

The partition-function chain above handles the generic case. For structured families, stronger results are possible. The most direct positive example is bounded treewidth, where variable elimination computes $A_1$ exactly without any precision limitation.

\begin{proposition}[Bounded-treewidth tractability]
\label{prop:treewidth}
Consider local integer-valued energy functions
$E(x) = \sum_j E_j(x_{S_j})$ with bounded locality $|S_j|\leq k$
and total range $E(x)-E_0\in\{0,1,\ldots,m\}$, given a tree decomposition
of the primal graph of width~$w$, the quantity $A_1$ is computable exactly
in $\mathrm{poly}(n,m)\cdot 2^{O(w)}$ time.
\end{proposition}

\begin{proof}
Write the partition-function polynomial as $Z(t) = \sum_x t^{E(x)} = \sum_{q=0}^m d_q t^q$, and also in factor-graph form as $Z(t) = \sum_x \prod_j t^{E_j(x_{S_j})}$. Variable elimination on the tree decomposition then computes $Z(t)$ exactly. At each elimination step, factor tables have at most $2^{w+1}$ entries, each a polynomial of degree at most $m$. Multiplying factors convolves polynomials at cost $O(m^2)$ per entry, and summing out a variable adds two polynomials at cost $O(m)$. After $n$ elimination steps, we recover $Z(t) = \sum_q d_q t^q$, and then $A_1 = (1/N)\sum_{q > E_0} d_q/(q - E_0)$.
\end{proof}

The treewidth condition is sufficient, not necessary. A simpler criterion
applies when the spectrum itself is simple. If $H_z$ has at most
$\mathrm{poly}(n)$ distinct energy levels with known energies and degeneracies,
then $A_1 = (1/N)\sum_{k \geq 1} d_k/(E_k - E_0)$ is directly computable in
$\mathrm{poly}(n)$ time from its defining sum. This criterion is complementary
to bounded treewidth and applies whenever the spectrum is structurally simple,
regardless of the interaction graph. For example, Hamming-distance costs
$E(x) = |x \oplus z_0|$ have $M = n+1$ levels, degeneracies
$d_k = \binom{n}{k}$, and energies $E_k = k$. Then
$A_1 = (1/N)\sum_{k=1}^n \binom{n}{k}/k$ depends only on $n$ and is trivial to
compute.

The bridge between $A_1$ and $Z$ is one-directional. Tractable $Z$ implies tractable $A_1$, because the integral representations above reduce $A_1$ to a weighted integral of partition-function values. But exact $A_1$ does not determine low-temperature $Z(\beta)$: the single number $A_1$ compresses the entire degeneracy sequence $\{d_k\}$ into one weighted sum, losing the individual terms.

\begin{proposition}[Reverse bridge obstruction]
\label{prop:reverse-bridge}
There exist two diagonal Hamiltonians $H_z$, $H_z'$ on $N = 2^n$ states with the same ground degeneracy ratio $d_0/N$, same minimum excitation $\Delta_{\min}$, and $A_1(H_z) = A_1(H_z')$ exactly, yet $|Z_{H_z}(\beta) - Z_{H_z'}(\beta)|/N \geq 1/100$ at $\beta = O(1/\Delta_{\min})$.
\end{proposition}

\begin{proof}
Fix an integer $B \geq 3$. Define two spectra with the same $d_0/N = 1/2$ and $\Delta_{\min} = 1/B$. The first has $N/8$ states at energy $1/B$ and $3N/8$ states at energy $B$. The second has $N/16$ states at energy $1/B$ and $7N/16$ states at energy $c_B = 7B/(B^2+6)$. Direct computation gives the same value $A_1 = (B^2+3)/(8B)$ for both. At $\beta = B$, however,
\[
\frac{Z_1(B)}{N} = \frac{1}{2} + \frac{e^{-1}}{8} + \frac{3e^{-B^2}}{8},
\qquad
\frac{Z_2(B)}{N} = \frac{1}{2} + \frac{e^{-1}}{16} + \frac{7}{16}e^{-7B^2/(B^2+6)}.
\]
Since $7B^2/(B^2+6) \geq 4.2$ for $B \geq 3$, the difference is at least $e^{-1}/16 - (7/16)e^{-4.2} > 1/100$.
\end{proof}

Three natural conjectures about easy instances of $A_1$ computation are all false.

One might expect that unique solutions simplify the spectral structure enough to make $A_1$ tractable, since a single ground state leaves less room for combinatorial complexity in the excited spectrum.

\begin{proposition}[Unique solution does not imply easy $A_1$]
\label{prop:conjecture-unique}
There exist instances with $d_0 = 1$ for which computing $A_1$ is $\#$P-hard.
\end{proposition}

\begin{proof}
The proof of \autoref{prop:counting-hardness} applies directly to satisfiable instances with $d_0 = 1$. The interpolation reduction recovers $d_1, \ldots, d_{M-1}$ from $A_1$ evaluations, and counting assignments at each violation level remains $\#$P-hard. Concretely, for a satisfiable 3-SAT instance with $m$ clauses and a unique satisfying assignment, the clause-violation Hamiltonian $H_z = \sum_j C_j$ has $d_0 = 1$ and
\[
A_1 = \sum_{k=1}^m \frac{d_k}{kN},
\]
where $d_k$ counts assignments that violate exactly $k$ clauses. Recovering these counts from shifted $A_1$ evaluations is therefore $\#$P-hard.
\end{proof}

If all excited degeneracies are small, the spectrum is spread thin. A natural guess is that $A_1$ simplifies because each term $d_k/(E_k - E_0)$ contributes little.

\begin{proposition}[Bounded degeneracy is vacuous]
\label{prop:conjecture-bounded}
If $d_k \leq \mathrm{poly}(n)$ for all excited levels $k \geq 1$, and $M \leq \mathrm{poly}(n)$, then $d_0 \geq N - \mathrm{poly}(n)^2$, and the optimization problem is trivially solvable by random sampling.
\end{proposition}

\begin{proof}
The total state count satisfies $\sum_{k=0}^{M-1} d_k = N = 2^n$. If $d_k \leq \mathrm{poly}(n)$ for all $k \geq 1$ and $M \leq \mathrm{poly}(n)$, then $\sum_{k \geq 1} d_k \leq (M-1) \cdot \mathrm{poly}(n) \leq \mathrm{poly}(n)^2$, so $d_0 \geq N - \mathrm{poly}(n)^2$. For $n$ large enough, $d_0/N \geq 1 - o(1)$, and a random sample finds a ground state with probability $1 - o(1)$.
\end{proof}

Hard optimization problems have complex energy landscapes, so one might expect $A_1$ to be hard precisely when optimization is hard.

\begin{proposition}[Hard optimization does not imply hard $A_1$]
\label{prop:conjecture-hard-opt}
The tractability of $A_1$ is independent of optimization hardness. 2-SAT is in P but $\#$2-SAT is $\#$P-complete~\cite{valiant1979complexity}, giving easy optimization with hard $A_1$. In the reverse direction, Grover search with a promised ground degeneracy $d_0$ gives hard optimization but trivial $A_1 = (N - d_0)/(N\Delta)$, computable in $O(1)$ from the promise.
\end{proposition}

\begin{proof}
For the first direction, use the 2-SAT clause-violation Hamiltonian
$H_z=\sum_j C_j$. Optimization (finding whether the minimum is $0$ and producing
an optimal assignment) is polynomial-time because 2-SAT is in P, while computing
$A_1$ remains $\#$P-hard by \autoref{prop:counting-hardness} together with
$\#$2-SAT completeness~\cite{valiant1979complexity}. Thus easy optimization can
coexist with hard $A_1$.

For the reverse direction, consider the two-level Grover family
$H_z=\Delta(I-P_0)$ with promised $d_0=\mathrm{rank}(P_0)$. In this family,
\[
A_1=\frac{1}{N}\frac{N-d_0}{\Delta}=\frac{N-d_0}{N\Delta},
\]
so $A_1$ is computable in constant time from $(N,d_0,\Delta)$. Yet the
associated optimization/search task is unstructured search with $d_0$ marked
items, which is hard in the black-box model (query complexity
$\Theta(\sqrt{N/d_0})$ quantum). Hence hard optimization can coexist with easy
$A_1$.
\end{proof}

The tractability boundary for $A_1$ is therefore orthogonal to the standard
complexity-theoretic divisions. Unique solutions do not help. Bounded
degeneracies trivialize the problem for the wrong reason (the optimization
itself becomes easy). Easy optimization can coexist with hard $A_1$ (2-SAT),
and hard optimization can coexist with easy $A_1$ (Grover with a promise). The
parameter that controls $A_1$ tractability is the partition-function structure
of the energy landscape: treewidth of the interaction graph, availability of
polynomial-time sampling algorithms, and the algebraic properties of the
degeneracy sequence. This is a counting-theoretic boundary, not an
optimization-theoretic one.


\section{Complexity Landscape}
\label{sec:complexity-landscape}

We now have the full cost structure of ignorance within the adiabatic framework: uninformed schedules pay exponentially, partial knowledge helps linearly, adaptive measurement restores optimality, and the barrier is structural. The remaining question is quantitative: at the algorithmically relevant precision $\varepsilon = 2^{-n/2}$, what are the tight complexity bounds for estimating $A_1$ itself? The answer pins down the information cost of fixed-schedule optimization in both the query model and the computational model.

\begin{theorem}[Tight quantum query complexity at schedule precision]
\label{thm:tight-quantum}
At the schedule-relevant precision $\varepsilon = \Theta(2^{-n/2})$, the quantum query complexity of $A_1$ estimation is $\Theta(2^{n/2}) = \Theta(1/\varepsilon)$. The lower bound is witnessed by two-level instances with $\Delta_1 = 1$.
\end{theorem}

\begin{proof}
For $\varepsilon = \Theta(2^{-n/2})$ and $\Delta_1 = 1$,
\autoref{thm:quantum-A1} gives
$O(\sqrt{N} + 1/\varepsilon) = O(2^{n/2})$.

For the lower bound, restrict to $M = 2$ instances with $\Delta_1 = 1$. Then
estimating $A_1 = (N-d_0)/N$ to precision $\varepsilon$ is exactly additive
approximate counting for $d_0/N$. By the standard quantum query lower bound for
approximate counting on this two-level family~\cite{BrassardHoyerMoscaEtAl2002,
NayakWu1999}, this requires $\Omega(1/\varepsilon)$ queries (e.g., at the
worst-case operating point $d_0=\Theta(N)$ with density bounded away from
$0,1$). Hence at $\varepsilon=\Theta(2^{-n/2})$ we obtain
$\Omega(2^{n/2})$.

Combining upper and lower bounds gives
$\Theta(2^{n/2}) = \Theta(1/\varepsilon)$.
\end{proof}

\noindent The lower bound is inherited directly from approximate counting on the
two-level subclass. Metrology-style Fisher-information arguments give
compatible intuition for the same scaling, but are not needed for the
query-complexity proof.

The tight bound $\Theta(2^{n/2})$ for quantum $A_1$ estimation at schedule precision connects directly to the adaptive protocol of \autoref{sec:quantum-bypass}. The adaptive strategy achieves $T_{\mathrm{adapt}} = O(T_{\mathrm{inf}}) = O(2^{n/2})$, matching the estimation complexity. Both tasks require resolving $\Omega(2^{n/2})$ alternatives from quantum measurements that each reveal $O(1)$ bits. The adaptive protocol and the $A_1$ estimation algorithm operate at the same scale because they face the same information-theoretic bottleneck: the Heisenberg limit for sequential quantum sensing.

In the high-precision regime relevant to schedule placement, the quadratic quantum advantage persists.

\begin{proposition}[Precision phase diagram]
\label{prop:precision-phase}
For two-level instances with $\Delta_1 = 1$ and precision $\varepsilon \leq c/\sqrt{N}$ (constant $c$), the query complexity of $A_1$ estimation is $\Theta(1/\varepsilon)$ quantum and $\Theta(1/\varepsilon^2)$ classical. Equivalently, the classical-to-quantum overhead factor is $\Theta(1/\varepsilon)$ (quadratic quantum advantage).
\end{proposition}

\begin{proof}
In this regime, $1/\varepsilon \geq \sqrt{N}/c$, so the upper bound of \autoref{thm:quantum-A1} becomes $O(\sqrt{N} + 1/\varepsilon) = O(1/\varepsilon)$. The matching quantum lower bound is the standard two-level approximate-counting lower bound $\Omega(1/\varepsilon)$~\cite{BrassardHoyerMoscaEtAl2002, NayakWu1999}. The classical lower bound $\Omega(1/\varepsilon^2)$ follows from \autoref{thm:classical-lower-A1}, and Monte Carlo sampling gives the matching upper bound $O(1/\varepsilon^2)$.
\end{proof}

The previous proposition is still a query statement. The next theorem asks what
happens when we account for total computation time under a standard complexity
assumption.

\begin{theorem}[ETH computational complexity]
\label{thm:eth}
Under the Exponential Time Hypothesis (ETH), any classical algorithm computing $A_1$ at precision $2^{-n/2}$ requires $2^{\Omega(n)}$ time.
\end{theorem}

\begin{proof}[Proof sketch]
The NP-hardness reduction (\autoref{thm:np-hard-A1}) maps 3-SAT on $n_{\mathrm{var}}$ variables to $A_1$ estimation of a 3-local Hamiltonian on $n = O(n_{\mathrm{var}})$ qubits at precision $1/\mathrm{poly}(n)$. By the Impagliazzo-Paturi-Zane sparsification lemma~\cite{impagliazzo2001problems}, 3-SAT on $n_{\mathrm{var}}$ variables can be reduced to instances with $O(n_{\mathrm{var}})$ clauses, giving $n = O(n_{\mathrm{var}})$ qubits. Any algorithm computing $A_1$ at precision $2^{-n/2}$ can, in particular, compute $A_1$ at the coarser precision $1/\mathrm{poly}(n)$ (since $2^{-n/2} < 1/\mathrm{poly}(n)$ for large $n$), and thus solves 3-SAT by the reduction. Under ETH, 3-SAT on $n_{\mathrm{var}} = \Omega(n)$ variables requires $2^{\Omega(n_{\mathrm{var}})} = 2^{\Omega(n)}$ time. The linear clause-to-qubit correspondence guaranteed by sparsification is essential: without it, the exponential lower bound on variable count would not transfer to qubit count.
\end{proof}

Under ETH, the quadratic quantum advantage extends from the query model to the computational model.

\begin{corollary}[Quantum pre-computation cost]
\label{cor:quantum-precomp}
Estimating $A_1$ to the schedule-relevant precision
$\delta_{A_1} = \Theta(2^{-n/2})$ via quantum amplitude estimation costs
$\Theta(2^{n/2} \cdot \mathrm{poly}(n))$ time, matching the adaptive protocol's
runtime. In the same oracle-query model, classical estimation at this precision
requires $\Theta(2^n)$ queries by \autoref{prop:precision-phase}; under
unit-cost oracle access this implies $\Omega(2^n)$ pre-computation time
(consistent with \autoref{thm:eth}'s $2^{\Omega(n)}$ lower bound).
\end{corollary}

The information cost of fixed-schedule adiabatic optimization is exactly the Grover scale. Recovering the missing $n/2$ bits of $A_1$ costs $\Theta(2^{n/2})$ quantum time, the same order as Grover search and informed adiabatic evolution. A quantum pre-computation step followed by informed adiabatic evolution costs $\Theta(2^{n/2})$ in total. In the query model, a classical pre-computation step requires $\Theta(2^n)$ oracle queries, quadratically worse. The circuit model avoids the pre-computation entirely because amplitude amplification solves the search task without traversing the adiabatic path. From a parameterized-complexity viewpoint, $A_1$ estimation at precision $2^{-n/2}$ sits in $\mathrm{FBQTIME}(2^{n/2} \cdot \mathrm{poly}(n))$: not polynomial-time, but at the Grover scale.

These bounds assume that $A_1$ is estimated directly. One might hope that polynomial extrapolation from coarse-precision evaluations could bypass the exponential cost. The generic extrapolation barrier (\autoref{thm:generic-barrier}) rules this out. The mechanism is the Lebesgue constant $\Lambda_d(x^*) = \sum_{j=0}^d |\ell_j(x^*)|$, which measures how much evaluation noise is amplified when extrapolating from $d+1$ nodes in $[a,b]$ to a point $x^*$ outside the interval. When $x^*$ lies at least $b - a$ away from $[a,b]$, all Lagrange basis polynomials $\ell_j(x^*)$ have the same sign, so $\Lambda_d(x^*) \geq 2^{d-1}$: each additional node doubles the noise amplification rather than improving accuracy. For $A_1$ estimation, coarse evaluations at $d = \mathrm{poly}(n)$ nodes achieve polynomial precision, but the exponential Lebesgue constant means extrapolating to schedule-relevant precision $2^{-n/2}$ requires each node evaluation to have precision $2^{-\Omega(n)}$, no better than direct estimation.

\begin{proposition}[Two-level worst-case reduction]
\label{prop:structure-irrelevance}
The two-level family ($M = 2$) is a worst-case subclass for $A_1$ estimation at schedule-relevant precision: any worst-case lower bound for approximate counting on this subclass applies to general $A_1$ estimation.
\end{proposition}

\begin{proof}
Worst-case complexity over all instances is at least the complexity on any subclass. Restricting to $M = 2$ gives
\[
A_1 = \frac{N-d_0}{N\Delta_1},
\]
so for fixed $\Delta_1=1$, estimating $A_1$ to additive precision $\varepsilon$ is exactly approximate counting for $d_0/N$ at precision $\varepsilon$. Therefore, any lower bound for approximate counting on this subclass is automatically a lower bound for general $A_1$ estimation.
\end{proof}

\noindent The worst-case hardness of $A_1$ estimation does not hide in complex spectra. Simple two-level instances, where $A_1$ reduces to counting, already saturate the query lower bound. Algorithms exploiting the sum-of-reciprocals structure of $A_1$ for multilevel spectra cannot beat algorithms for plain mean estimation.

At schedule-level precision, bounded-treewidth instances remain tractable for exact $A_1$ computation (\autoref{prop:treewidth}). The partition-function methods of the previous section show that ferromagnetic Ising models admit polynomial-time $A_1$ approximation at coarse precision (\autoref{prop:ising-fpras}), but the required multiplicative accuracy forces exponential FPRAS runtime at the schedule-relevant precision $\delta_{A_1} = \Theta(2^{-n/2})$.

The interpolation theorem gives a quantitative law relating information to runtime. The sharpest formulation is a communication game.

Alice has the full classical description of $H_z$: all eigenvalues and degeneracies. Bob has a quantum computer with oracle access to $H_z$, and Alice may send $C$ classical bits to Bob. Bob must find a ground state in at most $T$ queries.

In the circuit model, Bob needs no message. He runs D\"urr-H\o yer and achieves $T = O(\sqrt{N/d_0})$ at $C = 0$. In the fixed-schedule adiabatic model, Bob must place the schedule's slowdown at the crossing, which requires knowing $s^*$ to precision $O(2^{-n/2})$. This is $n/2$ bits of information. Each bit Alice sends cuts the adiabatic runtime in half. If Alice sends $C$ bits encoding $A_1$ to precision $\varepsilon = \Theta(2^{-C})$, then
\[
T(C) = T_{\mathrm{inf}} \cdot \Theta(\max(1, 2^{n/2 - C})).
\]

\begin{theorem}[Bit-runtime information law]
\label{thm:bit-runtime}
The classical communication cost for the adiabatic model to achieve target
runtime $T$ satisfies
\[
C^*(T) = \max\!\left(0,\,\frac{n}{2} - \log_2\!\frac{T}{T_{\mathrm{inf}}}\right) + O(1),
\]
while $C^*_{\mathrm{circuit}}(T) = 0$ for all $T = \Omega(T_{\mathrm{inf}})$.
\end{theorem}

\begin{proof}
From $T(C)=T_{\mathrm{inf}}\cdot\Theta(\max(1,2^{n/2-C}))$, there exist constants
$a,b>0$ with
\[
a\,T_{\mathrm{inf}}\max(1,2^{n/2-C}) \le T(C) \le
b\,T_{\mathrm{inf}}\max(1,2^{n/2-C}).
\]
Inverting these inequalities gives
$C = \max(0,\,n/2-\log_2(T/T_{\mathrm{inf}})) + O(1)$. The circuit statement
follows because D\"urr-H\o yer achieves $T=O(T_{\mathrm{inf}})$ with zero message.
\end{proof}

The complete model comparison, synthesizing this chapter with Chapter~8, is given below.

\begin{center}
\begin{tabular}{llll}
\hline
Model & Info needed & Runtime & Communication \\
\hline
Circuit (D\"urr-H\o yer) & None & $\Theta(\sqrt{N/d_0})$ & 0 bits \\
Fixed AQO, informed & $A_1$ to $2^{-n/2}$ & $O(\sqrt{N/d_0})$ & $\Theta(n)$ bits \\
Fixed AQO, $C$ bits & $A_1$ to $2^{-C}$ & $T_{\mathrm{inf}}\cdot \Theta(\max(1,2^{n/2-C}))$ & $C$ bits \\
Fixed AQO, uninformed & None & $\Omega(2^{n/2}\,T_{\mathrm{inf}})$ & 0 bits \\
Adaptive AQO & $O(n)$ measurements & $O(\sqrt{N/d_0})$ & 0 bits \\
Constant-control, two-level & None & $\Theta(\sqrt{N/d_0})$ & 0 bits \\
Quantum $A_1$ estimation & $\varepsilon = 2^{-n/2}$ & $\Theta(2^{n/2})$ queries & --- \\
Classical $A_1$ estimation & $\varepsilon = 2^{-n/2}$ & $\Theta(2^n)$ queries & --- \\
\hline
\end{tabular}
\end{center}

\noindent The table reveals a clean separation. The circuit model and the adaptive adiabatic model both achieve optimal performance with zero classical communication. The fixed adiabatic model traces a diagonal: each missing bit of $A_1$ doubles the runtime. The $\Theta(n)$-bit gap between the fixed adiabatic model and the circuit model is exactly the information content of $A_1$ at schedule-relevant precision. This gap is a property of the computational model, not of the computational task. The circuit model does not somehow extract $A_1$ at zero cost. It does not need $A_1$ at all, because its mechanism (amplitude amplification) does not traverse the interpolation path where $A_1$ controls the crossing position.

The running example makes the tradeoff concrete. With $N = 4$, $d_0 = 1$, and $n = 2$, the circuit model uses $O(2)$ queries at $C = 0$. The informed adiabatic model also uses $O(2)$ queries, but only after receiving $C = 1$ bit encoding the crossing position. Without that bit, the fixed adiabatic model requires $\Omega(4)$ queries. The factor-of-two slowdown is exactly the cost of one missing bit.

These results organize into a five-level taxonomy of ignorance, measured by the multiplicative overhead $T/T_{\mathrm{inf}}$:

\begin{center}
\begin{tabular}{lll}
\hline
Level & Information available & Overhead $T/T_{\mathrm{inf}}$ \\
\hline
0 & None (uninformed fixed schedule) & $\Omega(2^{n/2})$ \\
1 & $A_1$ to precision $\varepsilon$ & $\Theta(\max(1, \varepsilon/\delta_{A_1}))$ \\
2 & $s^*$ in known interval $[u_L, u_R]$ & $\Theta(\max(1,\,(u_R-u_L)/\delta_s))$ \\
3 & Quantum measurement access & $O(1)$ via $O(n)$ probes \\
4 & Circuit model (no spectral info) & $1$ \\
\hline
\end{tabular}
\end{center}

\noindent Levels~0 through~3 are monotone within the adiabatic framework: each strictly refines the previous one by adding information about the spectrum. Level~4 is qualitatively different because it reflects a change in computational model rather than additional spectral information, so its inclusion in the hierarchy represents a model comparison rather than a refinement.

The adiabatic approach to unstructured search works, reaches Grover scaling, and is optimal within its schedule class. Its information requirements are a structural consequence of the rank-one interpolation path: the crossing position $s^*$ encodes $\Theta(n)$ bits of spectral data, that data is NP-hard to compute classically, and no instance-independent design within the framework can eliminate the dependence. These are not fundamental limits of quantum computation. They are limits of this specific model. The circuit model, which does not traverse an interpolation path, neither needs nor reveals the spectral parameter $A_1$. The adaptive adiabatic model, which does traverse the path but can measure during evolution, acquires the missing information at the Grover scale. The fixed-schedule model, forced to commit before execution, pays for ignorance at a rate of one bit per factor of two in runtime.

The ``information gap'' is therefore three things at once. It is a spectral gap: the minimum of $g(s)$ along the interpolation path, which sets the adiabatic timescale and whose geometry controls the runtime spectrum of Section~\ref{sec:gap-geometry}. It is an epistemic gap: the $\Theta(n)$ bits of $A_1$ that separate the informed schedule from the uninformed one, quantified by the interpolation theorem of Section~\ref{sec:partial-knowledge} and the bit-runtime law of \autoref{thm:bit-runtime}. And it is a model gap: the qualitative separation between computational models that need spectral information and those that do not, made precise by the no-go theorems of Section~\ref{sec:barrier-anatomy} and the $A_1$-blindness of the circuit model in Section~\ref{sec:quantum-bypass}. The next chapter translates these results into machine-checked formal proofs.
