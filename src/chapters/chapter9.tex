% Chapter 9: Information Gap
% ASSUMES: Chapter 5 defines H(s), H_z, H_0, |psi_0>, A_p, A_1, A_2,
%   s^*, delta_s, g_min, hat{g}, eigenvalue equation, spectral condition,
%   symmetric subspace, grover-gap.
% ASSUMES: Chapter 6 proves gap-left, gap-right, complete-profile, f(s*)=4, s_0,
%   c_L = A_1(A_1+1)/A_2, c_R = Delta/30, piecewise linear lower bound.
% ASSUMES: Chapter 7 defines runtime theorem, JRS bound, RC local condition,
%   p=2 schedule, integral T = (C/eps) int g^{-2} ds.
% ASSUMES: Chapter 8 proves NP-hardness (Thm 8.1), #P-hardness (Thm 8.2),
%   interpolation barrier (Thm 8.3), quantum algorithm (Thm 8.4),
%   classical lower bound (Thm 8.5), quadratic separation (Cor 8.1).

The adiabatic algorithm of \autoref{thm:aqo-runtime} achieves the Grover speedup $\widetilde{O}(\sqrt{N/d_0})$, but its schedule depends on $s^* = A_1/(A_1+1)$, whose computation is NP-hard (\autoref{thm:np-hard-A1}). In the circuit model, Grover's algorithm achieves the same speedup without computing any spectral parameter. The adiabatic framework demands the schedule be fixed before evolution begins. What runtime is achievable by an adiabatic algorithm that knows nothing about the problem Hamiltonian beyond its dimension?

The spectral gap $g(s)$ determines the runtime: physics. The gap in knowledge about where the spectral gap reaches its minimum determines what runtime is achievable: information theory. And whether the gap in knowledge matters at all depends on the computational model: complexity theory.


\section{The Cost of Ignorance}
\label{sec:cost-of-ignorance}

Throughout this chapter, asymptotic notation ($O$, $\Omega$, $\Theta$) refers to the limit $N \to \infty$ (equivalently $n \to \infty$ with $N = 2^n$). The spectral parameters $d_0$, $M$, $\Delta$, $A_1$, $A_2$ and the target error $\varepsilon$ are treated as fixed positive constants independent of $n$ unless explicitly stated otherwise. When we write ``$O(T_{\mathrm{inf}})$,'' the implicit constant may depend on these spectral parameters but not on $n$. A \emph{fixed schedule} is determined before the instance is revealed: it depends only on the problem size $n$ and the target error $\varepsilon$, not on spectral properties. An \emph{instance-independent} algorithm uses the same Hamiltonian design for all energy assignments with the same degeneracy structure.

The NP-hardness of $A_1$ is a statement about worst-case classical computation. It does not directly tell us how much runtime an adiabatic algorithm loses by not knowing $A_1$. If a fixed schedule that ignores $A_1$ still achieved $O(\sqrt{N/d_0})$, the hardness would be academic. It is not.

The separation between informed and uninformed schedules is a minimax result: a two-player game where the schedule designer moves first, then an adversary selects the worst-case gap function. To formalize this, we need a class of gap functions broad enough that the adversary can place the gap minimum anywhere in an uncertainty interval $[s_L, s_R]$.

\begin{definition}[Gap class]
\label{def:gap-class}
The gap class $\mathcal{G}(s_L, s_R, \Delta_*)$ consists of all gap functions $g: [0,1] \to \mathbb{R}_{>0}$ satisfying: the minimum $g(s^*) = \Delta_*$ is achieved at a unique point $s^* \in [s_L, s_R]$, and $g(s) > \Delta_*$ for all $s \neq s^*$.
\end{definition}

\noindent For the running example ($M = 2$, $d_0 = 1$, $N = 4$), $s^* = A_1/(A_1+1) = 3/7$ and $\Delta_* = g_{\min} = 1/\sqrt{4} = 1/2$. Any gap function in $\mathcal{G}(0, 1, 1/2)$ has its minimum somewhere in $[0, 1]$ at value $1/2$; the adversary's freedom is in choosing where.

\noindent The parameter $\Delta_*$ denotes the minimum of the abstract gap function $g$; it should not be confused with $\Delta = E_1 - E_0$ (the spectral gap of $H_z$) or with $g_{\min}$ (the minimum gap of the rank-one Hamiltonian $H(s)$). For the rank-one gap profile, $\Delta_* = g_{\min} = \Theta(\sqrt{d_0/(NA_2)})$.

The schedule induces a velocity profile $v(s) > 0$ on $[0,1]$, with total evolution time $T = \int_0^1 v(s)^{-1}\,ds$.

The velocity bound at the crossing follows from the Roland-Cerf local adiabatic condition (Chapter~7): the schedule must satisfy $v(s) = |ds/dt| \leq \varepsilon\,g(s)^2/\lVert H'(s) \rVert$ at each $s$ to keep the transition probability below $\varepsilon$. Since $H'(s) = \ket{\psi_0}\!\bra{\psi_0} + H_z$ satisfies $\lVert H'(s) \rVert \leq \lVert \ket{\psi_0}\!\bra{\psi_0} \rVert + \lVert H_z \rVert \leq 1 + 1 = 2$ (using the eigenvalue normalization $E_{M-1} \leq 1$ from Chapter~5), the velocity is bounded by $v(s) \leq \varepsilon\,g(s)^2/2$. At the crossing where $g = \Delta_*$, this gives $v \leq \varepsilon\,\Delta_*^2/2$.

The crossing window has width $\delta_s = \Theta(\Delta_*)$, so a schedule must be slow throughout this window. To see this, recall from Chapter~5 that $\delta_s = \hat{g}/c_L$ (\autoref{eq:gmin-deltas-relation}), where $\hat{g} = \frac{2A_1}{A_1+1}\sqrt{d_0/(NA_2)}$ is the leading-order minimum gap satisfying $g_{\min} = (1 \pm O(\eta))\hat{g}$; since $c_L = A_1(A_1+1)/A_2$ is a fixed constant, $\delta_s = \Theta(g_{\min}) = \Theta(\Delta_*)$. We define $v_{\mathrm{slow}} = \varepsilon\,\Delta_*^2/2$ as the maximum crossing velocity. In the ratio $T_{\mathrm{unf}}/T_{\mathrm{inf}}$, this velocity cancels: both runtimes are computed under the same Roland-Cerf condition, so $v_{\mathrm{slow}}$ appears in both denominators, and the separation depends only on the geometric ratio $(s_R - s_L)/\Delta_*$.

\begin{lemma}[Adversarial gap construction]
\label{lem:adversarial-gap}
For any $s_{\mathrm{adv}} \in [s_L, s_R]$ and $\Delta_* > 0$, the gap function $g_{\mathrm{adv}}(s) = \Delta_* + (s - s_{\mathrm{adv}})^2$ belongs to $\mathcal{G}(s_L, s_R, \Delta_*)$.
\end{lemma}

\begin{proof}
The function satisfies $g_{\mathrm{adv}}(s_{\mathrm{adv}}) = \Delta_*$, $g_{\mathrm{adv}}(s) > \Delta_*$ for $s \neq s_{\mathrm{adv}}$, and $g_{\mathrm{adv}}(s) > 0$ for all $s$.
\end{proof}

\begin{lemma}[Velocity bound for uninformed schedules]
\label{lem:velocity-bound}
Let $u$ be a fixed schedule achieving error $\leq \varepsilon$ for all $g \in \mathcal{G}(s_L, s_R, \Delta_*)$. Then $v(s) \leq v_{\mathrm{slow}}$ for all $s \in [s_L, s_R]$, provided $N$ is sufficiently large that $\Delta_* < \min(1 - s_R, s_L)$.
\end{lemma}

\begin{proof}
Suppose $v(s') > v_{\mathrm{slow}}$ for some $s' \in [s_L, s_R]$. We construct a physical Hamiltonian in the rank-one family whose gap minimum occurs at $s'$, then show that the schedule violates the adiabatic condition on this instance.

Since $s^* = A_1/(A_1+1)$ is a continuous, strictly increasing function of $A_1 \in (0,\infty)$ with range $(0,1)$, there exists $A_1$ placing the crossing at $s'$. On the two-level family with solution fraction $\rho = d_0/N$, the leading-order gap minimum is $\hat{g} = 2(1-s')\sqrt{\rho(1-\rho)}$ (\autoref{eq:gmin-formula}), so choosing $\rho$ to satisfy $\hat{g} = \Delta_*$ (feasible whenever $\Delta_* \leq 1 - s'$, which holds asymptotically since $\Delta_* = \Theta(2^{-n/2})$ and $s' \leq s_R < 1$) produces a problem Hamiltonian $H_z$ with crossing at $s'$ and $g_{\min} = \Theta(\Delta_*)$. The adversary in the minimax game selects this physical Hamiltonian, not merely an abstract gap function.

The Roland-Cerf condition (\autoref{eq:roland-cerf-runtime}) on this Hamiltonian requires $v(s') \leq \varepsilon\,g_{\min}^2/\lVert H'(s') \rVert = \Theta(v_{\mathrm{slow}})$. Since $v(s') > v_{\mathrm{slow}}$ by assumption, the transition probability exceeds $\varepsilon$ for this instance (after absorbing the constant factor into $v_{\mathrm{slow}}$, which is defined only up to $\Theta$). Because the schedule is fixed before the instance is revealed, it must satisfy $v(s) \leq v_{\mathrm{slow}}$ at every $s \in [s_L, s_R]$.
\end{proof}

\begin{theorem}[Separation]
\label{thm:separation}
Let $T_{\mathrm{unf}}$ be the minimum time for any fixed schedule achieving error $\leq \varepsilon$ for all rank-one instances whose gap minima satisfy $s^* \in [s_L,s_R]$ and $g_{\min} = \Theta(\Delta_*)$, and let $T_{\mathrm{inf}}$ be the corresponding optimal informed runtime (with known $s^*$) on the same rank-one family. Then
\begin{equation}
\label{eq:separation-ratio}
\frac{T_{\mathrm{unf}}}{T_{\mathrm{inf}}} = \Omega\!\left(\frac{s_R - s_L}{\Delta_*}\right).
\end{equation}
\end{theorem}

\begin{proof}
By \autoref{lem:velocity-bound}, $v(s) \leq v_{\mathrm{slow}}$ for all $s \in [s_L, s_R]$. The uninformed time satisfies
\begin{equation}
T_{\mathrm{unf}} = \int_0^1 \frac{ds}{v(s)} \geq \int_{s_L}^{s_R} \frac{ds}{v(s)} \geq \frac{s_R - s_L}{v_{\mathrm{slow}}}.
\end{equation}
The informed schedule knows $s^*$ exactly and needs to be slow only in the crossing window of width $O(\Delta_*)$. For rank-one profiles, $\delta_s = \hat{g}/c_L = \Theta(g_{\min}) = \Theta(\Delta_*)$ by \autoref{eq:gmin-deltas-relation}, so \autoref{thm:aqo-runtime} gives $T_{\mathrm{inf}} = \Theta(\delta_s/v_{\mathrm{slow}}) = \Theta(\Delta_*/v_{\mathrm{slow}})$. The velocity factors cancel:
\begin{equation}
\frac{T_{\mathrm{unf}}}{T_{\mathrm{inf}}} = \Omega\!\left(\frac{s_R - s_L}{\Delta_*}\right). \qedhere
\end{equation}
\end{proof}

\begin{corollary}[Unstructured search]
\label{cor:separation-grover}
For $n$-qubit unstructured search with $s_L, s_R$ bounded away from $0$ and $1$ (so that the condition of \autoref{lem:velocity-bound} holds for large $N$), $\Delta_* = \Theta(2^{-n/2})$ and $s_R - s_L = \Theta(1)$, giving $T_{\mathrm{unf}}/T_{\mathrm{inf}} = \Omega(2^{n/2})$.
\end{corollary}

For the running example ($M = 2$, $d_0 = 1$, $N = 4$), the separation ratio is $(s_R - s_L)/\Delta_* = 1/(1/2) = 2 = \sqrt{N}$. Uninformed adiabatic evolution on this four-element instance takes twice as long as informed evolution, a discrepancy that grows exponentially with $n$.

The logical structure is: NP-hardness forces the gap-uninformed model for any fixed schedule with polynomial-time classical preprocessing; the gap-uninformed model has the $\Omega(2^{n/2})$ minimax lower bound from the adversarial geometry of \autoref{lem:adversarial-gap}; therefore this class of algorithms pays the penalty. The penalty comes from the geometry, not from computational complexity directly.


\section{Partial Knowledge and Hedging}
\label{sec:partial-knowledge}

The separation theorem quantifies the worst case: an adversary who places the gap minimum anywhere in $[s_L, s_R]$ forces the schedule to be uniformly slow. But NP-hardness does not mean $A_1$ is completely unknown. What is the value of partial knowledge?

Suppose an algorithm has access to an estimate $A_{1,\mathrm{est}}$ satisfying $|A_{1,\mathrm{est}} - A_1| \leq \varepsilon$. The uncertainty propagates to the crossing position through the map $f(x) = x/(x+1)$, whose derivative is $f'(x) = 1/(x+1)^2$.

\begin{lemma}[$A_1$-to-$s^*$ precision propagation]
\label{lem:precision-propagation}
If $|A_{1,\mathrm{est}} - A_1| \leq \varepsilon$ with $|\varepsilon| \leq (1+A_1)/2$, then $|s^*_{\mathrm{est}} - s^*| \leq 2|\varepsilon|/(A_1+1)^2$.
\end{lemma}

\begin{proof}
Direct computation gives the exact identity
\begin{equation}
\label{eq:precision-exact}
s^*_{\mathrm{est}} - s^* = \frac{A_1 + \varepsilon}{1 + A_1 + \varepsilon} - \frac{A_1}{1 + A_1} = \frac{\varepsilon}{(1+A_1)(1+A_1+\varepsilon)}.
\end{equation}
Under $|\varepsilon| \leq (1+A_1)/2$, the denominator satisfies $1 + A_1 + \varepsilon \geq (1+A_1)/2$, so
\begin{equation}
|s^*_{\mathrm{est}} - s^*| \leq \frac{|\varepsilon|}{(1+A_1) \cdot (1+A_1)/2} = \frac{2|\varepsilon|}{(1+A_1)^2}. \qedhere
\end{equation}
\end{proof}

Given $A_1$ precision $\varepsilon$, the true crossing position lies within radius $2\varepsilon/(A_1+1)^2$ of the estimate by \autoref{lem:precision-propagation}, giving an uncertainty interval of width $W(\varepsilon) = 4\varepsilon/(A_1+1)^2$. The $\varepsilon$-informed gap class is $\mathcal{G}_\varepsilon = \mathcal{G}(s_L(\varepsilon), s_R(\varepsilon), \Delta_*)$, where the endpoints are determined by the estimate and precision. Applying \autoref{thm:separation} to $\mathcal{G}_\varepsilon$ with interval width $W(\varepsilon)$ gives a lower bound; the matching upper bound comes from a schedule that is uniformly slow across the uncertainty interval and fast elsewhere.

\begin{theorem}[Interpolation]
\label{thm:interpolation}
For $A_1$ precision $\varepsilon$, the optimal adiabatic runtime satisfies
\begin{equation}
\label{eq:interpolation}
T(\varepsilon) = \Theta\!\left(T_{\mathrm{inf}} \cdot \max\!\left(1, \frac{\varepsilon}{\delta_{A_1}}\right)\right),
\end{equation}
where $\delta_{A_1} = 2\sqrt{d_0 A_2/N}$ is the precision threshold for optimality.
\end{theorem}

\begin{proof}
\emph{Lower bound.} For $\varepsilon \geq \delta_{A_1}$, \autoref{thm:separation} applied to $\mathcal{G}_\varepsilon$ gives $T(\varepsilon) \geq W(\varepsilon)/v_{\mathrm{slow}}$. Taking the ratio with $T_{\mathrm{inf}} = \Theta(\delta_s/v_{\mathrm{slow}})$ and using the identity
\begin{equation}
\label{eq:precision-identity}
(A_1+1)^2 \cdot \delta_s = (A_1+1)^2 \cdot \frac{2}{(A_1+1)^2}\sqrt{\frac{d_0 A_2}{N}} = 2\sqrt{\frac{d_0 A_2}{N}} = \delta_{A_1}
\end{equation}
yields $T(\varepsilon)/T_{\mathrm{inf}} \geq \Theta(\varepsilon/\delta_{A_1})$. For $\varepsilon < \delta_{A_1}$, the trivial bound $T(\varepsilon) \geq T_{\mathrm{inf}}$ holds regardless of precision.

\emph{Upper bound.} For $\varepsilon \geq \delta_{A_1}$, construct a schedule with crossing velocity $v_{\mathrm{slow}} = \Theta(\varepsilon\,\Delta_*^2)$ throughout the uncertainty interval $[s_L(\varepsilon), s_R(\varepsilon)]$ and fast velocity $v_{\mathrm{fast}} = O(1)$ outside. The slow region has width $W(\varepsilon) = \Theta(\varepsilon/\delta_{A_1}) \cdot \delta_s$, so the total time is $T = W(\varepsilon)/v_{\mathrm{slow}} + O(1) = \Theta(T_{\mathrm{inf}} \cdot \varepsilon/\delta_{A_1})$, since $T_{\mathrm{inf}} = \Theta(\delta_s/v_{\mathrm{slow}})$. For $\varepsilon < \delta_{A_1}$, the optimal informed schedule achieves $T = O(T_{\mathrm{inf}})$.
\end{proof}

The interpolation is linear: no threshold, no cliff, no phase transition. At precision $1/\mathrm{poly}(n)$ (NP-hard), the overhead is $\Theta(2^{n/2}/\mathrm{poly}(n))$, nearly the full exponential. At precision $2^{-n/2}$ (algorithmically relevant), the overhead is $\Theta(1)$. The space between these two precision scales is the ``information gap.'' For the running example, the explicit precision table is:

\begin{center}
\begin{tabular}{ll}
\hline
Precision $\varepsilon$ & $T(\varepsilon)/T_{\mathrm{inf}}$ \\
\hline
$2^{-n/2}$ & $\Theta(1)$ \\
$2^{-n/4}$ & $\Theta(2^{n/4})$ \\
$1/n$ & $\Theta(2^{n/2}/n)$ \\
$1/\mathrm{poly}(n)$ & $\Theta(2^{n/2}/\mathrm{poly}(n))$ \\
$1$ (no knowledge) & $\Theta(2^{n/2})$ \\
\hline
\end{tabular}
\end{center}

The interpolation theorem treats $A_1$ precision as a continuous resource. A complementary question is operational: given that $s^*$ lies in a known interval $[u_L, u_R]$ but the exact position is unknown, what is the best fixed schedule? A hedging schedule distributes its slowdown across the entire uncertainty interval rather than concentrating at a single point: velocity $v_{\mathrm{slow}}$ for $s \in [u_L, u_R]$ and $v_{\mathrm{fast}}$ outside, subject to the normalization $(u_R - u_L)/v_{\mathrm{slow}} + (1 - u_R + u_L)/v_{\mathrm{fast}} = 1$.

Write $w = u_R - u_L$ for the interval width. The JRS error functional integrates $\lVert H' \rVert^2/g^3$; since $\lVert H'(s) \rVert = O(1)$ for the rank-one family, the effective integrand is $g^{-3}$, in contrast to the Roland-Cerf condition which integrates $g^{-2}$. For a piecewise-constant velocity profile, the JRS error bound (\autoref{eq:jrs-bound}) gives a transition probability proportional to $v \cdot \int g^{-3}\,ds$ on each segment. The total error is $v_{\mathrm{slow}} I_{\mathrm{slow}} + v_{\mathrm{fast}} I_{\mathrm{fast}}$, where $I_{\mathrm{slow}} = \int_{u_L}^{u_R} g(u)^{-3}\,du$ and $I_{\mathrm{fast}} = \int_{[0,1]\setminus[u_L,u_R]} g(u)^{-3}\,du$. Since the crossing lies within the slow region, $I_{\mathrm{slow}} \gg I_{\mathrm{fast}}$.

\begin{theorem}[Hedging]
\label{thm:hedging}
Let $R = I_{\mathrm{slow}}/I_{\mathrm{fast}} \gg 1$. Under normalization $T = 1$, the optimal hedging schedule for interval $[u_L, u_R]$ achieves $\mathrm{Error}_{\mathrm{hedge}}/\mathrm{Error}_{\mathrm{uniform}} \to u_R - u_L$ as $R \to \infty$, with optimal slow velocity $v_{\mathrm{slow}} = w + \sqrt{(1-w)w/R}$.
\end{theorem}

\begin{proof}
Write $w = u_R - u_L$ for the interval width. The normalization constraint $w/v_{\mathrm{slow}} + (1-w)/v_{\mathrm{fast}} = 1$ fixes the total time $T = 1$, so the JRS error integral of \autoref{eq:jrs-bound} reduces to $E = v_{\mathrm{slow}} I_{\mathrm{slow}} + v_{\mathrm{fast}} I_{\mathrm{fast}}$. The constraint gives
\begin{equation}
v_{\mathrm{fast}} = \frac{(1-w)\,v_{\mathrm{slow}}}{v_{\mathrm{slow}} - w},
\end{equation}
valid for $v_{\mathrm{slow}} > w$. Substituting into the error:
\begin{equation}
E(v_{\mathrm{slow}}) = v_{\mathrm{slow}}\, I_{\mathrm{slow}} + \frac{(1-w)\,v_{\mathrm{slow}}}{v_{\mathrm{slow}} - w}\, I_{\mathrm{fast}}.
\end{equation}
Differentiating with respect to $v_{\mathrm{slow}}$ and setting to zero:
\begin{equation}
\frac{dE}{dv_{\mathrm{slow}}} = I_{\mathrm{slow}} - \frac{(1-w)\,w\, I_{\mathrm{fast}}}{(v_{\mathrm{slow}} - w)^2} = 0.
\end{equation}
Solving: $(v_{\mathrm{slow}} - w)^2 = (1-w)\,w\,I_{\mathrm{fast}}/I_{\mathrm{slow}} = (1-w)\,w/R$, so
\begin{equation}
v_{\mathrm{slow}} = w + \sqrt{(1-w)\,w/R}.
\end{equation}
At this optimum, $v_{\mathrm{fast}} = (1-w)\,v_{\mathrm{slow}}/\sqrt{(1-w)\,w/R} = \sqrt{R\,w\,(1-w)} + (1-w)$. The optimal error, substituting $v_{\mathrm{slow}} - w = \sqrt{(1-w)\,w/R}$, is
\begin{equation}
E_{\mathrm{opt}} = \bigl(w + \sqrt{(1-w)\,w/R}\bigr)\,I_{\mathrm{slow}} + \bigl(\sqrt{R\,w\,(1-w)} + (1-w)\bigr)\,I_{\mathrm{fast}}.
\end{equation}
Since $R = I_{\mathrm{slow}}/I_{\mathrm{fast}} \gg 1$, the terms involving $\sqrt{R}$ contribute $2\sqrt{w\,(1-w)\,I_{\mathrm{slow}}\,I_{\mathrm{fast}}} = o(I_{\mathrm{slow}})$, and the dominant term is $w\,I_{\mathrm{slow}}$, while the uniform error is $E_{\mathrm{unif}} = I_{\mathrm{slow}} + I_{\mathrm{fast}} \approx I_{\mathrm{slow}}$. Therefore $E_{\mathrm{opt}}/E_{\mathrm{unif}} \to w = u_R - u_L$ as $R \to \infty$.
\end{proof}

For an uncertainty interval $[0.4, 0.8]$, the hedging schedule achieves error $E_{\mathrm{opt}}/E_{\mathrm{unif}} = 0.4$ compared to a uniform schedule at the same total runtime, a 60\% reduction in transition probability. Bounded uncertainty about $s^*$ yields a constant-factor improvement proportional to the interval width, not an exponential overhead. The hedging schedule corresponds to Level~2 of the ignorance taxonomy developed in \autoref{sec:complexity-landscape}.


\section{Quantum Bypass}
\label{sec:quantum-bypass}

The separation theorem and the interpolation theorem characterize the cost of ignorance within the fixed-schedule model. An adiabatic device, however, is a physical system that can be measured during execution. The original paper~\cite{braida2024unstructured} posed the question: ``Can this limitation be overcome when one only has access to a device operating in the adiabatic setting?''

The answer is yes. Han, Park, and Choi~\cite{HanParkChoi2025} independently proposed a constant geometric speed (CGS) schedule that traverses the eigenstate path at uniform arc length, using eigenstate overlaps computed on the fly via the quantum Zeno Monte Carlo method to adaptively adjust the parameter velocity. Their approach improves the gap scaling from $O(\Delta_*^{-2})$ to $O(\Delta_*^{-1})$ and demonstrates numerically that the quadratic speedup persists for adiabatic unstructured search without prior spectral knowledge. The binary-search protocol below differs in mechanism --- it uses phase estimation to actively locate the crossing before executing the informed schedule --- but achieves the same asymptotic scaling $O(T_{\mathrm{inf}})$ with a rigorous optimality proof. NP-hardness conflates two distinct tasks: \emph{computing} $s^*$ from the classical description of $H_z$ is hard, but \emph{detecting} $s^*$ by probing the quantum system $H(s)$ at selected parameter values is efficient. The mechanism is phase estimation: the ground and first excited energies of $H(s)$ differ by $g(s)$, and the initial state $\ket{\psi_0}$ transitions from ground-state-like to excited-state-like as $s$ crosses $s^*$. A binary search with phase estimation at each midpoint locates the crossing.

The protocol requires two ingredients: the overlap structure of $\ket{\psi_0}$ with the instantaneous eigenstates of $H(s)$, and the cost of phase estimation at each probe point.

The state $\ket{\psi_0}$ is the exact ground state of $H(0) = -\ket{\psi_0}\!\bra{\psi_0}$. As $s$ increases from $0$ to $1$, the ground state $\ket{E_0(s)}$ of $H(s)$ evolves continuously within the two-dimensional symmetric subspace of Chapter~5. The effective two-level Hamiltonian has diagonal elements that cross near $s^*$ and off-diagonal coupling $|V(s)| = (1-s)\sqrt{d_0(N-d_0)}/N = \Theta(\sqrt{d_0/N})$. The overlap $|\!\braket{\psi_0}{E_0(s)}\!|^2$ is governed by the mixing angle $\theta(s)$ satisfying $\sin 2\theta(s) = 2|V(s)|/g(s)$, with $|\!\braket{\psi_0}{E_0(s)}\!|^2 = \cos^2\theta(s)$.

For $s < s^*$ with $s^* - s \gg \delta_s$, the gap $g(s) \geq c_L(s^* - s)$ exceeds the coupling, so $\theta(s) = O(\sqrt{d_0/N}/(c_L(s^* - s))) = O(\delta_s/(s^* - s)) \ll 1$ and the overlap is $1 - O(\delta_s^2/(s^* - s)^2)$: close to $1$ everywhere except near the crossing window. At the crossing $s \approx s^*$, the diagonal elements are nearly degenerate, $\theta \approx \pi/4$, and the overlap is approximately $1/2$. For $s > s^*$ with $s - s^* \gg \delta_s$, the ground state has swapped character: $\ket{\psi_0}$ projects primarily onto the excited branch, and the overlap drops to $O(\delta_s^2/(s - s^*)^2)$. At $s = 1$, this gives $|\!\braket{\psi_0}{E_0(1)}\!|^2 = O(\delta_s^2) = O(d_0/N)$. This transition is what phase estimation detects.

\begin{definition}[Adaptive adiabatic protocol]
\label{def:adaptive-protocol}
The protocol operates in two phases.

\emph{Phase 1 (Location).} Initialize $s_{\mathrm{lo}} = 0$, $s_{\mathrm{hi}} = 1$. For $i = 1, \ldots, \lceil n/2 \rceil$:
\begin{enumerate}
\item Prepare the state $\ket{\psi_0} = \ket{+}^{\otimes n}$.
\item Set $s_{\mathrm{mid}} = (s_{\mathrm{lo}} + s_{\mathrm{hi}})/2$.
\item Apply phase estimation of the Hamiltonian $H(s_{\mathrm{mid}}) = -(1-s_{\mathrm{mid}})\ket{\psi_0}\!\bra{\psi_0} + s_{\mathrm{mid}} H_z$ to the state $\ket{\psi_0}$. This requires simulating the unitary $e^{-iH(s_{\mathrm{mid}})t}$ for time $t = O(1/g(s_{\mathrm{mid}}))$.
\item If the measured energy corresponds to the ground state of $H(s_{\mathrm{mid}})$: the crossing has not yet occurred, so set $s_{\mathrm{lo}} = s_{\mathrm{mid}}$.
\item If the measured energy corresponds to an excited state: the crossing has already occurred, so set $s_{\mathrm{hi}} = s_{\mathrm{mid}}$.
\end{enumerate}
After $\lceil n/2 \rceil$ iterations, $s^*$ is located to precision $O(2^{-n/2})$.

\emph{Phase 2 (Execution).} Reset the state to $\ket{\psi_0}$. Evolve from $s = 0$ to $s = 1$ using the informed local schedule of \autoref{thm:aqo-runtime}, with the crossing position estimated in Phase~1.
\end{definition}

Phase estimation of $H(s_{\mathrm{mid}})$ projects $\ket{\psi_0}$ onto an eigenstate of $H(s_{\mathrm{mid}})$ and returns the corresponding energy. The Hamiltonian $H(s_{\mathrm{mid}})$ is a sum of two terms: the rank-one projector $-(1-s_{\mathrm{mid}})\ket{\psi_0}\!\bra{\psi_0}$ (implementable via a single-qubit rotation in the $\ket{\psi_0}/\ket{\psi_0^\perp}$ basis) and the diagonal operator $s_{\mathrm{mid}} H_z$ (implementable via the problem oracle). Their sum can be simulated via product formulas with $\mathrm{poly}(n)$ gate overhead per unit time. Efficient eigenvalue estimation from initial-state overlap is studied by Poulin and Wocjan~\cite{poulin2009preparing} and Ge, Tura, and Cirac~\cite{ge2019faster}; the binary search protocol above requires only constant-precision energy discrimination, which is simpler than the full eigenvalue estimation problem.

For $s_{\mathrm{mid}} < s^*$, the state $\ket{\psi_0}$ has $\Theta(1)$ overlap with $\ket{E_0(s_{\mathrm{mid}})}$, so phase estimation returns the ground energy with constant probability. For $s_{\mathrm{mid}} > s^*$ with $|s_{\mathrm{mid}} - s^*| \gg \delta_s$, the overlap $|\!\braket{\psi_0}{E_0(s_{\mathrm{mid}})}\!|^2 = O(d_0/N)$, so phase estimation returns an excited energy with probability $1 - O(d_0/N) = 1 - o(1)$. The binary search tolerates $O(1)$ errors per level, so the constant success probability suffices.

\begin{lemma}[Phase estimation cost]
\label{lem:phase-estimation-cost}
Distinguishing the ground state from the first excited state of $H(s_{\mathrm{mid}})$ via phase estimation requires time $O(1/g(s_{\mathrm{mid}}))$.
\end{lemma}

\begin{proof}
Phase estimation resolves energies separated by $\delta E$ using evolution under $e^{-iH(s_{\mathrm{mid}})t}$ for time $t = O(1/\delta E)$. The two lowest energies of $H(s_{\mathrm{mid}})$ differ by $g(s_{\mathrm{mid}})$, so $t = O(1/g(s_{\mathrm{mid}}))$.
\end{proof}

\begin{lemma}[Phase 1 cost]
\label{lem:phase1-cost}
The total time for Phase 1 is $O(T_{\mathrm{inf}})$.
\end{lemma}

\begin{proof}
Let $d_i = |s_{\mathrm{mid},i} - s^*|$ be the distance from the $i$-th midpoint to the true crossing. From the piecewise gap profile established in Chapter~6: outside the crossing window ($|s - s^*| > \delta_s$), the gap satisfies $g(s) \geq c_{\min} |s - s^*|$ where $c_{\min} = \min(c_L, c_R)$ with $c_L = A_1(A_1+1)/A_2$ and $c_R = \Delta/30$ (both positive constants independent of $n$); inside the crossing window ($|s - s^*| \leq \delta_s$), the gap satisfies $g(s) \geq g_{\min}$. Since $g_{\min}$ is the global minimum, both cases combine to
\begin{equation}
\label{eq:gap-lower-bound}
g(s_{\mathrm{mid},i}) \geq \max(g_{\min},\; c_{\min} \cdot d_i).
\end{equation}
The phase estimation cost at iteration $i$ is therefore
\begin{equation}
\label{eq:pe-cost}
O\!\left(\frac{1}{g(s_{\mathrm{mid},i})}\right) \leq O\!\left(\min\!\left(\frac{1}{g_{\min}},\; \frac{1}{c_{\min} \cdot d_i}\right)\right).
\end{equation}
The two bounds in~\eqref{eq:pe-cost} cross at $d_i = g_{\min}/c_{\min}$. Since $g_{\min} = c_L \delta_s \cdot (1 - O(\eta))$ and $c_{\min} \leq c_L$, the crossover distance satisfies $d_{\mathrm{cross}} = g_{\min}/c_{\min} = (c_L/c_{\min})\,\delta_s \geq \delta_s$. The ratio $c_L/c_{\min}$ is a positive constant independent of $n$: both $c_L = A_1(A_1+1)/A_2$ and $c_R = \Delta/30$ are determined by the spectral parameters, so $c_{\min} = \min(c_L, c_R) > 0$ is a fixed constant. At most $O(\log(c_L/c_{\min}) + 1) = O(1)$ binary search midpoints fall in the near regime $d_i \leq d_{\mathrm{cross}}$.

Group the $\lceil n/2 \rceil$ iterations by the distance $d_i$ in dyadic shells $S_j = [2^{-j-1}, 2^{-j}]$. Let $L_i = 2^{-i+1}$ be the binary-search interval width at step $i$. Since the next interval is the half containing $s^*$, the midpoint distances obey
\[
d_{i+1} = \left|d_i - \frac{L_i}{4}\right| = \left|d_i - 2^{-i-1}\right|.
\]
This recurrence gives $O(1)$ occupancy per shell: for $i > j+1$, $d_i \leq L_i/2 = 2^{-i} < 2^{-j-1}$, so $d_i \notin S_j$; and for $i \leq j$, if $d_i \in (2^{-j-1}, 2^{-j})$, then $d_{i+1} \notin (2^{-j-1}, 2^{-j})$. Thus the shell interior is visited at most once, with only a dyadic-rational edge case where the boundary value $2^{-j-1}$ can appear twice consecutively. Hence each shell contributes $O(1)$ midpoint queries.

\emph{Far shells} ($j < \log_2(1/\delta_s) \approx n/2$): here $d_i > \delta_s$, so the binding bound in~\eqref{eq:pe-cost} is $O(1/(c_{\min} \cdot d_i)) = O(2^j/c_{\min})$, where $c_{\min}$ enters the implicit constant.

\emph{Near shells} ($j \geq n/2$): here $d_i \leq \delta_s$, so the binding bound is $O(1/g_{\min}) = O(1/\Delta_*) = O(2^{n/2})$.

There are $O(1)$ near shells (at most $O(1)$ midpoints can have $d_i \leq d_{\mathrm{cross}}$ in a binary search). The total cost is:
\begin{equation}
\sum_{j=0}^{n/2-1} O(1) \cdot O(2^j) + O(1) \cdot O(2^{n/2}) = O(2^{n/2}) + O(2^{n/2}) = O(2^{n/2}) = O(T_{\mathrm{inf}}).
\end{equation}
The state preparation cost is $O(n)$ per iteration and $O(n)$ iterations, giving $O(n^2) = o(T_{\mathrm{inf}})$.
\end{proof}

\begin{theorem}[Adaptive adiabatic optimality]
\label{thm:adaptive}
The adaptive protocol of \autoref{def:adaptive-protocol} achieves runtime $T_{\mathrm{adapt}} = O(T_{\mathrm{inf}})$ with $\Theta(n)$ measurements.
\end{theorem}

\begin{proof}
Phase~1 locates $s^*$ to precision $O(2^{-n/2}) = O(\delta_s)$ using total time $O(T_{\mathrm{inf}})$ by \autoref{lem:phase1-cost}. This precision is within the crossing window width $\delta_s = O(\Delta_*)$. Phase~2 has time $O(T_{\mathrm{inf}})$ by \autoref{thm:aqo-runtime}, since the estimate of $s^*$ is accurate to $O(\delta_s)$. The total is $O(T_{\mathrm{inf}}) + O(T_{\mathrm{inf}}) = O(T_{\mathrm{inf}})$.
\end{proof}

\begin{theorem}[Measurement lower bound]
\label{thm:measurement-lower}
Any adaptive algorithm in the binary decision-probe model (each measurement returns one bit indicating the ground/excited branch at the probe point) achieving $T = O(T_{\mathrm{inf}})$ requires $\Omega(n)$ measurements.
\end{theorem}

\begin{proof}
The crossing position $s^*$ can lie anywhere in an interval of width $\Theta(1)$. To achieve the informed runtime, the algorithm must locate $s^*$ to precision $\delta_s = O(2^{-n/2})$, since any larger uncertainty incurs the overhead of \autoref{thm:interpolation}. This means distinguishing among $\Omega(2^{n/2})$ possible positions. In the binary decision-probe model, each measurement contributes at most one bit by definition. The information needed is $\log_2(2^{n/2}) = n/2$ bits, requiring $\Omega(n)$ measurements.
\end{proof}

The three adiabatic regimes:

\begin{center}
\begin{tabular}{lll}
\hline
Strategy & Runtime & Measurements \\
\hline
Fixed, uninformed & $\Omega(2^{n/2} \cdot T_{\mathrm{inf}})$ & 0 \\
Adaptive & $O(T_{\mathrm{inf}})$ & $\Theta(n)$ \\
Fixed, informed & $O(T_{\mathrm{inf}})$ & 0 \\
\hline
\end{tabular}
\end{center}

\noindent For the running example ($N = 4$, $d_0 = 1$, $n = 2$): Phase~1 performs $\lceil 1 \rceil = 1$ iteration, probing $s_{\mathrm{mid}} = 0.5$. The true crossing is at $s^* = 3/7 \approx 0.429$, and $s_{\mathrm{mid}}$ is past the crossing but within the crossing window ($|s_{\mathrm{mid}} - s^*| = |1/2 - 3/7| = |7/14 - 6/14| = 1/14 \ll \delta_s \approx 1/4$). To compute the exact overlap, restrict to the symmetric subspace spanned by $\ket{G} = \ket{z_0}$ and $\ket{B} = (1/\sqrt{3})\sum_{z \neq z_0}\ket{z}$, where $z_0$ is the marked item. At $s = 0.5$, the effective $2 \times 2$ Hamiltonian has diagonal elements $-1/8$ and $1/8$ and off-diagonal coupling $-\sqrt{3}/8$. The ground eigenvector is $(\sqrt{3}/2)\ket{G} + (1/2)\ket{B}$, and $\ket{\psi_0} = (1/2)\ket{G} + (\sqrt{3}/2)\ket{B}$. The overlap is $|(\sqrt{3}/2)(1/2) + (1/2)(\sqrt{3}/2)|^2 = |\sqrt{3}/2|^2 = 3/4$. The exact overlap is therefore $|\!\braket{\psi_0}{E_0(0.5)}\!|^2 = 3/4$, so phase estimation is probabilistic rather than decisive at this scale: it reports the ground energy with probability $3/4$ and an excited energy with probability $1/4$.

The small instance $n = 2$ illustrates the protocol's cost structure but not its asymptotic sharpness --- the overlap transition becomes increasingly sharp as $N$ grows and $\delta_s \to 0$, making each binary search step reliable with $\Theta(1)$ probability whenever $|s_{\mathrm{mid}} - s^*| \gg \delta_s$. The gap at $s_{\mathrm{mid}} = 0.5$ is $g(0.5) = 1/\sqrt{N} = 0.5$, so the phase estimation cost is $O(1/g(0.5)) = O(2) = O(T_{\mathrm{inf}})$.

The adaptive protocol acquires $A_1$ through measurement; the circuit model bypasses $A_1$ entirely. The D\"urr-H\o yer quantum minimum-finding algorithm~\cite{durr1996quantum} achieves $\Theta(\sqrt{N/d_0})$ by maintaining a threshold and iteratively lowering it using Grover search, never traversing an adiabatic path and never encountering an avoided crossing. The mechanism is amplitude amplification with iterative thresholding, which uses no spectral parameter --- no $A_1$, $s^*$, $\Delta$, or gap profile.

\begin{proposition}[$A_1$-blindness]
\label{prop:A1-blindness}
Let $X_{\mathrm{DH}}$ denote the output of the amplified D\"urr-H\o yer algorithm (with $r = \Theta(n)$ repetitions). Then $I(X_{\mathrm{DH}};\,A_1 \mid S_0, E_0) \leq 2^{-\Omega(n)}$. Conditioned on success ($X_{\mathrm{DH}} \in S_0$), the mutual information is exactly zero.
\end{proposition}

\begin{proof}
Two problem Hamiltonians $H_z, H_z'$ are ground-equivalent if they share the same ground energy $E_0$ and ground space $S_0$. By symmetry of Grover's algorithm applied to the uniform initial state, the output distribution conditioned on success is $\mathrm{Uniform}(S_0)$, regardless of the excited spectrum. Since $A_1$ depends only on the excited spectrum (via $\{d_k, E_k\}_{k \geq 1}$), we have
\[
I(X_{\mathrm{DH}};\,A_1 \mid \text{success}, S_0, E_0) = 0.
\]
Let $F$ be the failure indicator of the amplified routine, and fix any prior over ground-equivalent instances conditioned on $(S_0,E_0)$. With $r = \Theta(n)$ repetitions using Boyer-Brassard-H\o yer-Tapp amplification~\cite{BBHT1998}, the per-trial success probability is at least $2/3$, so
\[
p_f := \Pr[F=1] \leq (1/3)^r = 2^{-\Omega(n)}.
\]
Using chain rule and the fact that $F$ is a function of $X_{\mathrm{DH}}$:
\begin{align}
I(X_{\mathrm{DH}};\,A_1 \mid S_0,E_0)
&\le I(X_{\mathrm{DH}},F;\,A_1 \mid S_0,E_0) \nonumber\\
&= I(F;\,A_1 \mid S_0,E_0) + I(X_{\mathrm{DH}};\,A_1 \mid F,S_0,E_0) \nonumber\\
&= I(F;\,A_1 \mid S_0,E_0) + p_f\, I(X_{\mathrm{DH}};\,A_1 \mid F=1,S_0,E_0) \nonumber\\
&\le H(F) + p_f\, H(X_{\mathrm{DH}} \mid F=1,S_0,E_0). \label{eq:A1-blindness-mi}
\end{align}
The $F=0$ term vanishes by the conditional independence established above. Now $H(F) \le h_2(p_f)$, where $h_2(p) = -p\log p - (1-p)\log(1-p)$ is the binary entropy, and $H(X_{\mathrm{DH}} \mid F=1,S_0,E_0) \le \log N = n$, so Eq.~\eqref{eq:A1-blindness-mi} gives
\[
I(X_{\mathrm{DH}};\,A_1 \mid S_0,E_0) \le h_2(p_f) + p_f n = 2^{-\Omega(n)}.
\]
\end{proof}

The circuit model does not merely avoid computing $A_1$; it is provably blind to it. The adiabatic model requires and leaks information about $A_1$: a schedule tuned to $A_1$ achieves success probability $\geq 1 - \varepsilon$, while the same schedule applied to a ground-equivalent instance with different $A_1$ yields low success probability. The adaptive adiabatic model sits between: it acquires $A_1$ through $O(n)$ quantum measurements, paying $O(T_{\mathrm{inf}})$ for the acquisition. The three models form a hierarchy of spectral information usage, from full blindness (circuit) through active acquisition (adaptive) to passive dependence (fixed schedule).

The adaptive protocol relies on the rank-one gap profile, which grows linearly from the crossing ($\alpha = 1$). This linear growth is what makes each binary search step informative: the gap at distance $d$ from the crossing is $\Theta(d)$, so the phase estimation cost at distance $d$ is $O(1/d)$, and the geometric series converges. What happens when the gap approaches its minimum more gently?


\section{Gap Geometry and Schedule Optimality}
\label{sec:gap-geometry}

The flatness exponent $\alpha$ parametrizes how the gap approaches its minimum: $g(s) \approx c|s - s^*|^\alpha$ outside the crossing window. For the rank-one profile, $\alpha = 1$, and the runtime is $O(1/\Delta_*)$. Flatter gap profiles ($\alpha > 1$) are worse. Guo and An~\cite{GuoAn2025} identified the measure condition --- a regularity condition on the gap function controlling whether the power-law schedule of exponent $p = 3/2$ achieves $O(1/g_{\min})$ --- and proved its sufficiency. We prove the complementary degradation: for $\alpha > 1$, the measure condition fails and the variationally optimal $p = 3/2$ schedule degrades from $T = O(1/\Delta_*)$ to $T = O(1/\Delta_*^{3-2/\alpha})$. Whether a non-power-law schedule family can achieve $T = O(1/\Delta_*)$ when the measure condition fails remains open. The constant geometric speed (CGS) schedule of Han, Park, and Choi~\cite{HanParkChoi2025} achieves $O(1/g_{\min})$ by adaptively measuring the gap, but it uses runtime quantum feedback and is not a fixed schedule. Among fixed, non-adaptive schedules, no family is known to beat $O(1/\Delta_*^{3-2/\alpha})$ for $\alpha > 1$.

Consider a gap function with flatness exponent $\alpha > 0$: near the minimum, $g(s) = \Delta_* + c|s - s^*|^\alpha$ for a constant $c > 0$. The measure condition requires that $\mu(\{s : g(s) \leq x\}) \leq Cx$ for all $x > 0$, where $C$ is independent of $\Delta_*$.

\begin{theorem}[Geometric characterization]
\label{thm:geometric-char}
The measure condition holds with $C$ independent of $\Delta_*$ if and only if $\alpha \leq 1$.
\end{theorem}

\begin{proof}
For $x \geq \Delta_*$, the sublevel set $\{s : g(s) \leq x\}$ near $s^*$ has measure $\mu = 2((x - \Delta_*)/c)^{1/\alpha}$.

\emph{Case $\alpha \leq 1$.} The ratio $\mu/x = 2((x - \Delta_*)/c)^{1/\alpha}/x$ is an increasing function of $x$ for $\alpha \leq 1$: differentiating, $d(\mu/x)/dx = (2/(c^{1/\alpha}\alpha x^2))\,((x - \Delta_*)/c)^{1/\alpha - 1}\,((1/\alpha - 1)(x - \Delta_*) + \Delta_*/\alpha)$, where both terms in the parentheses are nonnegative since $1/\alpha - 1 \geq 0$ and $\Delta_* > 0$. But $\mu$ is also capped by $1$ (the measure of $[0,1]$), and the cap is achieved at $x_{\mathrm{cap}} = \Delta_* + c(1/2)^\alpha$, where the sublevel set spans $[0,1]$. For $x > x_{\mathrm{cap}}$, $\mu/x = 1/x < 1/x_{\mathrm{cap}}$. Taking the supremum: $C = \sup_{x > 0} \mu/x \leq 1/x_{\mathrm{cap}} \leq 2^\alpha/c$, independent of $\Delta_*$.

\emph{Case $\alpha > 1$.} At $x = 2\Delta_*$, the ratio is $\mu/x = 2(\Delta_*/c)^{1/\alpha}/(2\Delta_*) = c^{-1/\alpha}\Delta_*^{1/\alpha - 1}$. Since $1/\alpha - 1 < 0$, this diverges as $\Delta_* \to 0$. No finite $C$ works for all $\Delta_*$.
\end{proof}

The gap integral $\int_0^1 g(s)^{-\beta}\,ds$ controls the runtime for power-law schedules. A substitution $u = c|s - s^*|^\alpha/\Delta_*$ gives the following scaling.

\begin{lemma}[Gap integral]
\label{lem:gap-integral}
For $\beta > 1/\alpha$,
\begin{equation}
\label{eq:gap-integral}
\int_0^1 g(s)^{-\beta}\,ds = \Theta(\Delta_*^{1/\alpha - \beta}).
\end{equation}
For $\beta = 1/\alpha$, the integral is $\Theta(\log(1/\Delta_*))$. For $\beta < 1/\alpha$, the integral is $\Theta(1)$.
\end{lemma}

\begin{proof}
The substitution $u = (c|s-s^*|^\alpha)/\Delta_*$ transforms the near-minimum contribution to
\[
\Delta_*^{1/\alpha-\beta}\int_0^{U} u^{1/\alpha-1}(1+u)^{-\beta}\,du,
\]
where $U=\Theta(1/\Delta_*)$. As $u\to\infty$, the integrand behaves like $u^{1/\alpha-1-\beta}$.

If $\beta>1/\alpha$, the exponent is strictly less than $-1$, so the $u$-integral converges to a finite constant, yielding $\Theta(\Delta_*^{1/\alpha-\beta})$.

If $\beta=1/\alpha$, the integrand is asymptotically $u^{-1}$, so the integral contributes $\Theta(\log U)=\Theta(\log(1/\Delta_*))$.

If $\beta<1/\alpha$, the integral grows as $U^{1/\alpha-\beta}$, which cancels the prefactor $\Delta_*^{1/\alpha-\beta}$, giving $\Theta(1)$.

The contribution from outside a neighborhood of $s^*$ is always $O(1)$: for $|s-s^*|\ge\delta$ with $\delta$ fixed, $g(s)\ge g_0>0$ independent of $\Delta_*$, so $\int_{|s-s^*|\ge\delta} g(s)^{-\beta}ds \le g_0^{-\beta}$.
\end{proof}

\begin{theorem}[Scaling spectrum]
\label{thm:scaling-spectrum}
For a gap function with flatness exponent $\alpha > 2/3$, the adiabatic runtime with the $p = 3/2$ power-law schedule (variationally optimal in the JRS framework~\cite{GuoAn2025}) satisfies
\begin{equation}
\label{eq:scaling-spectrum}
T = \Theta(1/\Delta_*^{3 - 2/\alpha}).
\end{equation}
\end{theorem}

\begin{proof}
The power-law schedule $u'(s) = c_p\,g(u(s))^p$ has normalization constant $c_p = \int_0^1 g(v)^{-p}\,dv$. The JRS error functional becomes
\begin{equation}
\eta \leq \frac{1}{T}\,c_p \int_0^1 g(v)^{p-3}\,dv.
\end{equation}
By \autoref{lem:gap-integral}, $c_p = \Theta(\Delta_*^{1/\alpha - p})$ (requiring $p > 1/\alpha$) and the second integral is $\Theta(\Delta_*^{1/\alpha + p - 3})$ (requiring $3 - p > 1/\alpha$). Together these require $1/\alpha < p < 3 - 1/\alpha$, an interval of width $3 - 2/\alpha$, which is positive if and only if $\alpha > 2/3$. The symmetric choice $p = 3/2$ lies in this interval for all $\alpha > 2/3$. Their product is
\begin{equation}
c_p \int g^{p-3}\,dv = \Theta(\Delta_*^{(1/\alpha - p) + (1/\alpha + p - 3)}) = \Theta(\Delta_*^{2/\alpha - 3}).
\end{equation}
Setting $\eta = O(1)$ gives $T = \Omega(\Delta_*^{-(3 - 2/\alpha)}) = \Omega(1/\Delta_*^{3-2/\alpha})$. The $p = 3/2$ power-law schedule achieves this scaling, giving a matching upper bound $T = O(1/\Delta_*^{3-2/\alpha})$.
\end{proof}

\begin{center}
\begin{tabular}{llll}
\hline
$\alpha$ & Exponent $\gamma = 3 - 2/\alpha$ & Measure condition & Runtime \\
\hline
$1$ & $1$ & Holds & $\Theta(1/\Delta_*)$ \\
$2$ & $2$ & Fails & $\Theta(1/\Delta_*^2)$ \\
$3$ & $7/3$ & Fails & $\Theta(1/\Delta_*^{7/3})$ \\
$\infty$ & $3$ & Fails & $\Theta(1/\Delta_*^3)$ \\
\hline
\end{tabular}
\end{center}

The runtime exponents form a continuous spectrum from $1$ (V-shaped minimum, best case) to $3$ (flat minimum, worst case), refuting any binary dichotomy between ``easy'' and ``hard'' gap profiles. For the running example ($M = 2$, $d_0 = 1$, $N = 4$), $\alpha = 1$ and $\gamma = 1$, confirming the optimal $T = \Theta(1/\Delta_*)$ scaling.

\begin{remark}
The exponent $\gamma = 3$ at $\alpha = \infty$ reflects the $p = 3/2$ power-law schedule, which is variationally optimal within the JRS error functional but not universally optimal across all adiabatic bounds. The Roland-Cerf schedule ($p = 2$) gives $T = O(1/\Delta_*^2)$ at $\alpha = \infty$ via a tighter adiabatic condition for flat gaps. The table shows the scaling of the JRS-optimal schedule as the gap flattens; different schedule families and adiabatic bounds produce different exponent curves.
\end{remark}

\begin{proposition}[Structural $\alpha = 1$]
\label{prop:structural-alpha}
For the rank-one Hamiltonian $H(s) = -(1-s)\ket{\psi_0}\!\bra{\psi_0} + sH_z$ with $d_1 \geq 1$ and $\Delta > 0$, the flatness exponent is $\alpha = 1$.
\end{proposition}

\begin{proof}
Near $s^*$, the two lowest eigenvalues form an avoided crossing described by the standard formula $g(s) = \sqrt{g_{\min}^2 + c_L^2(s - s^*)^2}$. For $|s - s^*| \gg g_{\min}/c_L = \delta_s$, the gap grows linearly: $g(s) \approx c_L|s - s^*|$. The crossing is simple (not higher-order) because the coupling between the two lowest branches is proportional to $|\!\braket{\psi_0}{\phi_1}\!|^2 = d_1/N > 0$, where $\ket{\phi_1}$ is the symmetric state of the first excited level. A higher-order crossing ($\alpha > 1$) would require this coupling to vanish, which cannot happen when $d_1 > 0$.
\end{proof}

No choice of $H_z$ with $d_1 > 0$ and $\Delta > 0$ can produce $\alpha \neq 1$. Different values of $\alpha$ require different interpolation schemes (e.g., quantum phase transitions with $H(s)$ nonlinear in $s$, or systems with symmetry-enforced higher-order crossings). This structural $\alpha = 1$ explains why both the Roland-Cerf analysis and the Guo-An framework achieve the same asymptotic runtime.

Braida et al.~\cite{braida2024unstructured} and Guo and An~\cite{GuoAn2025} are independent works on the same problem class. The former provides the spectral analysis ($A_1$, $s^*$, piecewise gap bounds), while the latter provides the variational optimization (power-law schedule, measure condition).

\begin{theorem}[Measure condition for the rank-one gap profile]
\label{thm:measure-paper}
Under the spectral condition of Chapter~5, the piecewise-linear gap profile satisfies the measure condition with
\begin{equation}
\label{eq:measure-constant}
C \leq \frac{3A_2}{A_1(A_1+1)} + \frac{30(1 - s_0)}{\Delta},
\end{equation}
where $s_0$ is the right-arm basepoint defined in Chapter~6.
\end{theorem}

\begin{proof}
Fix $x > 0$. For $x < g_{\min}$, the sublevel set is empty. For $x \geq g_{\min}$, bound the contribution from each piece of the gap profile. The left arm ($g(s) \geq c_L(s^* - s)$) contributes at most $x/c_L$. The crossing window ($|s - s^*| \leq \delta_s$) has width $2\delta_s = 2\hat{g}/c_L$, contributing at most $2x/c_L$ for $x \geq \hat{g}$ (for $g_{\min} \leq x < \hat{g}$: since $g_{\min} \geq (1-2\eta)\hat{g}$ and $x \geq g_{\min}$, we have $\hat{g} \leq x/(1-2\eta)$; for $\eta \leq 1/6$, this gives $\hat{g} \leq 3x/2$, so the window contribution $2\hat{g}/c_L \leq 3x/c_L$; the condition $\eta \leq 1/6$ holds in the asymptotic regime where $\eta = O(\sqrt{d_0/(NA_2)}) \to 0$). The right arm ($g(s) \geq c_R(s - s_0)/(1 - s_0)$) contributes at most $x \cdot 30(1-s_0)/\Delta$. Combining and substituting $c_L = A_1(A_1+1)/A_2$ gives the bound.
\end{proof}

\begin{corollary}[Grover measure constant]
\label{cor:grover-C}
For Grover ($M = 2$, $d_0 = 1$, $d_1 = N - 1$, $E_0 = 0$, $E_1 = 1$), the exact measure constant is $C = 1$.
\end{corollary}

\begin{proof}
The exact gap is $g(s)^2 = (2s-1)^2(1 - 1/N) + 1/N$. Solving $g(s) \leq x$ gives $\mu(\{g \leq x\}) = \sqrt{(Nx^2 - 1)/(N-1)}$ for $x \in [1/\sqrt{N}, 1]$, with $\mu = 1$ for $x > 1$. The ratio $\mu/x$ is increasing on $[1/\sqrt{N}, 1]$ and equals $1$ at $x = 1$.
\end{proof}

For the Grover problem, the exact gap integral is $\int_0^1 g(s)^{-2}\,ds = (N/\sqrt{N-1})\arctan\sqrt{N-1} \to (\pi/2)\sqrt{N}$ as $N \to \infty$. This closed-form evaluation confirms the $O(\sqrt{N})$ runtime from the piecewise analysis and provides the exact constant. For the running example ($N = 4$), $\int_0^1 g(s)^{-2}\,ds = (4/\sqrt{3})\arctan\sqrt{3} = 4\pi/(3\sqrt{3}) \approx 2.42$, consistent with the runtime $T_{\mathrm{inf}} = O(\sqrt{4}) = O(2)$.

Both the Roland-Cerf $p = 2$ schedule and Guo-An's $p = 3/2$ schedule achieve the same asymptotic runtime $T = O(\sqrt{N/d_0}/\varepsilon)$ (where all spectral parameters $A_1$, $A_2$, $\Delta$ are absorbed into the implicit constant, as declared at the start of this chapter). The RC runtime involves the integral $I = \int_0^1 g(s)^{-2}\,ds$; Guo-An's involves $C^2/g_{\min}$.

\begin{theorem}[Constant comparison]
\label{thm:constant-comparison}
Write $a = 3/c_L$ and $r = 30(1-s_0)/\Delta$. Then $C^2 < I$ if and only if $(c_L - 1)r^2 - 2ar + a(1-a) > 0$. In the right-arm-dominated regime ($r \gg a$) with $c_L > 1$, this holds, with $C^2/I \to 1/c_L = A_2/(A_1(A_1+1))$.
\end{theorem}

\begin{proof}
With $C = a + r$ and $I = a + r^2 c_L$: $I - C^2 = (c_L - 1)r^2 - 2ar + a(1-a)$. For $c_L > 1$ and $r \gg a$, the leading term $(c_L - 1)r^2$ dominates.
\end{proof}

\begin{remark}
The framework comparison extends across gap geometries. For $\alpha < 1$, the Roland-Cerf integral $\int g^{-2}\,ds = \Theta(g_{\min}^{1/\alpha - 2})$ grows slower than $1/g_{\min}$, making the RC analysis tighter. For $\alpha = 1$, both give $\Theta(1/g_{\min})$, and the JRS constant $C^2$ can be smaller than the RC integral $I$ (\autoref{thm:constant-comparison}). For $\alpha > 1$, the measure constant $C \to \infty$ as $g_{\min} \to 0$, so the JRS framework degrades and only the RC analysis applies. The structural $\alpha = 1$ (\autoref{prop:structural-alpha}) sits at the exact boundary where both frameworks are valid and neither uniformly dominates.
\end{remark}

For the Grover problem, $c_L \to 2$ as $N \to \infty$, and using exact values $C_{\mathrm{exact}} = 1$, $I_{\mathrm{exact}} \to (\pi/2)\sqrt{N}$, the ratio $C^2/I \to 2/(\pi\sqrt{N}) \to 0$: the JRS certification is asymptotically tighter. The Grover gap has a closed-form expression that makes the exact measure constant computable. For structured Hamiltonians with richer spectra, the exact constants are not available analytically, and the bound-constant ratio from \autoref{thm:constant-comparison} provides the comparison. Evaluating the theorem for the open ferromagnetic Ising chain (\autoref{eq:Ising-Ham} with nearest-neighbor coupling $J = 1$ and uniform field $h = 1$, $n = 10$ spins) gives $C^2/I = 0.71$: the JRS advantage persists, though with a weaker margin than Grover. The two frameworks are complementary, not competing. The spectral analysis~\cite{braida2024unstructured} identifies $A_1$, $s^*$, and the piecewise gap structure. The variational optimization~\cite{GuoAn2025} determines the optimal power-law exponent. Together they give a complete picture: the rank-one $\alpha = 1$ gap sits at the exact boundary where both frameworks apply and the measure condition holds with a bounded constant.

The complementarity extends to their sensitivity under partial spectral knowledge. The RC framework ($p = 2$) constructs its schedule from the crossing position $s^*$, so its runtime degrades on the crossing-localization scale: $T_{\mathrm{RC}}(\varepsilon_{A_1}) = T_{\mathrm{RC},\infty} \cdot \Theta(\max(1, \varepsilon_{A_1}/\delta_{A_1}))$, where $\delta_{A_1} = 2\sqrt{d_0 A_2/N}$ is the precision threshold from \autoref{thm:interpolation}. The JRS framework ($p = 3/2$) instead requires certified bounds $(C_+, g_-)$ on the measure constant and minimum gap, producing a multiplicative overhead $((1 + \delta_C/C)^2)/(1 - \delta_g/g_{\min})$ where $\delta_C$ and $\delta_g$ are the estimation errors. The two frameworks have qualitatively different sensitivity profiles: RC needs $s^*$ to exponentially small precision $\delta_s = \Theta(2^{-n/2})$, while JRS needs $C$ and $g_{\min}$ only to constant relative precision. When spectral parameters are partially known --- the situation forced by NP-hardness --- the JRS framework may be more robust to imprecise inputs, even though both frameworks achieve the same asymptotic scaling.

The gap geometry and optimality analysis above assumes the rank-one interpolation $H(s) = -(1-s)\ket{\psi_0}\!\bra{\psi_0} + sH_z$. The rank-one structure is a design choice, not a physical constraint. Can a different design --- different initial state, ancilla qubits, multi-segment path --- avoid the $A_1$ dependence entirely?


\section{Anatomy of the Barrier}
\label{sec:barrier-anatomy}

No instance-independent modification within the rank-one framework can make $s^*$ spectrum-independent. The argument proceeds through four theorems that progressively close escape routes, culminating in a no-go theorem.

Recall from Chapter~5 that for any initial state $\ket{\psi} \in \mathbb{C}^N$, the weights $w_k(\psi) = \sum_{z \in \Omega_k} |\braket{z}{\psi}|^2$ determine the effective spectral parameter $A_1^{\mathrm{eff}}(\psi) = \sum_{k \geq 1} w_k(\psi)/(E_k - E_0)$ and the effective crossing position $s^*(\psi) = A_1^{\mathrm{eff}}(\psi)/(A_1^{\mathrm{eff}}(\psi) + 1)$. For the uniform superposition $\ket{\psi_0}$, $w_k = d_k/N$ and $A_1^{\mathrm{eff}} = A_1$.

\begin{theorem}[Product ancilla invariance]
\label{thm:product-ancilla}
For any product initial state $\ket{\Psi} = \ket{\psi_0} \otimes \ket{\phi}$ and uncoupled final Hamiltonian $H_f = H_z \otimes I_{2^m}$, the extended Hamiltonian $H_{\mathrm{ext}}(s) = -(1-s)\ket{\Psi}\!\bra{\Psi} + s(H_z \otimes I_{2^m})$ has the same crossing position $s^* = A_1/(A_1 + 1)$ as the bare system.
\end{theorem}

\begin{proof}
Decompose the extended Hilbert space $\mathbb{C}^N \otimes \mathbb{C}^{2^m}$ into the subspace $\mathcal{V}_\phi = \mathbb{C}^N \otimes \ket{\phi}$ and its orthogonal complement. States $\ket{z} \otimes \ket{a}$ with $\braket{\phi}{a} = 0$ satisfy $\braket{\Psi}{z,a} = 0$, making them exact eigenstates of $H_{\mathrm{ext}}(s)$ with eigenvalue $sE(z)$. These $N(2^m - 1)$ states do not participate in the avoided crossing. The restriction of $H_{\mathrm{ext}}(s)$ to $\mathcal{V}_\phi$ is unitarily equivalent to the bare Hamiltonian $H(s)$ via the isomorphism $\ket{\psi} \otimes \ket{\phi} \mapsto \ket{\psi}$.
\end{proof}

\begin{remark}
The crossing position is invariant, but the gap of $H_{\mathrm{ext}}(s)$ is strictly smaller than the bare gap: for $d_0 = 1$, the extra eigenvalues at $sE_0$ (from states $\ket{z} \otimes \ket{a}$ with $z \in \Omega_0$, $a \perp \ket{\phi}$) sit between the ground eigenvalue $\lambda_0(s) < sE_0$ and the crossing branch. Uncoupled ancillas make the gap worse, not better.
\end{remark}

\begin{theorem}[Universality of uniform superposition]
\label{thm:universality}
Among all states $\ket{\psi} \in \mathbb{C}^N$, the uniform superposition $\ket{\psi_0}$ is the unique state (up to per-basis-element phases) for which the weights $w_k(\psi)$ depend only on $\{E_k, d_k\}$ and not on the specific assignment of energies to computational basis states.
\end{theorem}

\begin{proof}
An energy assignment is a function $\sigma: \{0,\ldots,N-1\} \to \{E_0,\ldots,E_{M-1}\}$ with $|\sigma^{-1}(E_k)| = d_k$. The weights under assignment $\sigma$ are $w_k(\psi,\sigma) = \sum_{z:\sigma(z)=E_k} |\braket{z}{\psi}|^2$. We require $w_k(\psi,\sigma) = w_k(\psi,\sigma')$ for all assignments $\sigma, \sigma'$ with the same degeneracies.

Any two such assignments are related by a permutation $\pi$ of $\{0,\ldots,N-1\}$. The condition becomes $\sum_{z \in \Omega_k} |\braket{z}{\psi}|^2 = \sum_{z \in \Omega_k} |\braket{\pi^{-1}(z)}{\psi}|^2$ for all $k$ and all permutations $\pi$.

\emph{Necessity.} Consider two-level spectra with $d_0 = 1$. For any two basis states $z_a, z_b$, the transposition swapping them maps the assignment $\sigma$ (with $\sigma(z_a) = E_0$) to $\sigma'$ (with $\sigma'(z_b) = E_0$). The condition forces $|\braket{z_a}{\psi}|^2 = |\braket{z_b}{\psi}|^2$. Since $z_a, z_b$ are arbitrary, $|\braket{z}{\psi}|^2 = 1/N$ for all $z$.

\emph{Sufficiency.} If $|\braket{z}{\psi}|^2 = 1/N$ for all $z$, then $w_k = d_k/N$ regardless of the assignment.
\end{proof}

\begin{corollary}
\label{cor:universality}
Any instance-independent adiabatic algorithm (same Hamiltonian for all energy assignments with the same degeneracy structure) must use the uniform superposition as initial state, fixing the crossing at $s^* = A_1/(A_1+1)$.
\end{corollary}

\begin{theorem}[Coupled ancilla limitation]
\label{thm:coupled-ancilla}
Consider an extended Hamiltonian $H_{\mathrm{ext}}(s) = -(1-s)\ket{\Psi}\!\bra{\Psi} + s(H_z \otimes I + V)$ where $\ket{\Psi} = \ket{\psi_0} \otimes \ket{\phi}$ and $V$ is instance-independent. No fixed $V$ makes $A_1^{\mathrm{eff}}$ constant across all problem instances.
\end{theorem}

\begin{proof}
Consider the two-level family parametrized by $\Delta > 0$: $E_0 = 0$, $E_1 = \Delta$, $d_0 = 1$, $d_1 = N - 1$. For $\Delta > 2\lVert V \rVert$, by Weyl's inequality each eigenvalue of $H_f(\Delta) = H_z(\Delta) \otimes I + V$ lies within $\lVert V \rVert$ of an eigenvalue of $H_z(\Delta) \otimes I$, so the spectrum splits into two well-separated clusters: one near energy $0$ (within $\lVert V \rVert$ of $0$) and one near energy $\Delta$ (within $\lVert V \rVert$ of $\Delta$). Each eigenvalue $E_j$ in the excited cluster satisfies $|E_j - \Delta| \leq \lVert V \rVert$, so the excited contribution to $A_1^{\mathrm{eff}}$ is $\sum_{j \in \mathrm{excited}} |\!\braket{\Psi}{\phi_j}\!|^2/(E_j - E_0) = (1 - d_0/N)/(\Delta + O(\lVert V \rVert)) = \Theta(1/\Delta)$ for $\Delta \gg \lVert V \rVert$. Since $\lVert V \rVert$ is a fixed constant independent of $\Delta$, this contribution varies with $\Delta$, so $A_1^{\mathrm{eff}}(\Delta)$ is non-constant.
\end{proof}

\begin{theorem}[Multi-segment rigidity]
\label{thm:multi-segment}
Consider a two-segment path where segment 2 has Hamiltonian $H_2(t) = -(1-t)\ket{\psi_{\mathrm{mid}}}\!\bra{\psi_{\mathrm{mid}}} + tH_z$. If the algorithm is instance-independent, then the intermediate state $\ket{\psi_{\mathrm{mid}}}$ must be the uniform superposition, giving the same crossing $B_1 = A_1$.
\end{theorem}

\begin{proof}
Segment 2 is a rank-one adiabatic Hamiltonian with initial state $\ket{\psi_{\mathrm{mid}}}$. Its crossing position is $t^* = B_1/(B_1+1)$ where $B_1 = \sum_{k \geq 1} w_k(\psi_{\mathrm{mid}})/(E_k - E_0)$. If segment 1 does not involve $H_z$, then $\ket{\psi_{\mathrm{mid}}}$ is determined entirely by segment 1's Hamiltonian, which is instance-independent. Since $\ket{\psi_{\mathrm{mid}}}$ is then the same for all energy assignments with the same degeneracy structure, \autoref{thm:universality} forces $w_k = d_k/N$, so $B_1 = A_1$. If segment 1 involves $H_z$, then $\ket{\psi_{\mathrm{mid}}}$ already depends on the spectrum, and the algorithm is not instance-independent.
\end{proof}

\begin{theorem}[No-go]
\label{thm:no-go}
For any adiabatic algorithm using a rank-one initial Hamiltonian, a final Hamiltonian whose ground state encodes the solution, and instance-independent design, the crossing position cannot be made independent of the problem spectrum.
\end{theorem}

\begin{proof}
Combine Theorems~\ref{thm:product-ancilla}--\ref{thm:multi-segment}: \autoref{thm:universality} forces the uniform superposition; \autoref{thm:product-ancilla} shows uncoupled ancillas preserve $s^*$; \autoref{thm:coupled-ancilla} shows coupled ancillas shift $s^*$ but cannot make it constant; \autoref{thm:multi-segment} shows multi-segment paths within the rank-one framework cannot escape.
\end{proof}

For the running example ($N = 4$, $d_0 = 1$), product ancilla invariance (\autoref{thm:product-ancilla}) implies that appending any number of ancilla qubits in a product state leaves the crossing at $s^* = 3/7$.

The no-go theorem applies specifically to the rank-one framework with instance-independent design. Higher-rank initial Hamiltonians are a natural candidate for circumventing the rank-one obstruction. They do not succeed: the following propositions show that rank-$k$ projectors cannot make crossing positions spectrum-independent, first on two-level families and then on general multilevel families via a trace argument. For rank-$k$ projectors $P = UU^\dagger$, the secular equation generalizes to the $k \times k$ determinant condition $\det(I_k - (1-s)G(\lambda,s)) = 0$ where $G(\lambda,s) = U^\dagger(sH_z - \lambda I)^{-1}U$. On the two-level family ($E_0 = 0$, $E_1 = \Delta$), this reduces to $\det(I_k - (x/\Delta)B) = 0$ where $B = U_{\mathrm{exc}}^\dagger U_{\mathrm{exc}}$ and $x = (1-s)/s$. Each positive eigenvalue $\mu$ of $B$ gives a crossing branch $s(\Delta) = 1/(1+\Delta/\mu)$, which is non-constant as a function of $\Delta$.

\begin{proposition}[Rank-$k$ two-level obstruction]
\label{prop:rank-k-obstruction}
Fixed rank-$k$ projectors cannot make crossing positions spectrum-independent on fixed-degeneracy two-level families unless the projector has zero support on excited states.
\end{proposition}

For the general multilevel case, the trace argument provides a clean obstruction.

\begin{proposition}[Trace no-go]
\label{prop:trace-nogo}
For a rank-$k$ projector $P = UU^\dagger$ and the multilevel family with gaps $\Delta_1, \ldots, \Delta_{M-1}$, define the reduced matrix $A(\Delta) = \sum_{\ell=1}^{M-1} B_\ell/\Delta_\ell$ where $B_\ell = U_\ell^\dagger U_\ell \succeq 0$ collects the excited-level contributions. If $B_j \neq 0$ and $\Delta_j$ varies, then $\mathrm{tr}(A(\Delta)) = \sum_\ell \mathrm{tr}(B_\ell)/\Delta_\ell$ is non-constant in $\Delta_j$. By Weyl's eigenvalue monotonicity theorem, each eigenvalue of $A(\Delta)$ is a continuous function of $\Delta_j$, and the sum of the positive eigenvalues equals $\mathrm{tr}(A)$. Since the trace changes, at least one positive eigenvalue --- and hence at least one crossing position --- must change with $\Delta_j$.
\end{proposition}

\begin{remark}
When the excited blocks commute ($[B_\ell, B_m] = 0$ for all $\ell, m$), the reduced crossing equation admits simultaneous diagonalization, giving explicit per-branch formulas: each active branch $r$ with $G_r(\Delta) = \sum_\ell \mu_{\ell r}/\Delta_\ell > 0$ has crossing position $s_r = G_r/(1 + G_r)$, and varying any gap $\Delta_j$ gives $\partial s_r/\partial \Delta_j = -\mu_{jr}/(\Delta_j^2(1 + G_r)^2) \leq 0$, with strict inequality whenever the branch has support on the varied level ($\mu_{jr} > 0$). The commuting case thus provides explicit, quantitative non-constancy for each individual branch, complementing the trace argument's aggregate statement. Even the most tractable generalization of the secular equation --- simultaneous diagonalization with explicit formulas --- cannot achieve spectrum-independent crossings.
\end{remark}

The barrier is structural within the rank-one framework and extends to higher-rank families. Whether time-dependent couplings or non-rank-one intermediate Hamiltonians provide a genuine escape remains open. But the no-go is specific to the monotone-schedule adiabatic framework. Dropping the monotone constraint reveals that constant controls already suffice on the restricted two-level family.

\begin{proposition}[Constant-control optimality on two-level family]
\label{prop:constant-control}
For $H_z = I - P_0$ where $P_0$ projects onto the $d_0$-dimensional ground space, the continuous-time rank-one Hamiltonian $H = -\ket{\psi_0}\!\bra{\psi_0} + H_z$ with constant controls achieves $p_0(t^*) = 1$ at $t^* = (\pi/2)\sqrt{N/d_0}$, with controls independent of $A_1$ (on the two-level family $H_z = I - P_0$).
\end{proposition}

\begin{proof}
Let $\mu = d_0/N$, $\ket{G} = d_0^{-1/2}\sum_{x \in S_0}\ket{x}$, and $\ket{B} = (N - d_0)^{-1/2}\sum_{x \notin S_0}\ket{x}$. The initial state is $\ket{\psi_0} = \sqrt{\mu}\,\ket{G} + \sqrt{1-\mu}\,\ket{B}$. Dropping the global identity term, the effective Hamiltonian in the $(\ket{G}, \ket{B})$ basis is
\begin{equation}
\widetilde{H} = -\begin{pmatrix} \mu & \sqrt{\mu(1-\mu)} \\ \sqrt{\mu(1-\mu)} & -\mu \end{pmatrix},
\end{equation}
which satisfies $\widetilde{H}^2 = \mu\,I_2$. The matrix exponential is $e^{-it\widetilde{H}} = \cos(\sqrt{\mu}\,t)\,I_2 - i\sin(\sqrt{\mu}\,t)\,\widetilde{H}/\sqrt{\mu}$. Applying to $\ket{\psi_0}$ and computing the ground-state probability:
\begin{equation}
p_0(t) = |\!\braket{G}{e^{-it\widetilde{H}}|\psi_0}\!|^2 = \mu + (1 - \mu)\sin^2(\sqrt{\mu}\,t).
\end{equation}
At $t^* = (\pi/2)/\sqrt{\mu} = (\pi/2)\sqrt{N/d_0}$, $\sin^2(\sqrt{\mu}\,t^*) = 1$, so $p_0(t^*) = 1$.
\end{proof}

On the two-level family, the Hamiltonian self-calibrates via Rabi-like oscillation at frequency $\sqrt{\mu} = \sqrt{d_0/N}$: the time-independent Hamiltonian $H_r = -\ket{\psi_0}\!\bra{\psi_0} + r \cdot H_z$ with $r = 1$ achieves optimality without knowing $A_1$~\cite{eduardo}. For general spectra, the resonance shifts to $r^* = A_1$~\cite{eduardo}, moving the calibration problem from computing $A_1$ classically to detecting a spectral resonance quantumly.

A natural approach is Loschmidt echo measurement: evolve under $H_r$ and measure the return probability $|\!\braket{\psi_0}{e^{-iH_r t}\psi_0}\!|^2$, which oscillates with large amplitude near resonance and stays close to $1$ off resonance. On the two-level family, this works cleanly: a binary search over $r$ with $O(n)$ probe measurements, each of cost $O(1/g_{\min})$, locates $r^*$ with polynomial overhead; the details appear in~\cite{eduardo}. For general multilevel spectra, higher excited states contribute additional oscillation frequencies that mask the resonance signal. Whether the multilevel Loschmidt echo can be deconvolved efficiently, or whether a different calibration observable circumvents this interference, is open.

The constant-control counterexample applies only to the two-level family $H_z = I - P_0$. Under normalized controls, the barrier reappears.

\begin{proposition}[Normalized-control lower bound]
\label{prop:normalized-control}
Under normalized controls $|g(t)| \leq 1$ and the scaled family $H_z^{(\delta)} = \delta(I - P_0)$ with minimum excitation $\delta \in (0, 1]$, any instance-independent algorithm achieving success probability $\geq 2/3$ requires $T = \Omega(\sqrt{N/d_0}/\delta)$.
\end{proposition}

\begin{proof}
The oracle-dependent term $g(t)H_z^{(\delta)} = \delta g(t)(I - P_0)$ has instantaneous oracle strength $\delta|g(t)|$. The total oracle action is $\mathcal{A} = \int_0^T \delta|g(t)|\,dt \leq \delta T$. The continuous-time query lower bound for unstructured search with $d_0$ marked items among $N$ gives $\mathcal{A} = \Omega(\sqrt{N/d_0})$~\cite{farhi1998analog}, so $T = \Omega(\sqrt{N/d_0}/\delta)$.
\end{proof}

For $\delta = N^{-1/2}$, this gives $T = \Omega(N/\sqrt{d_0})$, the same exponential penalty as the fixed-schedule adiabatic model. The barrier reappears on worst-case instances even for general continuous-time rank-one algorithms, provided the controls are normalized. The scope of the barrier is therefore precise: it is a consequence of monotone schedules with bounded controls, not of continuous-time quantum computation in general.


\section{Computational Nature of $A_1$}
\label{sec:computational-nature}

The barrier cannot be designed away. What kind of computational hardness does it represent? The quantity $A_1$ is not merely NP-hard to compute --- its hardness is counting hardness, inherited from the partition function. The tractability boundary does not align with optimization hardness, is not determined by the number of solutions, and depends on structural properties of the energy landscape.

The quantity $A_1$ encodes spectral information beyond the minimum gap. Consider three energy levels with $E_0 = 0$ ($d_0 = 1$), $E_1 = 1/n$ ($d_1 = 1$), $E_2 = 1$ ($d_2 = N-2$). Then $\Delta = 1/n \to 0$ but $A_1 = (n + N - 2)/N \to 1$, so $1/\Delta \to \infty$ while $A_1 = \Theta(1)$. The tail of $N-2$ states at energy $1$ contributes $(N-2)/N \approx 1$ to $A_1$, dominating the single state at the gap edge that contributes $n/N \approx 0$. The crossing position $s^* = A_1/(A_1+1) \approx 1/2$ is determined by the bulk of the spectrum, not by the gap, making $A_1$ fundamentally a whole-spectrum quantity that $\Delta$ alone cannot predict.

The distinction between NP-hardness at precision $1/\mathrm{poly}(n)$ (\autoref{thm:np-hard-A1}) and $\#$P-hardness exactly (\autoref{thm:sharp-P-hard-A1}) matters because $A_1$ is fundamentally a counting quantity.

\begin{proposition}[$A_1$ hardness is counting hardness]
\label{prop:counting-hardness}
For Boolean CSPs where counting satisfying assignments is $\#$P-hard (including $k$-SAT for $k \geq 2$), computing $A_1$ of the clause-violation Hamiltonian is $\#$P-hard even restricted to satisfiable instances.
\end{proposition}

\begin{proof}
Encode the CSP as $H_z = \sum_{j=1}^m C_j$ where each $C_j(x) = 1$ if assignment $x$ violates clause $j$. The interpolation argument (\autoref{thm:sharp-P-hard-A1}) recovers all degeneracies $d_k$ from polynomially many evaluations of $A_1$ with shifted parameters, via Lagrange interpolation on the rational function $f(x) = \sum_k d_k/(\Delta_k + x/2)$. For satisfiable CSPs, $d_0$ counts satisfying assignments, and counting is $\#$P-hard by hypothesis.
\end{proof}

The partition function connection makes this precise. Shifting energies so that $E_0 = 0$ and defining the Laplace partition function $Z(\beta) = \sum_x e^{-\beta E(x)}$, the spectral parameter admits the integral representation
\begin{equation}
\label{eq:A1-laplace}
A_1 = \frac{1}{N}\int_0^\infty (Z(\beta) - d_0)\,d\beta.
\end{equation}
For integer spectra with $E(x) \in \{0,1,\ldots,m\}$, the ordinary generating function $Z(t) = \sum_x t^{E(x)}$ gives $A_1 = (1/N)\int_0^1 (Z(t) - d_0)/t\,dt$. Both representations appear to require knowing $d_0$, which is itself a counting-hard quantity for many CSPs. However, for additive approximation it suffices to replace $d_0$ by the single evaluation $Z(\tau)$ at a small $\tau > 0$. Define the $\tau$-truncated proxy
\begin{equation}
\label{eq:A1-proxy}
A_1^{(\tau)} = \frac{1}{N}\int_\tau^1 \frac{Z(t) - Z(\tau)}{t}\,dt.
\end{equation}
The additive error satisfies $0 \leq A_1 - A_1^{(\tau)} \leq \tau(1 + \ln(1/\tau))$, so choosing $\tau = O(\eta/\ln(1/\eta))$ gives an $\eta$-approximation to $A_1$ without direct access to $d_0$. When $E_0$ is known, even the coarse version is useful: sampling from the Boltzmann distribution at inverse temperature $\beta$ gives an unbiased estimator of $Z(\beta)/Z(0) = Z(\beta)/N$, and integrating via~\eqref{eq:A1-laplace} yields an additive approximation of $A_1$ without computing $d_0$ directly. These representations turn ``compute $A_1$'' into partition function evaluation, connecting tractability of $A_1$ directly to tractability of counting problems.

\begin{proposition}[Bounded treewidth tractability]
\label{prop:treewidth}
For local energy functions $E(x) = \sum_j E_j(x_{S_j})$ with bounded locality $|S_j| \leq k$ and a tree decomposition of the primal graph of width $w$, $A_1$ is computable exactly in $\mathrm{poly}(n,m) \cdot 2^{O(w)}$ time.
\end{proposition}

\begin{proof}
Write the partition function polynomial $Z(t) = \sum_x t^{E(x)} = \sum_{q=0}^m d_q t^q$ in factor-graph form: $Z(t) = \sum_x \prod_j t^{E_j(x_{S_j})}$. Variable elimination on the tree decomposition computes $Z(t)$ exactly. At each elimination step, factor tables have at most $2^{w+1}$ entries, each a polynomial of degree at most $m$; multiplying factors convolves the polynomials (cost $O(m^2)$ per entry), and summing out a variable adds two polynomials (cost $O(m)$). After $n$ elimination steps, the result is $Z(t) = \sum_q d_q t^q$. Then $A_1 = (1/N)\sum_{q > E_0} d_q/(q - E_0)$.
\end{proof}

The treewidth condition is sufficient but not necessary. A simpler criterion applies whenever the spectrum itself is simple: if $H_z$ has at most $\mathrm{poly}(n)$ distinct energy levels with known energies and degeneracies, then $A_1 = (1/N)\sum_{k \geq 1} d_k/(E_k - E_0)$ is directly computable in $\mathrm{poly}(n)$ time from the defining sum. This criterion is complementary to bounded treewidth --- it applies to spectra that are structurally simple regardless of the interaction graph. For instance, Hamming-distance cost functions $E(x) = |x \oplus z_0|$ for a fixed target $z_0$ have $M = n+1$ levels with degeneracies $d_k = \binom{n}{k}$ and energies $E_k = k$, so $A_1 = (1/N)\sum_{k=1}^n \binom{n}{k}/k$ depends only on $n$ and is trivially computable.

The partition function bridge is one-directional: tractable $Z$ implies tractable $A_1$ (via the integral representations above), but exact $A_1$ does not determine low-temperature $Z(\beta)$.

\begin{proposition}[Reverse bridge obstruction]
\label{prop:reverse-bridge}
There exist two diagonal Hamiltonians $H_z$, $H_z'$ on $N = 2^n$ states with the same ground degeneracy ratio $d_0/N$, same minimum excitation $\Delta_{\min}$, and $A_1(H_z) = A_1(H_z')$ exactly, yet $|Z_{H_z}(\beta) - Z_{H_z'}(\beta)|/N \geq 1/100$ at $\beta = O(1/\Delta_{\min})$.
\end{proposition}

\begin{proof}
Fix an integer $B \geq 3$. Define two spectra, both with $d_0/N = 1/2$ and $\Delta_{\min} = 1/B$: the first has $N/8$ states at energy $1/B$ and $3N/8$ states at energy $B$; the second has $N/16$ states at energy $1/B$ and $7N/16$ states at energy $c_B = 7B/(B^2+6)$. Direct computation gives $A_1 = (B^2+3)/(8B)$ for both. At $\beta = B$: $Z_1(B)/N = 1/2 + e^{-1}/8 + 3e^{-B^2}/8$ while $Z_2(B)/N = 1/2 + e^{-1}/16 + (7/16)e^{-7B^2/(B^2+6)}$. Since $7B^2/(B^2+6) \geq 4.2$ for $B \geq 3$, the difference is at least $e^{-1}/16 - (7/16)e^{-4.2} > 1/100$.
\end{proof}

Three natural conjectures about easy instances of $A_1$ computation are all false.

\begin{proposition}[Unique solution does not imply easy $A_1$]
\label{prop:conjecture-unique}
There exist instances with $d_0 = 1$ for which computing $A_1$ is $\#$P-hard.
\end{proposition}

\begin{proof}
The proof of \autoref{prop:counting-hardness} applies to satisfiable instances with $d_0 = 1$: the interpolation reduction recovers $d_1, \ldots, d_{M-1}$ from $A_1$ evaluations, and counting the number of assignments at each violation level is $\#$P-hard. Concretely, for a satisfiable 3-SAT instance with $m$ clauses and a unique satisfying assignment, the clause-violation Hamiltonian $H_z = \sum_j C_j$ has $d_0 = 1$ and $A_1 = \sum_{k=1}^m d_k/(kN)$, where $d_k$ counts assignments violating exactly $k$ clauses. Recovering these counts from $A_1$ (via shifted parameters) is $\#$P-hard.
\end{proof}

\begin{proposition}[Bounded degeneracy is vacuous]
\label{prop:conjecture-bounded}
If all $d_k \leq \mathrm{poly}(n)$ and $M \leq \mathrm{poly}(n)$, then $d_0 \geq N - \mathrm{poly}(n)^2$, and the optimization problem is trivially solvable by random sampling.
\end{proposition}

\begin{proof}
The total state count satisfies $\sum_{k=0}^{M-1} d_k = N = 2^n$. If $d_k \leq \mathrm{poly}(n)$ for all $k$ and $M \leq \mathrm{poly}(n)$, then $\sum_{k \geq 1} d_k \leq (M-1) \cdot \mathrm{poly}(n) \leq \mathrm{poly}(n)^2$, so $d_0 \geq N - \mathrm{poly}(n)^2$. For $n$ large enough, $d_0/N \geq 1 - o(1)$, and a random sample finds a ground state with probability $1 - o(1)$.
\end{proof}

\begin{proposition}[Hard optimization does not imply hard $A_1$]
\label{prop:conjecture-hard-opt}
The tractability of $A_1$ is independent of optimization hardness. 2-SAT is in P but $\#$2-SAT is $\#$P-complete~\cite{valiant1979complexity}, giving easy optimization with hard $A_1$. In the reverse direction, Grover search with a promised ground degeneracy $d_0$ gives hard optimization but trivial $A_1 = (N - d_0)/(N\Delta)$, computable in $O(1)$ from the promise.
\end{proposition}

The tractability boundary for $A_1$ is subtle. It does not align with optimization hardness, is not determined by the number of solutions, and depends on structural properties of the energy landscape (treewidth, partition function tractability) rather than on the difficulty of finding the ground state.


\section{The Complexity Landscape}
\label{sec:complexity-landscape}

The quantum query complexity of $A_1$ estimation at the algorithmically relevant precision $\varepsilon = 2^{-n/2}$ is $O(2^{n/2} \cdot \mathrm{poly}(n))$ (\autoref{thm:quantum-A1}), the classical lower bound is $\Omega(2^n)$ (\autoref{thm:classical-lower-A1}), and no polynomial-interpolation scheme can establish $\#$P-hardness at this precision (\autoref{thm:generic-barrier}). The quantum bound is tight.

\begin{theorem}[Tight quantum query complexity at schedule precision]
\label{thm:tight-quantum}
At the schedule-relevant precision $\varepsilon = \Theta(2^{-n/2})$, the quantum query complexity of $A_1$ estimation is $\Theta(2^{n/2}) = \Theta(1/\varepsilon)$. The lower bound is achieved on two-level instances with $\Delta_1 = 1$.
\end{theorem}

\begin{proof}
For $\varepsilon = \Theta(2^{-n/2})$ and $\Delta_1 = 1$, \autoref{thm:quantum-A1} gives an upper bound $O(\sqrt{N} + 1/\varepsilon) = O(2^{n/2})$. For the lower bound, consider $M = 2$ instances with $\Delta_1 = 1$: estimating $A_1 = (N-d_0)/N$ to precision $\varepsilon$ reduces to approximate counting. The Grover iterate $G = (2\ket{+}\!\bra{+} - I)(I - 2\Pi_S)$ has eigenphases $\pm 2\theta$ with $\sin^2\theta = d_0/N$. After $T$ sequential applications of $G$, the probe state acquires phase $T\phi$ where $\phi = 2\arcsin(\sqrt{d_0/N})$, and the quantum Fisher information for estimating $\phi$ from $T$ sequential applications is $F_Q = 4T^2$ (the standard Heisenberg limit for phase accumulation in sequential quantum metrology~\cite{braunstein1994statistical, giovannetti2006quantum}). The quantum Cram\'er-Rao bound gives $\mathrm{Var}(\hat{\phi}) \geq 1/F_Q = 1/(4T^2)$. The adversary chooses $d_0 = N/2$ so that $\sin^2\theta = 1/2$, where $|d(\sin^2\theta)/d\theta| = |\sin 2\theta| = 1$. Estimating $A_1 = 1 - \sin^2\theta$ to precision $\varepsilon$ at this operating point requires estimating $\theta$ to precision $\varepsilon$, hence $1/(4T^2) \leq \varepsilon^2$, giving $T \geq 1/(2\varepsilon)$ applications of $G$, each costing $O(1)$ oracle queries. With $\varepsilon = \Theta(2^{-n/2})$, this is $\Omega(2^{n/2})$, so the complexity is $\Theta(2^{n/2}) = \Theta(1/\varepsilon)$.
\end{proof}

\noindent The lower bound is worst-case over the unknown parameter $d_0$: the adversary fixes $d_0 = N/2$ to maximize the estimation difficulty. The Heisenberg limit $F_Q = 4T^2$ for sequential unitary applications is a fundamental result of quantum metrology that applies to all quantum estimation strategies, not just phase estimation --- it follows from the unitarity of quantum evolution and the Cram\'er-Rao inequality, as shown by Braunstein and Caves~\cite{braunstein1994statistical} and extended to the sequential setting by Giovannetti, Lloyd, and Maccone~\cite{giovannetti2006quantum}. Alternatively, the $\Omega(1/\varepsilon)$ lower bound follows directly from quantum approximate counting: estimating $d_0/N$ to additive precision $\varepsilon$ requires $\Omega(1/\varepsilon)$ quantum queries~\cite{BrassardHoyerMoscaEtAl2002, NayakWu1999}, and the reduction to $A_1$ is immediate on $M = 2$ instances.

This result connects directly to the adaptive protocol of \autoref{sec:quantum-bypass}: the adaptive protocol achieves $T_{\mathrm{adapt}} = O(T_{\mathrm{inf}}) = O(2^{n/2})$, which is the same order as the tight quantum query complexity $\Theta(2^{n/2})$ for $A_1$ estimation at the algorithmically relevant precision $\delta_{A_1} = \Theta(2^{-n/2})$. This is not a coincidence --- both tasks require distinguishing $\Omega(2^{n/2})$ possibilities with $O(1)$ information per quantum measurement.

In the high-precision regime relevant to schedule placement, the quadratic quantum advantage persists.

\begin{proposition}[Precision phase diagram]
\label{prop:precision-phase}
For two-level instances with $\Delta_1 = 1$ and precision $\varepsilon \leq c/\sqrt{N}$ (constant $c$), the query complexity of $A_1$ estimation is $\Theta(1/\varepsilon)$ quantum and $\Theta(1/\varepsilon^2)$ classical. The quantum-to-classical ratio is $\Theta(1/\varepsilon)$ in this regime.
\end{proposition}

\begin{proof}
In this regime, $1/\varepsilon \geq \sqrt{N}/c$, so the upper bound of \autoref{thm:quantum-A1} becomes $O(\sqrt{N} + 1/\varepsilon) = O(1/\varepsilon)$. The matching quantum lower bound is inherited from two-level approximate counting (\autoref{thm:tight-quantum}). The classical lower bound $\Omega(1/\varepsilon^2)$ follows from \autoref{thm:classical-lower-A1}, and Monte Carlo sampling gives the matching upper bound $O(1/\varepsilon^2)$.
\end{proof}

\begin{theorem}[ETH computational complexity]
\label{thm:eth}
Under the Exponential Time Hypothesis (ETH), any classical algorithm computing $A_1$ at precision $2^{-n/2}$ requires $2^{\Omega(n)}$ time.
\end{theorem}

\begin{proof}[Proof sketch]
The NP-hardness reduction (\autoref{thm:np-hard-A1}) maps 3-SAT on $n_{\mathrm{var}}$ variables to $A_1$ estimation of a 3-local Hamiltonian on $n = O(n_{\mathrm{var}})$ qubits at precision $1/\mathrm{poly}(n)$. By the Impagliazzo-Paturi-Zane sparsification lemma~\cite{impagliazzo2001problems}, 3-SAT on $n_{\mathrm{var}}$ variables can be reduced to instances with $O(n_{\mathrm{var}})$ clauses, giving $n = O(n_{\mathrm{var}})$ qubits. Any algorithm computing $A_1$ at precision $2^{-n/2}$ can, in particular, compute $A_1$ at the coarser precision $1/\mathrm{poly}(n)$ (since $2^{-n/2} < 1/\mathrm{poly}(n)$ for large $n$), and thus solves 3-SAT by the reduction. Under ETH, 3-SAT on $n_{\mathrm{var}} = \Omega(n)$ variables requires $2^{\Omega(n_{\mathrm{var}})} = 2^{\Omega(n)}$ time.
\end{proof}

Under ETH, the quadratic quantum advantage extends from the query model to the computational model.

\begin{corollary}[Quantum pre-computation cost]
\label{cor:quantum-precomp}
Estimating $A_1$ to the schedule-relevant precision $\delta_{A_1} = \Theta(2^{-n/2})$ via quantum amplitude estimation costs $\Theta(2^{n/2} \cdot \mathrm{poly}(n))$ time, matching the adaptive protocol's runtime. The classical pre-computation cost at the same precision is $\Omega(2^n)$.
\end{corollary}

In terms of parameterized complexity, the corollary places $A_1$ estimation at precision $2^{-n/2}$ in $\mathrm{FBQTIME}(2^{n/2} \cdot \mathrm{poly}(n))$: the problem is not polynomial-time in the standard sense but admits a quantum algorithm whose runtime matches the Grover scale. The information cost of fixed-schedule adiabatic optimization matches the quantum speedup scale: estimating the missing $n/2$ bits of $A_1$ costs $\Theta(2^{n/2})$ quantumly, the same scale as Grover search and the informed adiabatic runtime. The circuit model achieves that scale without this pre-step because amplitude amplification directly solves the search task.

The generic extrapolation barrier (\autoref{thm:generic-barrier}) shows that the interpolation breakdown at precision $2^{-n/2}$ is not an artifact of a specific construction. Any polynomial extrapolation scheme with $d = \mathrm{poly}(n)$ nodes faces Lebesgue amplification $\Lambda_d(x^*) \geq 2^{d-1}$ when the evaluation point is at distance at least $b-a$ from the sample interval $[a,b]$; this geometric condition must be checked in each interpolation setup and is not implied solely by $x^* = \Theta(2^{-n/2})$ with nodes in $[0,1/\mathrm{poly}(n)]$. Under that condition, polynomial-interpolation reductions for $A_1$ require precision $2^{-\Omega(n)}$.

\begin{proposition}[Two-level worst-case reduction]
\label{thm:structure-irrelevance}
The two-level family ($M = 2$) is a worst-case subclass for $A_1$ estimation at schedule-relevant precision: any worst-case lower bound for approximate counting on this subclass applies to general $A_1$ estimation.
\end{proposition}

\begin{proof}
Worst-case complexity over all instances is at least the complexity on any subclass. Restricting to $M = 2$ gives
\[
A_1 = \frac{N-d_0}{N\Delta_1},
\]
so for fixed $\Delta_1=1$, estimating $A_1$ to additive precision $\varepsilon$ is exactly approximate counting for $d_0/N$ at precision $\varepsilon$. Therefore, any lower bound for approximate counting on this subclass is automatically a lower bound for general $A_1$ estimation.
\end{proof}

\noindent The worst-case hardness of $A_1$ estimation does not hide in complex spectra. Simple two-level instances, where $A_1$ reduces to counting, already saturate the query lower bound. Algorithms exploiting the sum-of-reciprocals structure of $A_1$ for multilevel spectra cannot beat algorithms for plain mean estimation.

At the schedule precision, bounded-treewidth instances remain tractable for $A_1$ computation (\autoref{prop:treewidth}). For ferromagnetic Ising models, the partition function $Z(\beta)$ can be multiplicatively approximated in polynomial time~\cite{jerrum1993polynomial}, which gives an additive approximation of $A_1$ at coarse precision via the integral representations of \autoref{sec:barrier-anatomy}. However, achieving precision $\delta_{A_1} = \Theta(2^{-n/2})$ requires the multiplicative accuracy $\mu = O(2^{-n/2}/B)$ where $B = O(\log(1/\delta_{A_1})/\Delta_{\min})$, and the FPRAS runtime scales as $\mathrm{poly}(1/\mu)$. Since $\mu = O(2^{-n/2}/B)$ with $B = O(n/\Delta_{\min})$, this gives $\mathrm{poly}(1/\mu) = 2^{\Omega(n)}$. The ferromagnetic Ising approximation does not remain tractable at the algorithmically relevant precision.

The interpolation theorem (\autoref{thm:interpolation}) provides the quantitative link between information and runtime. To formalize this, consider a communication setting: Alice holds the complete classical description of $H_z$ (all eigenvalues and degeneracies), Bob holds a quantum computer with oracle access to $H_z$, and Alice can send $C$ classical bits to Bob. Bob's goal is to find a ground state using at most $T$ queries. In the circuit-oracle model, $C = 0$ suffices for $T = O(\sqrt{N/d_0})$: Bob runs the D\"urr-H\o yer algorithm without any communication. In the fixed-schedule adiabatic model, Bob must construct a schedule with velocity matched to the crossing, requiring $s^*$ to precision $\Delta_* = O(2^{-n/2})$, which costs $C = \Theta(n)$ bits. Each additional bit of $A_1$ precision halves the adiabatic runtime, until $n/2$ bits suffice for optimality. Formally, if Alice communicates $C$ bits encoding $A_1$ to precision $\varepsilon = \Theta(2^{-C})$, the adiabatic runtime satisfies $T(C) = T_{\mathrm{inf}} \cdot \Theta(\max(1, 2^{n/2 - C}))$.

\begin{theorem}[Bit-runtime information law]
\label{thm:bit-runtime}
The classical communication cost for the adiabatic model to achieve target runtime $T$ is $C^*(T) = \max(0,\,n/2 - \log_2(T/T_{\mathrm{inf}}))$ bits, while $C^*_{\mathrm{circuit}}(T) = 0$ for all $T \geq T_{\mathrm{inf}}$.
\end{theorem}

\begin{proof}
Inverting $T(C) = T_{\mathrm{inf}} \cdot 2^{n/2 - C}$ gives $C = n/2 - \log_2(T/T_{\mathrm{inf}})$. Clamping $C \geq 0$ and noting that the circuit model achieves $T = T_{\mathrm{inf}}$ at $C = 0$ by the D\"urr-H\o yer algorithm gives both formulas.
\end{proof}

The complete model comparison, synthesizing results from this chapter and Chapter~8, is:

\begin{center}
\begin{tabular}{llll}
\hline
Model & Info needed & Runtime & Communication \\
\hline
Circuit (D\"urr-H\o yer) & None & $\Theta(\sqrt{N/d_0})$ & 0 bits \\
Fixed AQO, informed & $A_1$ to $2^{-n/2}$ & $O(\sqrt{N/d_0})$ & $\Theta(n)$ bits \\
Fixed AQO, $C$ bits & $A_1$ to $2^{-C}$ & $T_{\mathrm{inf}} \cdot 2^{n/2-C}$ & $C$ bits \\
Fixed AQO, uninformed & None & $\Omega(N/\sqrt{d_0})$ & 0 bits \\
Adaptive AQO & $O(n)$ measurements & $O(\sqrt{N/d_0})$ & 0 bits \\
Constant-control, two-level & None & $\Theta(\sqrt{N/d_0})$ & 0 bits \\
Quantum $A_1$ estimation & $\varepsilon = 2^{-n/2}$ & $\Theta(2^{n/2})$ queries & --- \\
Classical $A_1$ estimation & $\varepsilon = 2^{-n/2}$ & $\Theta(2^n)$ queries & --- \\
\hline
\end{tabular}
\end{center}

\noindent The circuit model and the adaptive adiabatic model both achieve optimal performance with zero classical communication. The fixed adiabatic model traces a diagonal: each missing bit doubles the runtime. The $\Theta(n)$-bit gap between the circuit model and the fixed adiabatic model is exactly the information content of $A_1$ at the algorithmically relevant precision. The communication cost is a property of the computational model, not of the computational task.

For the running example ($N = 4$, $d_0 = 1$, $n = 2$): the circuit model uses $O(2)$ queries at $C = 0$; the informed adiabatic model uses $O(2)$ queries at $C = 1$ bit; the uninformed adiabatic model uses $\Omega(4)$ queries at $C = 0$. The one missing bit accounts for the factor-of-two gap.

The information gap is now resolved in all three of its meanings. The spectral gap determines the runtime: within the adiabatic framework, the minimum gap $g_{\min}$ sets the time scale $T = O(1/g_{\min})$, and the gap profile $g(s)$ determines how the schedule must be shaped. The gap in knowledge determines what runtime is achievable: with no knowledge of where $g_{\min}$ occurs, the runtime blows up by a factor of $(s_R - s_L)/\Delta_*$; with $\varepsilon$-precision knowledge, the overhead is $\Theta(\max(1, \varepsilon/\delta_{A_1}))$; with $O(n)$ quantum measurements, the overhead vanishes. And whether the gap in knowledge matters at all depends on the computational model: in the circuit model, the quantity $A_1$ is irrelevant and invisible; in the fixed-schedule adiabatic model, it is essential and NP-hard; in the adaptive adiabatic model, it is acquirable at polynomial cost.

The ignorance taxonomy has five levels, where the overhead is the multiplicative ratio $T/T_{\mathrm{inf}}$. Level 0 (no information): $\Omega(2^{n/2})$ overhead. Level 1 (precision $\varepsilon$): $\Theta(\max(1, \varepsilon/\delta_{A_1}))$ overhead. Level 2 (bounded interval $[u_L, u_R]$): constant overhead proportional to $u_R - u_L$. Level 3 (quantum measurement): $O(1)$ overhead with $O(n)$ measurements. Level 4 (circuit model): overhead $1$, no spectral information needed.

The adiabatic approach to unstructured search works, achieves the Grover speedup, and is optimal among all schedules. But its information requirements are a structural consequence of the rank-one interpolation path. These requirements are not a fundamental limitation of quantum computation --- they are a property of the adiabatic model. The next chapter translates these results into machine-checked formal proofs.
