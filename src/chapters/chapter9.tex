% Chapter 9: Information Gap
% ASSUMES: Chapter 5 defines H(s), H_z, H_0, |psi_0>, A_p, A_1, A_2,
%   s^*, delta_s, g_min, hat{g}, eigenvalue equation, spectral condition,
%   symmetric subspace, grover-gap.
% ASSUMES: Chapter 6 proves gap-left, gap-right, complete-profile, f(s*)=4, s_0,
%   c_L = A_1(A_1+1)/A_2, c_R = Delta/30, piecewise linear lower bound.
% ASSUMES: Chapter 7 defines runtime theorem, JRS bound, RC local condition,
%   p=2 schedule, integral T = (C/eps) int g^{-2} ds.
% ASSUMES: Chapter 8 proves NP-hardness (Thm 8.1), #P-hardness (Thm 8.2),
%   interpolation barrier (Thm 8.3), quantum algorithm (Thm 8.4),
%   classical lower bound (Thm 8.5), quadratic separation (Cor 8.1).

The adiabatic algorithm of \autoref{thm:aqo-runtime} achieves the Grover speedup $\widetilde{O}(\sqrt{N/d_0})$, but its schedule depends on $s^* = A_1/(A_1+1)$, whose computation is NP-hard (\autoref{thm:np-hard-A1}). In the circuit model, Grover's algorithm achieves the same speedup without computing any spectral parameter. The adiabatic framework demands the schedule be fixed before evolution begins. What runtime is achievable by an adiabatic algorithm that knows nothing about the problem Hamiltonian beyond its dimension?

This chapter follows one question through three lenses. At the physics level, runtime is set by the gap profile $g(s)$ along the interpolation path. At the information-theoretic level, the decisive unknown is where that profile reaches its minimum. At the complexity-theoretic level, we ask what resources are required to obtain that location and exploit it.


\section{The Cost of Ignorance}
\label{sec:cost-of-ignorance}

Throughout this chapter, asymptotic notation ($O$, $\Omega$, $\Theta$) refers to
the limit $N \to \infty$ (equivalently $n \to \infty$ with $N = 2^n$). Unless
stated otherwise, the spectral parameters $d_0$, $M$, $\Delta$, $A_1$, $A_2$
and the target error $\varepsilon$ are treated as fixed positive constants
independent of $n$. When we write ``$O(T_{\mathrm{inf}})$,'' the hidden constant
may depend on these spectral parameters but not on $n$.

A \emph{fixed schedule} is chosen before the instance is revealed. It may depend
on problem size $n$ and target error $\varepsilon$, but not on instance-specific
spectral data. An \emph{instance-independent} algorithm uses the same
Hamiltonian design for all energy assignments with the same degeneracy
structure.

The NP-hardness of $A_1$ is a statement about worst-case classical computation. It does not directly tell us how much runtime an adiabatic algorithm loses by not knowing $A_1$. If a fixed schedule that ignores $A_1$ still achieved $O(\sqrt{N/d_0})$, the hardness would be academic. It is not.

The separation between informed and uninformed schedules is a minimax result. It can be viewed as a two-player game in which the schedule designer moves first and an adversary then chooses a worst-case gap function. To formalize this, we need a gap class broad enough to let the adversary place the minimum anywhere in an uncertainty interval $[s_L, s_R]$.

\begin{definition}[Gap class]
\label{def:gap-class}
The gap class $\mathcal{G}(s_L, s_R, \Delta_*)$ consists of all gap functions $g: [0,1] \to \mathbb{R}_{>0}$ such that the minimum $g(s^*) = \Delta_*$ is achieved at a unique point $s^* \in [s_L, s_R]$, and $g(s) > \Delta_*$ for all $s \neq s^*$.
\end{definition}

\noindent For the running example ($M = 2$, $d_0 = 1$, $N = 4$), $s^* = A_1/(A_1+1) = 3/7$ and $\Delta_* = g_{\min} = 1/\sqrt{4} = 1/2$. Any gap function in $\mathcal{G}(0, 1, 1/2)$ has minimum value $1/2$ somewhere in $[0, 1]$. The adversary controls where that minimum sits.

\begin{definition}[RC-admissible fixed schedules]
\label{def:rc-admissible}
Fix a family of Hamiltonian instances. A fixed schedule $u$ with velocity profile $v(s)=|ds/dt|$ is \emph{RC-admissible} on an instance if it satisfies the local Roland-Cerf condition
\[
v(s) \leq \frac{\varepsilon\, g(s)^2}{\|H'(s)\|}\qquad\forall s\in[0,1].
\]
It is \emph{uniformly RC-admissible} on the family if this inequality holds for every instance in the family.
\end{definition}

\noindent The parameter $\Delta_*$ denotes the minimum of the abstract gap function $g$. It should not be confused with $\Delta = E_1 - E_0$ (the spectral gap of $H_z$) or with $g_{\min}$ (the minimum gap of the rank-one Hamiltonian $H(s)$). For rank-one gap profiles, $\Delta_* = g_{\min} = \Theta(\sqrt{d_0/(NA_2)})$.

The schedule induces a velocity profile $v(s) > 0$ on $[0,1]$, with total evolution time $T = \int_0^1 v(s)^{-1}\,ds$.

Within the uniformly RC-admissible class (Definition~\ref{def:rc-admissible}), the crossing velocity obeys a pointwise bound. Since $H'(s) = \ket{\psi_0}\!\bra{\psi_0} + H_z$ satisfies $\lVert H'(s) \rVert \leq \lVert \ket{\psi_0}\!\bra{\psi_0} \rVert + \lVert H_z \rVert \leq 1 + 1 = 2$ (using the eigenvalue normalization $E_{M-1} \leq 1$ from Chapter~5), RC-admissibility gives
\[
v(s) \leq \varepsilon\,g(s)^2/2.
\]
At a crossing point where $g=\Delta_*$, this yields $v \leq \varepsilon\,\Delta_*^2/2$.

The crossing window has width $\delta_s = \Theta(\Delta_*)$, so any valid
schedule must stay slow across that window. Chapter~5 gives
$\delta_s = \hat{g}/c_L$ (\autoref{eq:gmin-deltas-relation}), where
$\hat{g} = \frac{2A_1}{A_1+1}\sqrt{d_0/(NA_2)}$ and
$g_{\min} = (1 \pm O(\eta))\hat{g}$. Since $c_L = A_1(A_1+1)/A_2$ is fixed,
$\delta_s = \Theta(g_{\min}) = \Theta(\Delta_*)$. Define
$v_{\mathrm{slow}} = \varepsilon\,\Delta_*^2/2$ as the maximal crossing
velocity. In the ratio $T_{\mathrm{unf}}/T_{\mathrm{inf}}$, this velocity
cancels because both runtimes are computed under the same RC condition. The
separation therefore depends on the geometric factor $(s_R - s_L)/\Delta_*$.

\begin{lemma}[Adversarial gap construction]
\label{lem:adversarial-gap}
For any $s_{\mathrm{adv}} \in [s_L, s_R]$ and $\Delta_* > 0$, the gap function $g_{\mathrm{adv}}(s) = \Delta_* + (s - s_{\mathrm{adv}})^2$ belongs to $\mathcal{G}(s_L, s_R, \Delta_*)$.
\end{lemma}

\begin{proof}
The function satisfies $g_{\mathrm{adv}}(s_{\mathrm{adv}}) = \Delta_*$, $g_{\mathrm{adv}}(s) > \Delta_*$ for $s \neq s_{\mathrm{adv}}$, and $g_{\mathrm{adv}}(s) > 0$ for all $s$.
\end{proof}

\begin{lemma}[Velocity bound for uninformed schedules]
\label{lem:velocity-bound}
Let $u$ be a fixed schedule that is uniformly RC-admissible on the rank-one family in \autoref{thm:separation}. Then $v(s) \leq v_{\mathrm{slow}}$ for all $s \in [s_L, s_R]$, provided $N$ is sufficiently large that $\Delta_* < \min(1 - s_R, s_L)$.
\end{lemma}

\begin{proof}
Suppose $v(s') > v_{\mathrm{slow}}$ for some $s' \in [s_L, s_R]$. We construct a physical Hamiltonian in the rank-one family whose gap minimum occurs at $s'$, then apply RC-admissibility on that instance.

Since $s^* = A_1/(A_1+1)$ is a continuous, strictly increasing function of $A_1 \in (0,\infty)$ with range $(0,1)$, there exists $A_1$ placing the crossing at $s'$. On the two-level family with solution fraction $\rho = d_0/N$, the leading-order gap minimum is $\hat{g} = 2(1-s')\sqrt{\rho(1-\rho)}$ (\autoref{eq:gmin-formula}), so choosing $\rho$ to satisfy $\hat{g} = \Delta_*$ (feasible whenever $\Delta_* \leq 1 - s'$, which holds asymptotically since $\Delta_* = \Theta(2^{-n/2})$ and $s' \leq s_R < 1$) produces a problem Hamiltonian $H_z$ with crossing at $s'$ and $g_{\min} = \Theta(\Delta_*)$. The adversary in the minimax game selects this physical Hamiltonian, not merely an abstract gap function.

Uniform RC-admissibility on this Hamiltonian gives $v(s') \leq \varepsilon\,g_{\min}^2/\lVert H'(s') \rVert = \Theta(v_{\mathrm{slow}})$. After absorbing the fixed constant factor into $v_{\mathrm{slow}}$, this contradicts $v(s') > v_{\mathrm{slow}}$. Because $s'$ was arbitrary in $[s_L,s_R]$, the bound holds throughout the interval.
\end{proof}

\begin{theorem}[Separation (uniformly RC-admissible class)]
\label{thm:separation}
Let $T_{\mathrm{unf}}$ be the minimum time over all fixed schedules that are uniformly RC-admissible for all rank-one instances whose gap minima satisfy $s^* \in [s_L,s_R]$ and $g_{\min} = \Theta(\Delta_*)$, and let $T_{\mathrm{inf}}$ be the corresponding optimal informed runtime (with known $s^*$) on the same rank-one family. Then
\begin{equation}
\label{eq:separation-ratio}
\frac{T_{\mathrm{unf}}}{T_{\mathrm{inf}}} = \Omega\!\left(\frac{s_R - s_L}{\Delta_*}\right).
\end{equation}
\end{theorem}

\begin{proof}
By \autoref{lem:velocity-bound}, $v(s) \leq v_{\mathrm{slow}}$ for all $s \in [s_L, s_R]$. The uninformed time satisfies
\begin{equation}
T_{\mathrm{unf}} = \int_0^1 \frac{ds}{v(s)} \geq \int_{s_L}^{s_R} \frac{ds}{v(s)} \geq \frac{s_R - s_L}{v_{\mathrm{slow}}}.
\end{equation}
The informed schedule knows $s^*$ exactly and needs to be slow only in the crossing window of width $O(\Delta_*)$. For rank-one profiles, $\delta_s = \hat{g}/c_L = \Theta(g_{\min}) = \Theta(\Delta_*)$ by \autoref{eq:gmin-deltas-relation}, so \autoref{thm:aqo-runtime} gives $T_{\mathrm{inf}} = \Theta(\delta_s/v_{\mathrm{slow}}) = \Theta(\Delta_*/v_{\mathrm{slow}})$. The velocity factors cancel:
\begin{equation}
\frac{T_{\mathrm{unf}}}{T_{\mathrm{inf}}} = \Omega\!\left(\frac{s_R - s_L}{\Delta_*}\right). \qedhere
\end{equation}
\end{proof}

\begin{corollary}[Constant-width uncertainty family]
\label{cor:separation-grover}
For any $n$-qubit rank-one family in which the minimum gap scales as $\Delta_* = \Theta(2^{-n/2})$, the crossing uncertainty interval has constant width $s_R - s_L = \Theta(1)$, and the endpoints are bounded away from the boundaries ($\exists\,\gamma>0$ independent of $n$ with $\gamma \le s_L < s_R \le 1-\gamma$), the minimax separation satisfies
\[
\frac{T_{\mathrm{unf}}}{T_{\mathrm{inf}}} = \Omega(2^{n/2}).
\]
\end{corollary}

This corollary is a worst-case uncertainty statement. It does not apply to the standard Grover setting, where the crossing location is known a priori ($s^* = 1/2 + O(1/N)$). As a toy geometry with full uncertainty interval width $s_R - s_L = 1$ and $\Delta_* = 1/2$, the separation ratio is $(s_R - s_L)/\Delta_* = 2$. At $N=4$, that equals $\sqrt{N}$.

The logical consequence is simple. If classical preprocessing is polynomial-time,
NP-hardness forces a gap-uninformed model for fixed schedules. Inside the
uniformly RC-admissible class, that model has an $\Omega(2^{n/2})$ minimax
lower bound from the adversarial geometry of \autoref{lem:adversarial-gap}. The
penalty is therefore geometric, not a byproduct of the reduction alone.


\section{Partial Knowledge and Hedging}
\label{sec:partial-knowledge}

The separation theorem quantifies a worst case. If an adversary can place the gap minimum anywhere in $[s_L, s_R]$, the schedule must be uniformly slow. But NP-hardness does not imply that $A_1$ is completely unknown. The natural question is what partial knowledge is worth.

Suppose an algorithm has access to an estimate $A_{1,\mathrm{est}}$ satisfying $|A_{1,\mathrm{est}} - A_1| \leq \varepsilon$. The uncertainty propagates to the crossing position through the map $f(x) = x/(x+1)$, whose derivative is $f'(x) = 1/(x+1)^2$.

\begin{lemma}[$A_1$-to-$s^*$ precision propagation]
\label{lem:precision-propagation}
If $|A_{1,\mathrm{est}} - A_1| \leq \varepsilon$ with $|\varepsilon| \leq (1+A_1)/2$, then $|s^*_{\mathrm{est}} - s^*| \leq 2|\varepsilon|/(A_1+1)^2$.
\end{lemma}

\begin{proof}
Direct computation gives the exact identity
\begin{equation}
\label{eq:precision-exact}
s^*_{\mathrm{est}} - s^* = \frac{A_1 + \varepsilon}{1 + A_1 + \varepsilon} - \frac{A_1}{1 + A_1} = \frac{\varepsilon}{(1+A_1)(1+A_1+\varepsilon)}.
\end{equation}
Under $|\varepsilon| \leq (1+A_1)/2$, the denominator satisfies $1 + A_1 + \varepsilon \geq (1+A_1)/2$, so
\begin{equation}
|s^*_{\mathrm{est}} - s^*| \leq \frac{|\varepsilon|}{(1+A_1) \cdot (1+A_1)/2} = \frac{2|\varepsilon|}{(1+A_1)^2}. \qedhere
\end{equation}
\end{proof}

Given $A_1$ precision $\varepsilon$, \autoref{lem:precision-propagation} places
the true crossing within radius $2\varepsilon/(A_1+1)^2$ of the estimate. The
uncertainty interval therefore has width
$W(\varepsilon) = 4\varepsilon/(A_1+1)^2$. Define the $\varepsilon$-informed
gap class as $\mathcal{G}_\varepsilon = \mathcal{G}(s_L(\varepsilon),
s_R(\varepsilon), \Delta_*)$, where the endpoints come from this interval.
Applying \autoref{thm:separation} gives the lower bound. A matching upper bound
comes from a schedule that stays uniformly slow on this interval and fast
outside it.

\begin{theorem}[Interpolation]
\label{thm:interpolation}
For $A_1$ precision $\varepsilon$, the optimal adiabatic runtime satisfies
\begin{equation}
\label{eq:interpolation}
T(\varepsilon) = \Theta\!\left(T_{\mathrm{inf}} \cdot \max\!\left(1, \frac{\varepsilon}{\delta_{A_1}}\right)\right),
\end{equation}
where $\delta_{A_1} = 2\sqrt{d_0 A_2/N}$ is the precision threshold for optimality.
\end{theorem}

\begin{proof}
\emph{Lower bound.} For $\varepsilon \geq \delta_{A_1}$, \autoref{thm:separation} applied to $\mathcal{G}_\varepsilon$ gives $T(\varepsilon) \geq W(\varepsilon)/v_{\mathrm{slow}}$. Taking the ratio with $T_{\mathrm{inf}} = \Theta(\delta_s/v_{\mathrm{slow}})$ and using the identity
\begin{equation}
\label{eq:precision-identity}
(A_1+1)^2 \cdot \delta_s = (A_1+1)^2 \cdot \frac{2}{(A_1+1)^2}\sqrt{\frac{d_0 A_2}{N}} = 2\sqrt{\frac{d_0 A_2}{N}} = \delta_{A_1}
\end{equation}
yields $T(\varepsilon)/T_{\mathrm{inf}} \geq \Theta(\varepsilon/\delta_{A_1})$. For $\varepsilon < \delta_{A_1}$, the trivial bound $T(\varepsilon) \geq T_{\mathrm{inf}}$ holds regardless of precision.

\emph{Upper bound.} For $\varepsilon \geq \delta_{A_1}$, construct a schedule with crossing velocity $v_{\mathrm{slow}} = \Theta(\varepsilon_{\mathrm{ad}}\,\Delta_*^2)=\Theta(\Delta_*^2)$ throughout the uncertainty interval $[s_L(\varepsilon), s_R(\varepsilon)]$, where $\varepsilon_{\mathrm{ad}}$ is the fixed adiabatic error parameter from \autoref{def:rc-admissible}. Use fast velocity $v_{\mathrm{fast}} = O(1)$ outside this interval. The slow region has width $W(\varepsilon) = \Theta(\varepsilon/\delta_{A_1}) \cdot \delta_s$, so the total time is $T = W(\varepsilon)/v_{\mathrm{slow}} + O(1) = \Theta(T_{\mathrm{inf}} \cdot \varepsilon/\delta_{A_1})$, since $T_{\mathrm{inf}} = \Theta(\delta_s/v_{\mathrm{slow}})$. For $\varepsilon < \delta_{A_1}$, the optimal informed schedule achieves $T = O(T_{\mathrm{inf}})$.
\end{proof}

The interpolation is linear. There is no threshold, cliff, or phase transition. At precision $1/\mathrm{poly}(n)$ (NP-hard), the overhead is $\Theta(2^{n/2}/\mathrm{poly}(n))$, which is close to the full exponential penalty. At precision $2^{-n/2}$ (algorithmically relevant), the overhead is $\Theta(1)$. The region between these scales is the ``information gap.'' For the running example, the precision table is:

\begin{center}
\begin{tabular}{ll}
\hline
Precision $\varepsilon$ & $T(\varepsilon)/T_{\mathrm{inf}}$ \\
\hline
$2^{-n/2}$ & $\Theta(1)$ \\
$2^{-n/4}$ & $\Theta(2^{n/4})$ \\
$1/n$ & $\Theta(2^{n/2}/n)$ \\
$1/\mathrm{poly}(n)$ & $\Theta(2^{n/2}/\mathrm{poly}(n))$ \\
$1$ (no knowledge) & $\Theta(2^{n/2})$ \\
\hline
\end{tabular}
\end{center}

The interpolation theorem treats $A_1$ precision as a continuous resource. A
complementary operational question asks for the best fixed schedule when $s^*$
is only known to lie in an interval $[u_L, u_R]$. A hedging schedule spreads
slowdown across that full interval instead of concentrating it at one point. It
uses velocity $v_{\mathrm{slow}}$ on $[u_L, u_R]$ and $v_{\mathrm{fast}}$
outside, with normalization
$(u_R - u_L)/v_{\mathrm{slow}} + (1 - u_R + u_L)/v_{\mathrm{fast}} = 1$.

Write $w = u_R - u_L$ for the interval width. The JRS error functional uses
$\int \lVert H' \rVert^2 g^{-3}\,ds$. For the rank-one family,
$\lVert H'(s) \rVert = O(1)$, so the effective weight is $g^{-3}$ rather than
Roland-Cerf's $g^{-2}$. For a piecewise-constant velocity profile,
\autoref{eq:jrs-bound} gives a contribution proportional to
$v \cdot \int g^{-3}\,ds$ on each segment. The total error is
$v_{\mathrm{slow}} I_{\mathrm{slow}} + v_{\mathrm{fast}} I_{\mathrm{fast}}$,
where $I_{\mathrm{slow}} = \int_{u_L}^{u_R} g(u)^{-3}\,du$ and
$I_{\mathrm{fast}} = \int_{[0,1]\setminus[u_L,u_R]} g(u)^{-3}\,du$. Because
the crossing lies in the slow region, $I_{\mathrm{slow}} \gg I_{\mathrm{fast}}$.

\begin{theorem}[Hedging]
\label{thm:hedging}
Let $R = I_{\mathrm{slow}}/I_{\mathrm{fast}} \gg 1$. Under normalization $T = 1$, the optimal hedging schedule for interval $[u_L, u_R]$ achieves $\mathrm{Error}_{\mathrm{hedge}}/\mathrm{Error}_{\mathrm{uniform}} \to u_R - u_L$ as $R \to \infty$, with optimal slow velocity $v_{\mathrm{slow}} = w + \sqrt{(1-w)w/R}$.
\end{theorem}

\begin{proof}
Write $w = u_R - u_L$ for the interval width. The normalization constraint $w/v_{\mathrm{slow}} + (1-w)/v_{\mathrm{fast}} = 1$ fixes the total time $T = 1$, so the JRS error integral of \autoref{eq:jrs-bound} reduces to $E = v_{\mathrm{slow}} I_{\mathrm{slow}} + v_{\mathrm{fast}} I_{\mathrm{fast}}$. The constraint gives
\begin{equation}
v_{\mathrm{fast}} = \frac{(1-w)\,v_{\mathrm{slow}}}{v_{\mathrm{slow}} - w},
\end{equation}
valid for $v_{\mathrm{slow}} > w$. Substituting into the error gives
\begin{equation}
E(v_{\mathrm{slow}}) = v_{\mathrm{slow}}\, I_{\mathrm{slow}} + \frac{(1-w)\,v_{\mathrm{slow}}}{v_{\mathrm{slow}} - w}\, I_{\mathrm{fast}}.
\end{equation}
Differentiating with respect to $v_{\mathrm{slow}}$ and setting to zero:
\begin{equation}
\frac{dE}{dv_{\mathrm{slow}}} = I_{\mathrm{slow}} - \frac{(1-w)\,w\, I_{\mathrm{fast}}}{(v_{\mathrm{slow}} - w)^2} = 0.
\end{equation}
Solving yields $(v_{\mathrm{slow}} - w)^2 = (1-w)\,w\,I_{\mathrm{fast}}/I_{\mathrm{slow}} = (1-w)\,w/R$, so
\begin{equation}
v_{\mathrm{slow}} = w + \sqrt{(1-w)\,w/R}.
\end{equation}
At this optimum, $v_{\mathrm{fast}} = (1-w)\,v_{\mathrm{slow}}/\sqrt{(1-w)\,w/R} = \sqrt{R\,w\,(1-w)} + (1-w)$. The optimal error, substituting $v_{\mathrm{slow}} - w = \sqrt{(1-w)\,w/R}$, is
\begin{equation}
E_{\mathrm{opt}} = \bigl(w + \sqrt{(1-w)\,w/R}\bigr)\,I_{\mathrm{slow}} + \bigl(\sqrt{R\,w\,(1-w)} + (1-w)\bigr)\,I_{\mathrm{fast}}.
\end{equation}
Since $R = I_{\mathrm{slow}}/I_{\mathrm{fast}} \gg 1$, the terms involving $\sqrt{R}$ contribute $2\sqrt{w\,(1-w)\,I_{\mathrm{slow}}\,I_{\mathrm{fast}}} = o(I_{\mathrm{slow}})$, and the dominant term is $w\,I_{\mathrm{slow}}$, while the uniform error is $E_{\mathrm{unif}} = I_{\mathrm{slow}} + I_{\mathrm{fast}} \approx I_{\mathrm{slow}}$. Therefore $E_{\mathrm{opt}}/E_{\mathrm{unif}} \to w = u_R - u_L$ as $R \to \infty$.
\end{proof}

For an uncertainty interval $[0.4, 0.8]$, the hedging schedule achieves
$E_{\mathrm{opt}}/E_{\mathrm{unif}} = 0.4$ compared to a uniform schedule at the
same total runtime. That is a $60\%$ reduction in transition probability.
Bounded uncertainty about $s^*$ therefore yields a constant-factor improvement
proportional to interval width, not an exponential overhead. In the taxonomy of
\autoref{sec:complexity-landscape}, this is Level~2.


\section{Quantum Bypass}
\label{sec:quantum-bypass}

The separation theorem and the interpolation theorem characterize the cost of
ignorance within the fixed-schedule model. An adiabatic device, however, is a
physical system that can be measured during execution. The original
paper~\cite{braida2024unstructured} asks a concrete question: ``Can this
limitation be overcome when one only has access to a device operating in the
adiabatic setting?''

The answer is yes in an adaptive measurement model. Han, Park, and
Choi~\cite{HanParkChoi2025} independently proposed a constant geometric speed
(CGS) schedule that traverses the eigenstate path at uniform arc length, using
on-the-fly overlap estimates from quantum Zeno Monte Carlo to adjust the
velocity. Their method improves the gap scaling from $O(\Delta_*^{-2})$ to
$O(\Delta_*^{-1})$ and numerically preserves the quadratic speedup for adiabatic
unstructured search without prior spectral knowledge. The binary-search protocol
below uses a different mechanism. It first locates the crossing through branch
probes and then runs the informed schedule. In the ideal decision-probe model,
it achieves $O(T_{\mathrm{inf}})$.

NP-hardness bundles two different tasks. \emph{Computing} $s^*$ from the
classical description of $H_z$ is hard. \emph{Detecting} $s^*$ by probing the
quantum system $H(s)$ at selected parameter values can be efficient. We model
this with a binary decision-probe oracle. A probe at parameter value $s$
returns whether $s$ is at-or-left of the true crossing or to its right (ties go
left), and each probe costs $O(1/g(s))$.

\begin{definition}[Binary decision probe]
\label{def:binary-probe}
For an instance $H$, let $D_H:[0,1]\to\{0,1\}$ be the idealized oracle
\[
D_H(s)=
\begin{cases}
0,& s \le s^*(H),\\
1,& s > s^*(H),
\end{cases}
\]
with probe cost $O(1/g_H(s))$.
\end{definition}

\begin{definition}[Adaptive adiabatic protocol]
\label{def:adaptive-protocol}
The protocol operates in two phases.

\emph{Phase 1 (Location).} Initialize $s_{\mathrm{lo}} = 0$, $s_{\mathrm{hi}} = 1$. For $i = 1, \ldots, \lceil n/2 \rceil$:
\begin{enumerate}
\item Set $s_{\mathrm{mid}} = (s_{\mathrm{lo}} + s_{\mathrm{hi}})/2$.
\item Query $D_H(s_{\mathrm{mid}})$.
\item If $D_H(s_{\mathrm{mid}})=0$ (at or left of crossing), set $s_{\mathrm{lo}} = s_{\mathrm{mid}}$.
\item If $D_H(s_{\mathrm{mid}})=1$ (right of crossing), set $s_{\mathrm{hi}} = s_{\mathrm{mid}}$.
\end{enumerate}
After $\lceil n/2 \rceil$ iterations, $s^*$ is located to precision $O(2^{-n/2})$.

\emph{Phase 2 (Execution).} Reset the state to $\ket{\psi_0}$. Evolve from $s = 0$ to $s = 1$ using the informed local schedule of \autoref{thm:aqo-runtime}, with the crossing position estimated in Phase~1.
\end{definition}

\begin{lemma}[Decision-probe cost]
\label{lem:phase-estimation-cost}
Each call to $D_H(s_{\mathrm{mid}})$ costs $O(1/g(s_{\mathrm{mid}}))$.
\end{lemma}

\begin{proof}
Immediate from Definition~\ref{def:binary-probe}.
\end{proof}

\begin{lemma}[Phase 1 cost]
\label{lem:phase1-cost}
The total time for Phase 1 is $O(T_{\mathrm{inf}})$.
\end{lemma}

\begin{proof}
Let $d_i = |s_{\mathrm{mid},i} - s^*|$ be the distance from the $i$-th midpoint to the true crossing. Chapter~6 gives two complementary bounds. Outside the crossing window ($|s - s^*| > \delta_s$), the gap satisfies
\[
g(s) \geq c_{\min} |s - s^*|,
\]
where $c_{\min} = \min(c_L, c_R)$ with $c_L = A_1(A_1+1)/A_2$ and $c_R = \Delta/30$. Both constants are positive and independent of $n$. Inside the crossing window ($|s - s^*| \leq \delta_s$), the gap satisfies $g(s) \geq g_{\min}$. Because $g_{\min}$ is the global minimum, these two bounds combine to
\begin{equation}
\label{eq:gap-lower-bound}
g(s_{\mathrm{mid},i}) \geq \max(g_{\min},\; c_{\min} \cdot d_i).
\end{equation}
The probe cost at iteration $i$ is therefore
\begin{equation}
\label{eq:pe-cost}
O\!\left(\frac{1}{g(s_{\mathrm{mid},i})}\right) \leq O\!\left(\min\!\left(\frac{1}{g_{\min}},\; \frac{1}{c_{\min} \cdot d_i}\right)\right).
\end{equation}
The two bounds in~\eqref{eq:pe-cost} cross at $d_i = g_{\min}/c_{\min}$. Since $g_{\min} = c_L \delta_s \cdot (1 - O(\eta))$ and $c_{\min} \leq c_L$, the crossover distance satisfies
\[
d_{\mathrm{cross}} = g_{\min}/c_{\min} = (c_L/c_{\min})\,\delta_s \geq \delta_s.
\]
The ratio $c_L/c_{\min}$ is a positive constant independent of $n$ because $c_L = A_1(A_1+1)/A_2$ and $c_R = \Delta/30$ are fixed by spectral parameters. Therefore $c_{\min} = \min(c_L, c_R) > 0$ is also fixed. At most $O(\log(c_L/c_{\min}) + 1) = O(1)$ binary-search midpoints can fall in the near regime $d_i \leq d_{\mathrm{cross}}$.

Group the $\lceil n/2 \rceil$ iterations by the distance $d_i$ in dyadic shells $S_j = [2^{-j-1}, 2^{-j}]$. Let $L_i = 2^{-i+1}$ be the binary-search interval width at step $i$. Since the next interval is the half containing $s^*$, the midpoint distances obey
\[
d_{i+1} = \left|d_i - \frac{L_i}{4}\right| = \left|d_i - 2^{-i-1}\right|.
\]
This recurrence gives $O(1)$ occupancy per shell: for $i > j+1$, $d_i \leq L_i/2 = 2^{-i} < 2^{-j-1}$, so $d_i \notin S_j$; and for $i \leq j$, if $d_i \in (2^{-j-1}, 2^{-j})$, then $d_{i+1} \notin (2^{-j-1}, 2^{-j})$. Thus the shell interior is visited at most once, with only a dyadic-rational edge case where the boundary value $2^{-j-1}$ can appear twice consecutively. Hence each shell contributes $O(1)$ midpoint queries.

\emph{Far shells} ($j < \log_2(1/\delta_s) \approx n/2$): here $d_i > \delta_s$, so the binding bound in~\eqref{eq:pe-cost} is $O(1/(c_{\min} \cdot d_i)) = O(2^j/c_{\min})$, where $c_{\min}$ enters the implicit constant.

\emph{Near shells} ($j \geq n/2$): here $d_i \leq \delta_s$, so the binding bound is $O(1/g_{\min}) = O(1/\Delta_*) = O(2^{n/2})$.

There are $O(1)$ near shells because at most $O(1)$ midpoints can satisfy $d_i \leq d_{\mathrm{cross}}$ in a binary search. The total cost is
\begin{equation}
\sum_{j=0}^{n/2-1} O(1) \cdot O(2^j) + O(1) \cdot O(2^{n/2}) = O(2^{n/2}) + O(2^{n/2}) = O(2^{n/2}) = O(T_{\mathrm{inf}}).
\end{equation}
The state preparation cost is $O(n)$ per iteration and $O(n)$ iterations, giving $O(n^2) = o(T_{\mathrm{inf}})$.
\end{proof}

\begin{theorem}[Adaptive adiabatic optimality in the decision-probe model]
\label{thm:adaptive}
The adaptive protocol of \autoref{def:adaptive-protocol} achieves runtime $T_{\mathrm{adapt}} = O(T_{\mathrm{inf}})$ with $\Theta(n)$ measurements.
\end{theorem}

\begin{proof}
Phase~1 locates $s^*$ to precision $O(2^{-n/2}) = O(\delta_s)$ using total time $O(T_{\mathrm{inf}})$ by \autoref{lem:phase1-cost}. This precision is within the crossing window width $\delta_s = O(\Delta_*)$. Phase~2 has time $O(T_{\mathrm{inf}})$ by \autoref{thm:aqo-runtime}, since the estimate of $s^*$ is accurate to $O(\delta_s)$. The total is $O(T_{\mathrm{inf}}) + O(T_{\mathrm{inf}}) = O(T_{\mathrm{inf}})$.
\end{proof}

\begin{theorem}[Measurement lower bound]
\label{thm:measurement-lower}
Any adaptive algorithm in the binary decision-probe model (each measurement returns one bit indicating the ground/excited branch at the probe point) achieving $T = O(T_{\mathrm{inf}})$ requires $\Omega(n)$ measurements.
\end{theorem}

\begin{proof}
The crossing position $s^*$ can lie anywhere in an interval of width $\Theta(1)$. To achieve the informed runtime, the algorithm must locate $s^*$ to precision $\delta_s = O(2^{-n/2})$, since any larger uncertainty incurs the overhead of \autoref{thm:interpolation}. This means distinguishing among $\Omega(2^{n/2})$ possible positions. In the binary decision-probe model, each measurement contributes at most one bit by definition. The information needed is $\log_2(2^{n/2}) = n/2$ bits, requiring $\Omega(n)$ measurements.
\end{proof}

The three adiabatic regimes are:

\begin{center}
\begin{tabular}{lll}
\hline
Strategy & Runtime & Measurements \\
\hline
Fixed, uninformed & $\Omega(2^{n/2} \cdot T_{\mathrm{inf}})$ & 0 \\
Adaptive & $O(T_{\mathrm{inf}})$ & $\Theta(n)$ \\
Fixed, informed & $O(T_{\mathrm{inf}})$ & 0 \\
\hline
\end{tabular}
\end{center}

\noindent For the running example ($N = 4$, $d_0 = 1$, $n = 2$), Phase~1 performs $\lceil 1 \rceil = 1$ probe at $s_{\mathrm{mid}} = 0.5$, so the location stage already achieves width $1/2 = O(2^{-n/2})$. The gap at that point is $g(0.5)=1/\sqrt{N}=1/2$, which gives probe cost $O(1/g)=O(2)=O(T_{\mathrm{inf}})$.

\noindent \emph{Implementation note.} A physical instantiation of $D_H$ can be attempted via phase-estimation-based branch tests on $H(s_{\mathrm{mid}})$. The theorem above is intentionally stated in the ideal decision-probe model; a full finite-sample reliability analysis for this instantiation is separate from the present minimax argument.

The adaptive protocol acquires $A_1$ through measurement, while the circuit model bypasses $A_1$ entirely. The D\"urr-H\o yer minimum-finding algorithm~\cite{durr1996quantum} achieves $\Theta(\sqrt{N/d_0})$ by maintaining a threshold and iteratively lowering it with Grover search. It never traverses an adiabatic path and never encounters an avoided crossing. Its mechanism is amplitude amplification with iterative thresholding and no spectral inputs. It needs no $A_1$, no $s^*$, no $\Delta$, and no gap profile.

\begin{proposition}[$A_1$-blindness]
\label{prop:A1-blindness}
Let $X_{\mathrm{DH}}$ denote the output of the amplified D\"urr-H\o yer algorithm (with $r = \Theta(n)$ repetitions). Then $I(X_{\mathrm{DH}};\,A_1 \mid S_0, E_0) \leq 2^{-\Omega(n)}$. Conditioned on success ($X_{\mathrm{DH}} \in S_0$), the mutual information is exactly zero.
\end{proposition}

\begin{proof}
Two problem Hamiltonians $H_z, H_z'$ are ground-equivalent if they share the same ground energy $E_0$ and ground space $S_0$. By symmetry of Grover's algorithm applied to the uniform initial state, the output distribution conditioned on success is $\mathrm{Uniform}(S_0)$, regardless of the excited spectrum. Since $A_1$ depends only on the excited spectrum (via $\{d_k, E_k\}_{k \geq 1}$), we have
\[
I(X_{\mathrm{DH}};\,A_1 \mid \text{success}, S_0, E_0) = 0.
\]
Let $F$ be the failure indicator of the amplified routine, and fix any prior over ground-equivalent instances conditioned on $(S_0,E_0)$. With $r = \Theta(n)$ repetitions using Boyer-Brassard-H\o yer-Tapp amplification~\cite{BBHT1998}, the per-trial success probability is at least $2/3$, so
\[
p_f := \Pr[F=1] \leq (1/3)^r = 2^{-\Omega(n)}.
\]
Using chain rule and the fact that $F$ is a function of $X_{\mathrm{DH}}$:
\begin{align}
I(X_{\mathrm{DH}};\,A_1 \mid S_0,E_0)
&\le I(X_{\mathrm{DH}},F;\,A_1 \mid S_0,E_0) \nonumber\\
&= I(F;\,A_1 \mid S_0,E_0) + I(X_{\mathrm{DH}};\,A_1 \mid F,S_0,E_0) \nonumber\\
&= I(F;\,A_1 \mid S_0,E_0) + p_f\, I(X_{\mathrm{DH}};\,A_1 \mid F=1,S_0,E_0) \nonumber\\
&\le H(F) + p_f\, H(X_{\mathrm{DH}} \mid F=1,S_0,E_0). \label{eq:A1-blindness-mi}
\end{align}
The $F=0$ term vanishes by the conditional independence established above. Now $H(F) \le h_2(p_f)$, where $h_2(p) = -p\log p - (1-p)\log(1-p)$ is the binary entropy, and $H(X_{\mathrm{DH}} \mid F=1,S_0,E_0) \le \log N = n$, so Eq.~\eqref{eq:A1-blindness-mi} gives
\[
I(X_{\mathrm{DH}};\,A_1 \mid S_0,E_0) \le h_2(p_f) + p_f n = 2^{-\Omega(n)}.
\]
\end{proof}

The circuit model does not merely avoid computing $A_1$. It is provably blind
to it. The fixed adiabatic model, by contrast, both requires and leaks
information about $A_1$. A schedule tuned to one value of $A_1$ performs poorly
on ground-equivalent instances with different excited spectra. The adaptive
adiabatic model lies between these extremes. It acquires $A_1$ through $O(n)$
measurements and pays $O(T_{\mathrm{inf}})$ to do so. The three models
therefore form a clear hierarchy of spectral information usage, from blindness
(circuit) through acquisition (adaptive) to fixed dependence (non-adaptive
adiabatic).

The adaptive protocol relies on the rank-one gap profile, which grows linearly away from the crossing ($\alpha = 1$). That linear growth is exactly what makes binary search informative: at distance $d$ from the crossing, the gap is $\Theta(d)$, so a probe costs $O(1/d)$ and the geometric sum converges. The next question is what changes when the gap approaches its minimum more gently.


\section{Gap Geometry and Schedule Optimality}
\label{sec:gap-geometry}

The flatness exponent $\alpha$ parametrizes how the gap approaches its minimum.
Outside the crossing window, write $g(s) \approx c|s - s^*|^\alpha$. For
rank-one profiles, $\alpha = 1$, and runtime scales as $O(1/\Delta_*)$. Flatter
profiles with $\alpha > 1$ are worse.

Guo and An~\cite{GuoAn2025} identified a measure condition that guarantees
$O(1/g_{\min})$ for the $p = 3/2$ power-law schedule. Here we prove the
complementary statement. When $\alpha > 1$, that condition fails, and the same
variationally optimal schedule degrades from $T = O(1/\Delta_*)$ to
$T = O(1/\Delta_*^{3-2/\alpha})$.

Whether some non-power-law fixed schedule can still recover $O(1/\Delta_*)$ in
this regime remains open. The constant geometric speed approach of Han, Park,
and Choi~\cite{HanParkChoi2025} reaches $O(1/g_{\min})$ by adaptive gap
measurements, but that method uses runtime feedback and is not fixed. For fixed
non-adaptive schedules, no family is known to beat
$O(1/\Delta_*^{3-2/\alpha})$ when $\alpha > 1$.

Consider a gap function with flatness exponent $\alpha > 0$. Near the minimum, write $g(s) = \Delta_* + c|s - s^*|^\alpha$ for some constant $c > 0$. The measure condition asks for $\mu(\{s : g(s) \leq x\}) \leq Cx$ for all $x > 0$, with $C$ independent of $\Delta_*$.

\begin{theorem}[Geometric characterization]
\label{thm:geometric-char}
The measure condition holds with $C$ independent of $\Delta_*$ if and only if $\alpha \leq 1$.
\end{theorem}

\begin{proof}
For $x \geq \Delta_*$, the sublevel set $\{s : g(s) \leq x\}$ near $s^*$ has measure $\mu = 2((x - \Delta_*)/c)^{1/\alpha}$.

\emph{Case $\alpha \leq 1$.} The ratio $\mu/x = 2((x - \Delta_*)/c)^{1/\alpha}/x$ is increasing in $x$ when $\alpha \leq 1$. Differentiating gives
\[
\frac{d(\mu/x)}{dx}
= \frac{2}{c^{1/\alpha}\alpha x^2}
\left(\frac{x - \Delta_*}{c}\right)^{1/\alpha - 1}
\left(\left(\frac{1}{\alpha} - 1\right)(x - \Delta_*) + \frac{\Delta_*}{\alpha}\right),
\]
and both terms in the last parentheses are nonnegative because $1/\alpha - 1 \geq 0$ and $\Delta_* > 0$. Now $\mu$ is capped by $1$ (the measure of $[0,1]$), and the cap is reached at $x_{\mathrm{cap}} = \Delta_* + c(1/2)^\alpha$, where the sublevel set spans $[0,1]$. For $x > x_{\mathrm{cap}}$, we have $\mu/x = 1/x < 1/x_{\mathrm{cap}}$. Hence
\[
C = \sup_{x > 0} \frac{\mu}{x} \leq \frac{1}{x_{\mathrm{cap}}} \leq \frac{2^\alpha}{c},
\]
independent of $\Delta_*$.

\emph{Case $\alpha > 1$.} At $x = 2\Delta_*$, the ratio is $\mu/x = 2(\Delta_*/c)^{1/\alpha}/(2\Delta_*) = c^{-1/\alpha}\Delta_*^{1/\alpha - 1}$. Since $1/\alpha - 1 < 0$, this diverges as $\Delta_* \to 0$. No finite $C$ works for all $\Delta_*$.
\end{proof}

The gap integral $\int_0^1 g(s)^{-\beta}\,ds$ controls the runtime for power-law schedules. A substitution $u = c|s - s^*|^\alpha/\Delta_*$ gives the following scaling.

\begin{lemma}[Gap integral]
\label{lem:gap-integral}
For $\beta > 1/\alpha$,
\begin{equation}
\label{eq:gap-integral}
\int_0^1 g(s)^{-\beta}\,ds = \Theta(\Delta_*^{1/\alpha - \beta}).
\end{equation}
For $\beta = 1/\alpha$, the integral is $\Theta(\log(1/\Delta_*))$. For $\beta < 1/\alpha$, the integral is $\Theta(1)$.
\end{lemma}

\begin{proof}
The substitution $u = (c|s-s^*|^\alpha)/\Delta_*$ transforms the near-minimum contribution to
\[
\Delta_*^{1/\alpha-\beta}\int_0^{U} u^{1/\alpha-1}(1+u)^{-\beta}\,du,
\]
where $U=\Theta(1/\Delta_*)$. As $u\to\infty$, the integrand behaves like $u^{1/\alpha-1-\beta}$.

If $\beta>1/\alpha$, the exponent is strictly less than $-1$, so the $u$-integral converges to a finite constant, yielding $\Theta(\Delta_*^{1/\alpha-\beta})$.

If $\beta=1/\alpha$, the integrand is asymptotically $u^{-1}$, so the integral contributes $\Theta(\log U)=\Theta(\log(1/\Delta_*))$.

If $\beta<1/\alpha$, the integral grows as $U^{1/\alpha-\beta}$, which cancels the prefactor $\Delta_*^{1/\alpha-\beta}$, giving $\Theta(1)$.

The contribution from outside a neighborhood of $s^*$ is always $O(1)$. For fixed $\delta$, if $|s-s^*|\ge\delta$ then $g(s)\ge g_0>0$ independently of $\Delta_*$, so $\int_{|s-s^*|\ge\delta} g(s)^{-\beta}ds \le g_0^{-\beta}$.
\end{proof}

\begin{theorem}[Scaling spectrum]
\label{thm:scaling-spectrum}
For a gap function with flatness exponent $\alpha > 2/3$, the adiabatic runtime with the $p = 3/2$ power-law schedule (variationally optimal in the JRS framework~\cite{GuoAn2025}) satisfies
\begin{equation}
\label{eq:scaling-spectrum}
T = \Theta(1/\Delta_*^{3 - 2/\alpha}).
\end{equation}
\end{theorem}

\begin{proof}
The power-law schedule $u'(s) = c_p\,g(u(s))^p$ has normalization constant $c_p = \int_0^1 g(v)^{-p}\,dv$. The JRS error functional becomes
\begin{equation}
\eta \leq \frac{1}{T}\,c_p \int_0^1 g(v)^{p-3}\,dv.
\end{equation}
By \autoref{lem:gap-integral}, $c_p = \Theta(\Delta_*^{1/\alpha - p})$ (requiring $p > 1/\alpha$) and the second integral is $\Theta(\Delta_*^{1/\alpha + p - 3})$ (requiring $3 - p > 1/\alpha$). Together these require $1/\alpha < p < 3 - 1/\alpha$, an interval of width $3 - 2/\alpha$, which is positive if and only if $\alpha > 2/3$. The symmetric choice $p = 3/2$ lies in this interval for all $\alpha > 2/3$. Their product is
\begin{equation}
c_p \int g^{p-3}\,dv = \Theta(\Delta_*^{(1/\alpha - p) + (1/\alpha + p - 3)}) = \Theta(\Delta_*^{2/\alpha - 3}).
\end{equation}
Setting $\eta = O(1)$ gives $T = \Omega(\Delta_*^{-(3 - 2/\alpha)}) = \Omega(1/\Delta_*^{3-2/\alpha})$. The $p = 3/2$ power-law schedule achieves this scaling, giving a matching upper bound $T = O(1/\Delta_*^{3-2/\alpha})$.
\end{proof}

\begin{center}
\begin{tabular}{llll}
\hline
$\alpha$ & Exponent $\gamma = 3 - 2/\alpha$ & Measure condition & Runtime \\
\hline
$1$ & $1$ & Holds & $\Theta(1/\Delta_*)$ \\
$2$ & $2$ & Fails & $\Theta(1/\Delta_*^2)$ \\
$3$ & $7/3$ & Fails & $\Theta(1/\Delta_*^{7/3})$ \\
$\infty$ & $3$ & Fails & $\Theta(1/\Delta_*^3)$ \\
\hline
\end{tabular}
\end{center}

The runtime exponents form a continuous spectrum from $1$ (V-shaped minimum, best case) to $3$ (flat minimum, worst case), refuting any binary dichotomy between ``easy'' and ``hard'' gap profiles. For the running example ($M = 2$, $d_0 = 1$, $N = 4$), $\alpha = 1$ and $\gamma = 1$, confirming the optimal $T = \Theta(1/\Delta_*)$ scaling.

\begin{remark}
The exponent $\gamma = 3$ at $\alpha = \infty$ reflects the $p = 3/2$ power-law schedule, which is variationally optimal within the JRS error functional but not universally optimal across all adiabatic bounds. The Roland-Cerf schedule ($p = 2$) gives $T = O(1/\Delta_*^2)$ at $\alpha = \infty$ via a tighter adiabatic condition for flat gaps. The table shows the scaling of the JRS-optimal schedule as the gap flattens; different schedule families and adiabatic bounds produce different exponent curves.
\end{remark}

\begin{proposition}[Structural $\alpha = 1$]
\label{prop:structural-alpha}
For the rank-one Hamiltonian $H(s) = -(1-s)\ket{\psi_0}\!\bra{\psi_0} + sH_z$ with $d_1 \geq 1$ and $\Delta > 0$, the flatness exponent is $\alpha = 1$.
\end{proposition}

\begin{proof}
Near $s^*$, the two lowest eigenvalues form an avoided crossing described by the standard formula $g(s) = \sqrt{g_{\min}^2 + c_L^2(s - s^*)^2}$. For $|s - s^*| \gg g_{\min}/c_L = \delta_s$, the gap grows linearly: $g(s) \approx c_L|s - s^*|$. The crossing is simple (not higher-order) because the coupling between the two lowest branches is proportional to $|\!\braket{\psi_0}{\phi_1}\!|^2 = d_1/N > 0$, where $\ket{\phi_1}$ is the symmetric state of the first excited level. A higher-order crossing ($\alpha > 1$) would require this coupling to vanish, which cannot happen when $d_1 > 0$.
\end{proof}

No choice of $H_z$ with $d_1 > 0$ and $\Delta > 0$ can produce $\alpha \neq 1$.
To get different values of $\alpha$, one must change the interpolation scheme.
Examples include quantum phase transitions with $H(s)$ nonlinear in $s$ or
systems with symmetry-enforced higher-order crossings. This structural
$\alpha = 1$ explains why both the Roland-Cerf analysis and the Guo-An
framework achieve the same asymptotic runtime.

Braida et al.~\cite{braida2024unstructured} and Guo and An~\cite{GuoAn2025} are independent works on the same problem class. The former provides the spectral analysis ($A_1$, $s^*$, piecewise gap bounds), while the latter provides the variational optimization (power-law schedule, measure condition).

\begin{theorem}[Measure condition for the rank-one gap profile]
\label{thm:measure-paper}
Under the spectral condition of Chapter~5, the piecewise-linear gap profile satisfies the measure condition with
\begin{equation}
\label{eq:measure-constant}
C \leq \frac{3A_2}{A_1(A_1+1)} + \frac{30(1 - s_0)}{\Delta},
\end{equation}
where $s_0$ is the right-arm basepoint defined in Chapter~6.
\end{theorem}

\begin{proof}
Fix $x > 0$. For $x < g_{\min}$, the sublevel set is empty. For $x \geq g_{\min}$, bound each region of the piecewise gap profile separately.

The left arm satisfies $g(s) \geq c_L(s^* - s)$ and contributes at most $x/c_L$. The crossing window has width $2\delta_s = 2\hat{g}/c_L$. If $x \geq \hat{g}$, this contributes at most $2x/c_L$. If $g_{\min} \leq x < \hat{g}$, use $g_{\min} \geq (1-2\eta)\hat{g}$ and $x \geq g_{\min}$ to get $\hat{g} \leq x/(1-2\eta)$. For $\eta \leq 1/6$, this implies $\hat{g} \leq 3x/2$, so the window contribution is at most $2\hat{g}/c_L \leq 3x/c_L$. The condition $\eta \leq 1/6$ holds asymptotically because $\eta = O(\sqrt{d_0/(NA_2)}) \to 0$.

The right arm satisfies $g(s) \geq c_R(s - s_0)/(1 - s_0)$ and contributes at most $x \cdot 30(1-s_0)/\Delta$. Adding the three contributions and substituting $c_L = A_1(A_1+1)/A_2$ gives the stated bound.
\end{proof}

\begin{corollary}[Grover measure constant]
\label{cor:grover-C}
For Grover ($M = 2$, $d_0 = 1$, $d_1 = N - 1$, $E_0 = 0$, $E_1 = 1$), the exact measure constant is $C = 1$.
\end{corollary}

\begin{proof}
The exact gap is $g(s)^2 = (2s-1)^2(1 - 1/N) + 1/N$. Solving $g(s) \leq x$ gives
\[
\mu(\{g \leq x\}) = \sqrt{\frac{Nx^2 - 1}{N-1}}
\]
for $x \in [1/\sqrt{N}, 1]$, with $\mu = 1$ for $x > 1$. The ratio $\mu/x$ is increasing on $[1/\sqrt{N}, 1]$ and equals $1$ at $x = 1$.
\end{proof}

For the Grover problem, the exact gap integral is $\int_0^1 g(s)^{-2}\,ds = (N/\sqrt{N-1})\arctan\sqrt{N-1} \to (\pi/2)\sqrt{N}$ as $N \to \infty$. This closed-form evaluation confirms the $O(\sqrt{N})$ runtime from the piecewise analysis and provides the exact constant. For the running example ($N = 4$), $\int_0^1 g(s)^{-2}\,ds = (4/\sqrt{3})\arctan\sqrt{3} = 4\pi/(3\sqrt{3}) \approx 2.42$, consistent with the runtime $T_{\mathrm{inf}} = O(\sqrt{4}) = O(2)$.

Both the Roland-Cerf $p = 2$ schedule and Guo-An's $p = 3/2$ schedule achieve
the same asymptotic runtime
$T = O(\sqrt{N/d_0}/\varepsilon)$. As declared at the start of this chapter, all
spectral parameters $A_1$, $A_2$, and $\Delta$ are absorbed into the implicit
constant. The RC runtime involves the integral
$I = \int_0^1 g(s)^{-2}\,ds$, while Guo-An's bound involves $C^2/g_{\min}$.

\begin{theorem}[Constant comparison]
\label{thm:constant-comparison}
Write $a = 3/c_L$ and $r = 30(1-s_0)/\Delta$. Then $C^2 < I$ if and only if $(c_L - 1)r^2 - 2ar + a(1-a) > 0$. In the right-arm-dominated regime ($r \gg a$) with $c_L > 1$, this holds, with $C^2/I \to 1/c_L = A_2/(A_1(A_1+1))$.
\end{theorem}

\begin{proof}
With $C = a + r$ and $I = a + r^2 c_L$, we obtain
\[
I - C^2 = (c_L - 1)r^2 - 2ar + a(1-a).
\]
For $c_L > 1$ and $r \gg a$, the leading term $(c_L - 1)r^2$ dominates.
\end{proof}

\begin{remark}
The framework comparison extends across gap geometries. For $\alpha < 1$, the Roland-Cerf integral $\int g^{-2}\,ds = \Theta(g_{\min}^{1/\alpha - 2})$ grows slower than $1/g_{\min}$, making the RC analysis tighter. For $\alpha = 1$, both give $\Theta(1/g_{\min})$, and the JRS constant $C^2$ can be smaller than the RC integral $I$ (\autoref{thm:constant-comparison}). For $\alpha > 1$, the measure constant $C \to \infty$ as $g_{\min} \to 0$, so the JRS framework degrades and only the RC analysis applies. The structural $\alpha = 1$ (\autoref{prop:structural-alpha}) sits at the exact boundary where both frameworks are valid and neither uniformly dominates.
\end{remark}

For the Grover problem, $c_L \to 2$ as $N \to \infty$. Using the exact values $C_{\mathrm{exact}} = 1$ and $I_{\mathrm{exact}} \to (\pi/2)\sqrt{N}$ gives $C^2/I \to 2/(\pi\sqrt{N}) \to 0$, so the JRS certification is asymptotically tighter. This comparison is unusually explicit because the Grover gap has a closed form.

For structured Hamiltonians with richer spectra, exact constants are rarely
analytic, and \autoref{thm:constant-comparison} becomes the practical tool. For
example, evaluate it on the open ferromagnetic Ising chain from
\autoref{eq:Ising-Ham} with nearest-neighbor coupling $J = 1$, uniform field
$h = 1$, and $n = 10$ spins. This gives $C^2/I = 0.71$. The JRS advantage
remains, but with a weaker margin than in Grover.

The two frameworks are best viewed as complementary. The spectral analysis~\cite{braida2024unstructured} identifies $A_1$, $s^*$, and the piecewise gap structure, while the variational analysis~\cite{GuoAn2025} identifies the optimal power-law exponent. Together they give a complete account of the rank-one $\alpha = 1$ case, which sits exactly at the boundary where both frameworks apply and the measure condition remains bounded.

This complementarity persists under partial spectral knowledge. The RC framework ($p = 2$) builds the schedule from crossing position $s^*$, so its runtime degrades on the crossing-localization scale:
\[
T_{\mathrm{RC}}(\varepsilon_{A_1})
= T_{\mathrm{RC},\infty}\cdot \Theta\!\left(\max\!\left(1, \frac{\varepsilon_{A_1}}{\delta_{A_1}}\right)\right),
\]
where $\delta_{A_1} = 2\sqrt{d_0 A_2/N}$ is the threshold from \autoref{thm:interpolation}. The JRS framework ($p = 3/2$) instead uses certified bounds $(C_+, g_-)$ on the measure constant and minimum gap, leading to multiplicative overhead
\[
\frac{(1 + \delta_C/C)^2}{1 - \delta_g/g_{\min}},
\]
where $\delta_C$ and $\delta_g$ are estimation errors. The sensitivity profiles are therefore different: RC needs exponentially precise localization of $s^*$, while JRS needs only constant relative accuracy in $C$ and $g_{\min}$. In the partial-information regime forced by NP-hardness, this can make JRS more robust in practice even though both methods share the same asymptotic scaling.

The gap geometry and optimality analysis above assumes the rank-one
interpolation
\[
H(s) = -(1-s)\ket{\psi_0}\!\bra{\psi_0} + sH_z.
\]
The rank-one structure is a design choice, not a physical constraint. Can a
different design, with a different initial state, ancilla qubits, or a
multi-segment path, avoid the $A_1$ dependence entirely?


\section{Anatomy of the Barrier}
\label{sec:barrier-anatomy}

No instance-independent modification within the rank-one framework can make
$s^*$ spectrum-independent. We now close the obvious escape routes one by one.
The order matters. We start with the weakest modification and end with the most
general path redesign, then combine the four obstructions into a single no-go
statement.

Recall from Chapter~5 that for any initial state
$\ket{\psi} \in \mathbb{C}^N$, the weights
$w_k(\psi) = \sum_{z \in \Omega_k} |\braket{z}{\psi}|^2$
determine
$A_1^{\mathrm{eff}}(\psi) = \sum_{k \geq 1} w_k(\psi)/(E_k - E_0)$
and therefore
$s^*(\psi) = A_1^{\mathrm{eff}}(\psi)/(A_1^{\mathrm{eff}}(\psi) + 1)$.
For the uniform superposition $\ket{\psi_0}$, we recover
$w_k = d_k/N$ and $A_1^{\mathrm{eff}} = A_1$.

\begin{theorem}[Product ancilla invariance]
\label{thm:product-ancilla}
For any product initial state $\ket{\Psi} = \ket{\psi_0} \otimes \ket{\phi}$ and uncoupled final Hamiltonian $H_f = H_z \otimes I_{2^m}$, the extended Hamiltonian $H_{\mathrm{ext}}(s) = -(1-s)\ket{\Psi}\!\bra{\Psi} + s(H_z \otimes I_{2^m})$ has the same crossing position $s^* = A_1/(A_1 + 1)$ as the bare system.
\end{theorem}

\begin{proof}
Decompose the extended Hilbert space $\mathbb{C}^N \otimes \mathbb{C}^{2^m}$ into the subspace $\mathcal{V}_\phi = \mathbb{C}^N \otimes \ket{\phi}$ and its orthogonal complement. States $\ket{z} \otimes \ket{a}$ with $\braket{\phi}{a} = 0$ satisfy $\braket{\Psi}{z,a} = 0$, so they are exact eigenstates of $H_{\mathrm{ext}}(s)$ with eigenvalue $sE(z)$. These $N(2^m - 1)$ states do not participate in the avoided crossing. On $\mathcal{V}_\phi$, the restriction of $H_{\mathrm{ext}}(s)$ is unitarily equivalent to the bare Hamiltonian $H(s)$ via the map $\ket{\psi} \otimes \ket{\phi} \mapsto \ket{\psi}$.
\end{proof}

\begin{remark}
The crossing position is invariant, but the gap of $H_{\mathrm{ext}}(s)$ is strictly smaller than the bare gap: for $d_0 = 1$, the extra eigenvalues at $sE_0$ (from states $\ket{z} \otimes \ket{a}$ with $z \in \Omega_0$, $a \perp \ket{\phi}$) sit between the ground eigenvalue $\lambda_0(s) < sE_0$ and the crossing branch. Uncoupled ancillas make the gap worse, not better.
\end{remark}

This first result is already instructive. Extra qubits by themselves do not
move the bottleneck. They only add spectators and can even shrink the usable
gap. The next route is therefore to change the initial state itself while
keeping the algorithm instance-independent.

\begin{theorem}[Universality of uniform superposition]
\label{thm:universality}
Among all states $\ket{\psi} \in \mathbb{C}^N$, the uniform superposition $\ket{\psi_0}$ is the unique state (up to per-basis-element phases) for which the weights $w_k(\psi)$ depend only on $\{E_k, d_k\}$ and not on the specific assignment of energies to computational basis states.
\end{theorem}

\begin{proof}
An energy assignment is a function $\sigma: \{0,\ldots,N-1\} \to \{E_0,\ldots,E_{M-1}\}$ with $|\sigma^{-1}(E_k)| = d_k$. The weights under assignment $\sigma$ are $w_k(\psi,\sigma) = \sum_{z:\sigma(z)=E_k} |\braket{z}{\psi}|^2$. We require $w_k(\psi,\sigma) = w_k(\psi,\sigma')$ for all assignments $\sigma, \sigma'$ with the same degeneracies.

Any two such assignments are related by a permutation $\pi$ of $\{0,\ldots,N-1\}$. The condition becomes $\sum_{z \in \Omega_k} |\braket{z}{\psi}|^2 = \sum_{z \in \Omega_k} |\braket{\pi^{-1}(z)}{\psi}|^2$ for all $k$ and all permutations $\pi$.

\emph{Necessity.} Consider two-level spectra with $d_0 = 1$. For any two basis states $z_a, z_b$, the transposition swapping them maps the assignment $\sigma$ (with $\sigma(z_a) = E_0$) to $\sigma'$ (with $\sigma'(z_b) = E_0$). The condition forces $|\braket{z_a}{\psi}|^2 = |\braket{z_b}{\psi}|^2$. Since $z_a, z_b$ are arbitrary, $|\braket{z}{\psi}|^2 = 1/N$ for all $z$.

\emph{Sufficiency.} If $|\braket{z}{\psi}|^2 = 1/N$ for all $z$, then $w_k = d_k/N$ regardless of the assignment.
\end{proof}

\begin{corollary}
\label{cor:universality}
Any instance-independent adiabatic algorithm (same Hamiltonian for all energy assignments with the same degeneracy structure) must use the uniform superposition as initial state, fixing the crossing at $s^* = A_1/(A_1+1)$.
\end{corollary}

So changing the state cannot help unless we give up instance independence. A
more aggressive route is to keep the uniform state but add a fixed ancilla
coupling $V$ and hope it cancels the instance dependence.

\begin{theorem}[Coupled ancilla limitation]
\label{thm:coupled-ancilla}
Consider an extended Hamiltonian $H_{\mathrm{ext}}(s) = -(1-s)\ket{\Psi}\!\bra{\Psi} + s(H_z \otimes I + V)$ where $\ket{\Psi} = \ket{\psi_0} \otimes \ket{\phi}$ and $V$ is instance-independent. No fixed $V$ makes $A_1^{\mathrm{eff}}$ constant across all problem instances.
\end{theorem}

\begin{proof}
Consider the two-level family parametrized by $\Delta > 0$, with $E_0 = 0$, $E_1 = \Delta$, $d_0 = 1$, and $d_1 = N - 1$. For $\Delta > 2\lVert V \rVert$, Weyl's inequality implies that each eigenvalue of $H_f(\Delta) = H_z(\Delta) \otimes I + V$ lies within $\lVert V \rVert$ of an eigenvalue of $H_z(\Delta) \otimes I$. The spectrum therefore splits into two separated clusters, one near energy $0$ and one near energy $\Delta$. For each eigenvalue $E_j$ in the excited cluster, $|E_j - \Delta| \leq \lVert V \rVert$. Hence the excited contribution to $A_1^{\mathrm{eff}}$ is
\[
\sum_{j \in \mathrm{excited}} \frac{|\!\braket{\Psi}{\phi_j}\!|^2}{E_j - E_0}
= \frac{1 - d_0/N}{\Delta + O(\lVert V \rVert)}
= \Theta(1/\Delta)
\]
for $\Delta \gg \lVert V \rVert$. Because $\lVert V \rVert$ is fixed independently of $\Delta$, this contribution varies with $\Delta$, so $A_1^{\mathrm{eff}}(\Delta)$ cannot be constant.
\end{proof}

Coupling helps move levels, but it does not erase spectrum dependence. The last
escape route inside the same design philosophy is to insert intermediate
segments and move the problematic crossing to a more controlled stage of the
path.

\begin{theorem}[Multi-segment rigidity]
\label{thm:multi-segment}
Consider a two-segment path where segment 2 has Hamiltonian $H_2(t) = -(1-t)\ket{\psi_{\mathrm{mid}}}\!\bra{\psi_{\mathrm{mid}}} + tH_z$. If the algorithm is instance-independent, then the intermediate state $\ket{\psi_{\mathrm{mid}}}$ must be the uniform superposition, giving the same crossing $B_1 = A_1$.
\end{theorem}

\begin{proof}
Segment 2 is a rank-one adiabatic Hamiltonian with initial state $\ket{\psi_{\mathrm{mid}}}$. Its crossing position is $t^* = B_1/(B_1+1)$ where $B_1 = \sum_{k \geq 1} w_k(\psi_{\mathrm{mid}})/(E_k - E_0)$. If segment 1 does not involve $H_z$, then $\ket{\psi_{\mathrm{mid}}}$ is determined entirely by segment 1's Hamiltonian, which is instance-independent. Since $\ket{\psi_{\mathrm{mid}}}$ is then the same for all energy assignments with the same degeneracy structure, \autoref{thm:universality} forces $w_k = d_k/N$, so $B_1 = A_1$. If segment 1 involves $H_z$, then $\ket{\psi_{\mathrm{mid}}}$ already depends on the spectrum, and the algorithm is not instance-independent.
\end{proof}

\begin{theorem}[No-go]
\label{thm:no-go}
For any adiabatic algorithm using a rank-one initial Hamiltonian, a final Hamiltonian whose ground state encodes the solution, and instance-independent design, the crossing position cannot be made independent of the problem spectrum.
\end{theorem}

\begin{proof}
Combine Theorems~\ref{thm:product-ancilla}--\ref{thm:multi-segment}. \autoref{thm:universality} forces the uniform superposition. \autoref{thm:product-ancilla} shows that uncoupled ancillas preserve $s^*$. \autoref{thm:coupled-ancilla} shows that coupled ancillas can shift $s^*$ but cannot make it constant. \autoref{thm:multi-segment} then rules out escape through multi-segment paths inside the rank-one framework.
\end{proof}

For the running example ($N = 4$, $d_0 = 1$), product ancilla invariance (\autoref{thm:product-ancilla}) implies that appending any number of ancilla qubits in a product state leaves the crossing at $s^* = 3/7$.

We have now exhausted the rank-one design space under instance independence.
The natural objection is that rank one may simply be too narrow. We next move
to rank-$k$ projectors and show that the same dependence persists.

For rank-$k$ projectors $P = UU^\dagger$, the secular equation becomes a
$k \times k$ determinant condition
\[
\det(I_k - (1-s)G(\lambda,s)) = 0, \qquad
G(\lambda,s) = U^\dagger(sH_z - \lambda I)^{-1}U.
\]
On the two-level family ($E_0 = 0$, $E_1 = \Delta$), this reduces to
\[
\det(I_k - (x/\Delta)B) = 0, \qquad
B = U_{\mathrm{exc}}^\dagger U_{\mathrm{exc}}, \quad x = (1-s)/s.
\]

Each positive eigenvalue $\mu$ of $B$ gives a crossing branch $s(\Delta) = 1/(1+\Delta/\mu)$, and this branch is non-constant in $\Delta$.

\begin{proposition}[Rank-$k$ two-level obstruction]
\label{prop:rank-k-obstruction}
Fixed rank-$k$ projectors cannot make crossing positions spectrum-independent on fixed-degeneracy two-level families unless the projector has zero support on excited states.
\end{proposition}

The two-level obstruction already blocks constant crossings in the simplest
nontrivial family. For general multilevel spectra, a trace argument gives an
even cleaner obstruction.

\begin{proposition}[Trace no-go]
\label{prop:trace-nogo}
For a rank-$k$ projector $P = UU^\dagger$ and the multilevel family with gaps $\Delta_1, \ldots, \Delta_{M-1}$, define the reduced matrix $A(\Delta) = \sum_{\ell=1}^{M-1} B_\ell/\Delta_\ell$ where $B_\ell = U_\ell^\dagger U_\ell \succeq 0$ collects the excited-level contributions. If $B_j \neq 0$ and $\Delta_j$ varies, then $\mathrm{tr}(A(\Delta)) = \sum_\ell \mathrm{tr}(B_\ell)/\Delta_\ell$ is non-constant in $\Delta_j$. By Weyl's eigenvalue monotonicity theorem, each eigenvalue of $A(\Delta)$ is a continuous function of $\Delta_j$, and the sum of the positive eigenvalues equals $\mathrm{tr}(A)$. Since the trace changes, at least one positive eigenvalue, and hence at least one crossing position, must change with $\Delta_j$.
\end{proposition}

\begin{remark}
When the excited blocks commute ($[B_\ell, B_m] = 0$ for all $\ell, m$), the reduced crossing equation can be simultaneously diagonalized. This gives explicit per-branch formulas. For each active branch $r$ with $G_r(\Delta) = \sum_\ell \mu_{\ell r}/\Delta_\ell > 0$, the crossing position is $s_r = G_r/(1 + G_r)$. Varying any gap $\Delta_j$ yields
\[
\frac{\partial s_r}{\partial \Delta_j}
= -\frac{\mu_{jr}}{\Delta_j^2(1 + G_r)^2}
\leq 0,
\]
with strict inequality whenever $\mu_{jr} > 0$. So the commuting case gives explicit quantitative non-constancy for each branch, complementing the trace argument's aggregate statement. Even this most tractable version of the generalized secular equation cannot produce spectrum-independent crossings.
\end{remark}

At this point the pattern is hard to miss. The barrier is structural inside the
rank-one framework and survives the move to higher-rank projectors. It may
still be possible to escape with genuinely different control models, such as
time-dependent couplings or non-rank-one intermediate Hamiltonians, but those
lie outside the present theorem. This distinction matters because once we drop
the monotone-schedule restriction, constant controls already recover the Grover
timescale on the restricted two-level family.

\begin{proposition}[Constant-control optimality on two-level family]
\label{prop:constant-control}
For $H_z = I - P_0$ where $P_0$ projects onto the $d_0$-dimensional ground space, the continuous-time rank-one Hamiltonian $H = -\ket{\psi_0}\!\bra{\psi_0} + H_z$ with constant controls achieves $p_0(t^*) = 1$ at $t^* = (\pi/2)\sqrt{N/d_0}$, with controls independent of $A_1$ (on the two-level family $H_z = I - P_0$).
\end{proposition}

\begin{proof}
Let $\mu = d_0/N$, $\ket{G} = d_0^{-1/2}\sum_{x \in S_0}\ket{x}$, and $\ket{B} = (N - d_0)^{-1/2}\sum_{x \notin S_0}\ket{x}$. The initial state is $\ket{\psi_0} = \sqrt{\mu}\,\ket{G} + \sqrt{1-\mu}\,\ket{B}$. Dropping the global identity term, the effective Hamiltonian in the $(\ket{G}, \ket{B})$ basis is
\begin{equation}
\widetilde{H} = -\begin{pmatrix} \mu & \sqrt{\mu(1-\mu)} \\ \sqrt{\mu(1-\mu)} & -\mu \end{pmatrix},
\end{equation}
which satisfies $\widetilde{H}^2 = \mu\,I_2$. The matrix exponential is $e^{-it\widetilde{H}} = \cos(\sqrt{\mu}\,t)\,I_2 - i\sin(\sqrt{\mu}\,t)\,\widetilde{H}/\sqrt{\mu}$. Applying to $\ket{\psi_0}$ and computing the ground-state probability:
\begin{equation}
p_0(t) = |\!\braket{G}{e^{-it\widetilde{H}}|\psi_0}\!|^2 = \mu + (1 - \mu)\sin^2(\sqrt{\mu}\,t).
\end{equation}
At $t^* = (\pi/2)/\sqrt{\mu} = (\pi/2)\sqrt{N/d_0}$, $\sin^2(\sqrt{\mu}\,t^*) = 1$, so $p_0(t^*) = 1$.
\end{proof}

On the two-level family, the Hamiltonian self-calibrates through a Rabi-like oscillation at frequency $\sqrt{\mu} = \sqrt{d_0/N}$. The time-independent Hamiltonian $H_r = -\ket{\psi_0}\!\bra{\psi_0} + r \cdot H_z$ with $r = 1$ is optimal without explicit knowledge of $A_1$~\cite{eduardo}. For general spectra, the resonance shifts to $r^* = A_1$~\cite{eduardo}. The calibration problem then changes from classical computation of $A_1$ to quantum detection of a resonance.

A natural approach is Loschmidt-echo measurement. Evolve under $H_r$ and measure return probability $|\!\braket{\psi_0}{e^{-iH_r t}\psi_0}\!|^2$, which oscillates with large amplitude near resonance and stays close to $1$ away from resonance. On two-level families this works cleanly: binary search over $r$ with $O(n)$ probe measurements, each costing $O(1/g_{\min})$, locates $r^*$ with polynomial overhead (details in~\cite{eduardo}). For general multilevel spectra, additional excited-state frequencies can mask the resonance signal. Whether one can efficiently deconvolve this multilevel echo, or replace it with a better observable, remains open.

The constant-control counterexample applies only to the two-level family $H_z = I - P_0$. Under normalized controls, the barrier reappears.

\begin{proposition}[Normalized-control lower bound]
\label{prop:normalized-control}
Under normalized controls $|g(t)| \leq 1$ and the scaled family $H_z^{(\delta)} = \delta(I - P_0)$ with minimum excitation $\delta \in (0, 1]$, any instance-independent algorithm achieving success probability $\geq 2/3$ requires $T = \Omega(\sqrt{N/d_0}/\delta)$.
\end{proposition}

\begin{proof}
The oracle-dependent term $g(t)H_z^{(\delta)} = \delta g(t)(I - P_0)$ has instantaneous oracle strength $\delta|g(t)|$. The total oracle action is $\mathcal{A} = \int_0^T \delta|g(t)|\,dt \leq \delta T$. The continuous-time query lower bound for unstructured search with $d_0$ marked items among $N$ gives $\mathcal{A} = \Omega(\sqrt{N/d_0})$~\cite{farhi1998analog}, so $T = \Omega(\sqrt{N/d_0}/\delta)$.
\end{proof}

For $\delta = N^{-1/2}$, this yields $T = \Omega(N/\sqrt{d_0})$, the same exponential penalty as the fixed-schedule adiabatic model. So the barrier reappears on worst-case instances even for general continuous-time rank-one algorithms when controls are normalized. The scope is precise: the obstruction comes from monotone schedules with bounded controls, not from continuous-time quantum computation itself.


\section{Computational Nature of $A_1$}
\label{sec:computational-nature}

The barrier cannot be designed away. What kind of hardness is it? The quantity $A_1$ is not merely NP-hard to compute. Its core hardness is counting hardness inherited from partition-function structure. The tractability boundary does not align with optimization hardness, is not controlled by the number of solutions alone, and depends on structural properties of the energy landscape.

The quantity $A_1$ encodes spectral information beyond the minimum gap. Consider
three energy levels with $E_0 = 0$ ($d_0 = 1$), $E_1 = 1/n$ ($d_1 = 1$), and
$E_2 = 1$ ($d_2 = N-2$). Then $\Delta = 1/n \to 0$ but
$A_1 = (n + N - 2)/N \to 1$, so $1/\Delta \to \infty$ while $A_1 = \Theta(1)$.
The tail of $N-2$ states at energy $1$ contributes $(N-2)/N \approx 1$ to
$A_1$, while the single state at the gap edge contributes only $n/N \approx 0$.
So the crossing position $s^* = A_1/(A_1+1) \approx 1/2$ is determined by the
bulk of the spectrum, not by the minimum gap. In this sense $A_1$ is a
whole-spectrum quantity that $\Delta$ alone cannot predict.

The distinction between NP-hardness at precision $1/\mathrm{poly}(n)$ (\autoref{thm:np-hard-A1}) and $\#$P-hardness exactly (\autoref{thm:sharp-P-hard-A1}) matters because $A_1$ is fundamentally a counting quantity.

\begin{proposition}[$A_1$ hardness is counting hardness]
\label{prop:counting-hardness}
For Boolean CSPs where counting satisfying assignments is $\#$P-hard (including $k$-SAT for $k \geq 2$), computing $A_1$ of the clause-violation Hamiltonian is $\#$P-hard even restricted to satisfiable instances.
\end{proposition}

\begin{proof}
Encode the CSP as $H_z = \sum_{j=1}^m C_j$ where each $C_j(x) = 1$ if assignment $x$ violates clause $j$. The interpolation argument (\autoref{thm:sharp-P-hard-A1}) recovers all degeneracies $d_k$ from polynomially many evaluations of $A_1$ with shifted parameters, via Lagrange interpolation on the rational function $f(x) = \sum_k d_k/(\Delta_k + x/2)$. For satisfiable CSPs, $d_0$ counts satisfying assignments, and counting is $\#$P-hard by hypothesis.
\end{proof}

The partition function connection makes this precise. Shifting energies so that $E_0 = 0$ and defining the Laplace partition function $Z(\beta) = \sum_x e^{-\beta E(x)}$, the spectral parameter admits the integral representation
\begin{equation}
\label{eq:A1-laplace}
A_1 = \frac{1}{N}\int_0^\infty (Z(\beta) - d_0)\,d\beta.
\end{equation}
For integer spectra with $E(x) \in \{0,1,\ldots,m\}$, the ordinary generating function $Z(t) = \sum_x t^{E(x)}$ gives
\[
A_1 = \frac{1}{N}\int_0^1 \frac{Z(t) - d_0}{t}\,dt.
\]
At first glance both formulas seem to require $d_0$, which is itself counting-hard for many CSPs. For additive approximation, however, one can replace $d_0$ by a single low-temperature sample $Z(\tau)$ with small $\tau > 0$. Define the $\tau$-truncated proxy
\begin{equation}
\label{eq:A1-proxy}
A_1^{(\tau)} = \frac{1}{N}\int_\tau^1 \frac{Z(t) - Z(\tau)}{t}\,dt.
\end{equation}
The additive error satisfies $0 \leq A_1 - A_1^{(\tau)} \leq \tau(1 + \ln(1/\tau))$. Choosing $\tau = O(\eta/\ln(1/\eta))$ therefore gives an $\eta$-approximation to $A_1$ without direct access to $d_0$.

When $E_0$ is known, this coarse form is already useful. Sampling from the Boltzmann distribution at inverse temperature $\beta$ gives an unbiased estimator of $Z(\beta)/Z(0) = Z(\beta)/N$. Integrating those estimates through Eq.~\eqref{eq:A1-laplace} gives an additive approximation to $A_1$ without explicitly counting $d_0$. In this way, computing $A_1$ becomes a partition-function task, and tractability of $A_1$ tracks tractability of counting.

\begin{proposition}[Bounded treewidth tractability]
\label{prop:treewidth}
For local energy functions $E(x) = \sum_j E_j(x_{S_j})$ with bounded locality $|S_j| \leq k$ and a tree decomposition of the primal graph of width $w$, $A_1$ is computable exactly in $\mathrm{poly}(n,m) \cdot 2^{O(w)}$ time.
\end{proposition}

\begin{proof}
Write the partition-function polynomial as $Z(t) = \sum_x t^{E(x)} = \sum_{q=0}^m d_q t^q$, and also in factor-graph form as $Z(t) = \sum_x \prod_j t^{E_j(x_{S_j})}$. Variable elimination on the tree decomposition then computes $Z(t)$ exactly. At each elimination step, factor tables have at most $2^{w+1}$ entries, each a polynomial of degree at most $m$. Multiplying factors convolves polynomials at cost $O(m^2)$ per entry, and summing out a variable adds two polynomials at cost $O(m)$. After $n$ elimination steps, we recover $Z(t) = \sum_q d_q t^q$, and then $A_1 = (1/N)\sum_{q > E_0} d_q/(q - E_0)$.
\end{proof}

The treewidth condition is sufficient, not necessary. A simpler criterion
applies when the spectrum itself is simple. If $H_z$ has at most
$\mathrm{poly}(n)$ distinct energy levels with known energies and degeneracies,
then $A_1 = (1/N)\sum_{k \geq 1} d_k/(E_k - E_0)$ is directly computable in
$\mathrm{poly}(n)$ time from its defining sum. This criterion is complementary
to bounded treewidth and applies whenever the spectrum is structurally simple,
regardless of the interaction graph. For example, Hamming-distance costs
$E(x) = |x \oplus z_0|$ have $M = n+1$ levels, degeneracies
$d_k = \binom{n}{k}$, and energies $E_k = k$. Then
$A_1 = (1/N)\sum_{k=1}^n \binom{n}{k}/k$ depends only on $n$ and is trivial to
compute.

The partition-function bridge is one-directional. Tractable $Z$ implies tractable $A_1$ (via the integral representations above), but exact $A_1$ does not determine low-temperature $Z(\beta)$.

\begin{proposition}[Reverse bridge obstruction]
\label{prop:reverse-bridge}
There exist two diagonal Hamiltonians $H_z$, $H_z'$ on $N = 2^n$ states with the same ground degeneracy ratio $d_0/N$, same minimum excitation $\Delta_{\min}$, and $A_1(H_z) = A_1(H_z')$ exactly, yet $|Z_{H_z}(\beta) - Z_{H_z'}(\beta)|/N \geq 1/100$ at $\beta = O(1/\Delta_{\min})$.
\end{proposition}

\begin{proof}
Fix an integer $B \geq 3$. Define two spectra with the same $d_0/N = 1/2$ and $\Delta_{\min} = 1/B$. The first has $N/8$ states at energy $1/B$ and $3N/8$ states at energy $B$. The second has $N/16$ states at energy $1/B$ and $7N/16$ states at energy $c_B = 7B/(B^2+6)$. Direct computation gives the same value $A_1 = (B^2+3)/(8B)$ for both. At $\beta = B$, however,
\[
\frac{Z_1(B)}{N} = \frac{1}{2} + \frac{e^{-1}}{8} + \frac{3e^{-B^2}}{8},
\qquad
\frac{Z_2(B)}{N} = \frac{1}{2} + \frac{e^{-1}}{16} + \frac{7}{16}e^{-7B^2/(B^2+6)}.
\]
Since $7B^2/(B^2+6) \geq 4.2$ for $B \geq 3$, the difference is at least $e^{-1}/16 - (7/16)e^{-4.2} > 1/100$.
\end{proof}

Three natural conjectures about easy instances of $A_1$ computation are all false.

\begin{proposition}[Unique solution does not imply easy $A_1$]
\label{prop:conjecture-unique}
There exist instances with $d_0 = 1$ for which computing $A_1$ is $\#$P-hard.
\end{proposition}

\begin{proof}
The proof of \autoref{prop:counting-hardness} applies directly to satisfiable instances with $d_0 = 1$. The interpolation reduction recovers $d_1, \ldots, d_{M-1}$ from $A_1$ evaluations, and counting assignments at each violation level remains $\#$P-hard. Concretely, for a satisfiable 3-SAT instance with $m$ clauses and a unique satisfying assignment, the clause-violation Hamiltonian $H_z = \sum_j C_j$ has $d_0 = 1$ and
\[
A_1 = \sum_{k=1}^m \frac{d_k}{kN},
\]
where $d_k$ counts assignments that violate exactly $k$ clauses. Recovering these counts from shifted $A_1$ evaluations is therefore $\#$P-hard.
\end{proof}

\begin{proposition}[Bounded degeneracy is vacuous]
\label{prop:conjecture-bounded}
If $d_k \leq \mathrm{poly}(n)$ for all excited levels $k \geq 1$, and $M \leq \mathrm{poly}(n)$, then $d_0 \geq N - \mathrm{poly}(n)^2$, and the optimization problem is trivially solvable by random sampling.
\end{proposition}

\begin{proof}
The total state count satisfies $\sum_{k=0}^{M-1} d_k = N = 2^n$. If $d_k \leq \mathrm{poly}(n)$ for all $k \geq 1$ and $M \leq \mathrm{poly}(n)$, then $\sum_{k \geq 1} d_k \leq (M-1) \cdot \mathrm{poly}(n) \leq \mathrm{poly}(n)^2$, so $d_0 \geq N - \mathrm{poly}(n)^2$. For $n$ large enough, $d_0/N \geq 1 - o(1)$, and a random sample finds a ground state with probability $1 - o(1)$.
\end{proof}

\begin{proposition}[Hard optimization does not imply hard $A_1$]
\label{prop:conjecture-hard-opt}
The tractability of $A_1$ is independent of optimization hardness. 2-SAT is in P but $\#$2-SAT is $\#$P-complete~\cite{valiant1979complexity}, giving easy optimization with hard $A_1$. In the reverse direction, Grover search with a promised ground degeneracy $d_0$ gives hard optimization but trivial $A_1 = (N - d_0)/(N\Delta)$, computable in $O(1)$ from the promise.
\end{proposition}

The tractability boundary for $A_1$ is subtle. It does not align with
optimization hardness, is not determined by the number of solutions, and depends
on structural properties of the energy spectrum (treewidth, partition-function
tractability) rather than on the difficulty of finding the ground state.


\section{Complexity Landscape}
\label{sec:complexity-landscape}

This section closes the complexity picture at the algorithmically relevant
precision $\varepsilon = 2^{-n/2}$. The next four results play different roles.
We first pin down the quantum query complexity at this precision, then compare
it with classical sampling, then lift the separation to ETH-based time
complexity, and finally connect the result back to adiabatic pre-computation.

\begin{theorem}[Tight quantum query complexity at schedule precision]
\label{thm:tight-quantum}
At the schedule-relevant precision $\varepsilon = \Theta(2^{-n/2})$, the quantum query complexity of $A_1$ estimation is $\Theta(2^{n/2}) = \Theta(1/\varepsilon)$. The lower bound is achieved on two-level instances with $\Delta_1 = 1$.
\end{theorem}

\begin{proof}
For $\varepsilon = \Theta(2^{-n/2})$ and $\Delta_1 = 1$,
\autoref{thm:quantum-A1} gives
$O(\sqrt{N} + 1/\varepsilon) = O(2^{n/2})$.

For the lower bound, restrict to $M = 2$ instances with $\Delta_1 = 1$. Then
estimating $A_1 = (N-d_0)/N$ to precision $\varepsilon$ is approximate
counting. The Grover iterate
$G = (2\ket{+}\!\bra{+} - I)(I - 2\Pi_S)$
has eigenphases $\pm 2\theta$ with $\sin^2\theta = d_0/N$. After $T$ sequential
uses of $G$, the probe accumulates phase $T\phi$ where
$\phi = 2\arcsin(\sqrt{d_0/N})$.

The quantum Fisher information for estimating $\phi$ is
$F_Q = 4T^2$, the standard Heisenberg scaling for sequential phase accumulation
\cite{braunstein1994statistical, giovannetti2006quantum}. By the quantum
Cram\'er-Rao bound,
$\mathrm{Var}(\hat{\phi}) \geq 1/F_Q = 1/(4T^2)$.
Now choose the worst-case operating point $d_0 = N/2$, so
$\sin^2\theta = 1/2$ and
$|d(\sin^2\theta)/d\theta| = |\sin 2\theta| = 1$.
At this point, estimating
$A_1 = 1 - \sin^2\theta$ to precision $\varepsilon$ requires estimating
$\theta$ to precision $\varepsilon$. Hence
$1/(4T^2) \leq \varepsilon^2$, so $T \geq 1/(2\varepsilon)$ applications of $G$.
Each application uses $O(1)$ oracle calls. Therefore the lower bound is
$\Omega(1/\varepsilon)$, and at
$\varepsilon = \Theta(2^{-n/2})$ we get $\Omega(2^{n/2})$.

Combining upper and lower bounds gives
$\Theta(2^{n/2}) = \Theta(1/\varepsilon)$.
\end{proof}

\noindent The lower bound is worst-case in the unknown parameter $d_0$, and the
adversary picks $d_0 = N/2$ because that point is maximally difficult to
estimate. The Heisenberg limit $F_Q = 4T^2$ for sequential unitary applications
is standard in quantum metrology and applies to general quantum estimation, not
only phase estimation. It follows from unitary dynamics together with the
Cram\'er-Rao inequality, as shown by Braunstein and
Caves~\cite{braunstein1994statistical} and extended to sequential settings by
Giovannetti, Lloyd, and Maccone~\cite{giovannetti2006quantum}. The same
$\Omega(1/\varepsilon)$ lower bound also follows from quantum approximate
counting. Estimating $d_0/N$ to additive precision $\varepsilon$ requires
$\Omega(1/\varepsilon)$ quantum queries~\cite{BrassardHoyerMoscaEtAl2002, NayakWu1999},
and for $M = 2$ this is exactly the $A_1$ estimation problem.

This theorem connects directly to the adaptive protocol of \autoref{sec:quantum-bypass}. The adaptive strategy achieves $T_{\mathrm{adapt}} = O(T_{\mathrm{inf}}) = O(2^{n/2})$, which matches the tight complexity $\Theta(2^{n/2})$ for estimating $A_1$ at precision $\delta_{A_1} = \Theta(2^{-n/2})$. The match is structural rather than accidental. In both tasks, the algorithm must resolve $\Omega(2^{n/2})$ alternatives while each quantum measurement reveals only $O(1)$ information.

In the high-precision regime relevant to schedule placement, the quadratic quantum advantage persists.

\begin{proposition}[Precision phase diagram]
\label{prop:precision-phase}
For two-level instances with $\Delta_1 = 1$ and precision $\varepsilon \leq c/\sqrt{N}$ (constant $c$), the query complexity of $A_1$ estimation is $\Theta(1/\varepsilon)$ quantum and $\Theta(1/\varepsilon^2)$ classical. The quantum-to-classical ratio is $\Theta(1/\varepsilon)$ in this regime.
\end{proposition}

\begin{proof}
In this regime, $1/\varepsilon \geq \sqrt{N}/c$, so the upper bound of \autoref{thm:quantum-A1} becomes $O(\sqrt{N} + 1/\varepsilon) = O(1/\varepsilon)$. The matching quantum lower bound is inherited from two-level approximate counting (\autoref{thm:tight-quantum}). The classical lower bound $\Omega(1/\varepsilon^2)$ follows from \autoref{thm:classical-lower-A1}, and Monte Carlo sampling gives the matching upper bound $O(1/\varepsilon^2)$.
\end{proof}

The previous proposition is still a query statement. The next theorem asks what
happens when we account for total computation time under a standard complexity
assumption.

\begin{theorem}[ETH computational complexity]
\label{thm:eth}
Under the Exponential Time Hypothesis (ETH), any classical algorithm computing $A_1$ at precision $2^{-n/2}$ requires $2^{\Omega(n)}$ time.
\end{theorem}

\begin{proof}[Proof sketch]
The NP-hardness reduction (\autoref{thm:np-hard-A1}) maps 3-SAT on $n_{\mathrm{var}}$ variables to $A_1$ estimation of a 3-local Hamiltonian on $n = O(n_{\mathrm{var}})$ qubits at precision $1/\mathrm{poly}(n)$. By the Impagliazzo-Paturi-Zane sparsification lemma~\cite{impagliazzo2001problems}, 3-SAT on $n_{\mathrm{var}}$ variables can be reduced to instances with $O(n_{\mathrm{var}})$ clauses, giving $n = O(n_{\mathrm{var}})$ qubits. Any algorithm computing $A_1$ at precision $2^{-n/2}$ can, in particular, compute $A_1$ at the coarser precision $1/\mathrm{poly}(n)$ (since $2^{-n/2} < 1/\mathrm{poly}(n)$ for large $n$), and thus solves 3-SAT by the reduction. Under ETH, 3-SAT on $n_{\mathrm{var}} = \Omega(n)$ variables requires $2^{\Omega(n_{\mathrm{var}})} = 2^{\Omega(n)}$ time.
\end{proof}

Under ETH, the quadratic quantum advantage extends from the query model to the computational model.

\begin{corollary}[Quantum pre-computation cost]
\label{cor:quantum-precomp}
Estimating $A_1$ to the schedule-relevant precision $\delta_{A_1} = \Theta(2^{-n/2})$ via quantum amplitude estimation costs $\Theta(2^{n/2} \cdot \mathrm{poly}(n))$ time, matching the adaptive protocol's runtime. The classical pre-computation cost at the same precision is $\Omega(2^n)$.
\end{corollary}

From a parameterized-complexity viewpoint, the corollary places $A_1$
estimation at precision $2^{-n/2}$ in
\[
\mathrm{FBQTIME}(2^{n/2} \cdot \mathrm{poly}(n)).
\]
The problem is therefore not polynomial-time in the usual sense, but it does
admit a quantum algorithm at the Grover scale. This also clarifies the
information cost of fixed-schedule adiabatic optimization. Recovering the
missing $n/2$ bits of $A_1$ costs $\Theta(2^{n/2})$ quantum time, the same scale
as Grover search and informed adiabatic evolution. The circuit model reaches
that scale without a pre-computation step because amplitude amplification solves
the search task directly.

The generic extrapolation barrier (\autoref{thm:generic-barrier}) shows that
interpolation failure at precision $2^{-n/2}$ is not tied to one construction.
For any polynomial extrapolation scheme with $d = \mathrm{poly}(n)$ nodes, the
Lebesgue constant obeys $\Lambda_d(x^*) \geq 2^{d-1}$ when the evaluation point
sits at least $b-a$ away from the sample interval $[a,b]$. This geometric
condition must be verified in each setup. It does not follow only from
$x^* = \Theta(2^{-n/2})$ with nodes in $[0,1/\mathrm{poly}(n)]$. Once the
condition holds, polynomial-interpolation reductions for $A_1$ demand precision
$2^{-\Omega(n)}$.

\begin{proposition}[Two-level worst-case reduction]
\label{thm:structure-irrelevance}
The two-level family ($M = 2$) is a worst-case subclass for $A_1$ estimation at schedule-relevant precision: any worst-case lower bound for approximate counting on this subclass applies to general $A_1$ estimation.
\end{proposition}

\begin{proof}
Worst-case complexity over all instances is at least the complexity on any subclass. Restricting to $M = 2$ gives
\[
A_1 = \frac{N-d_0}{N\Delta_1},
\]
so for fixed $\Delta_1=1$, estimating $A_1$ to additive precision $\varepsilon$ is exactly approximate counting for $d_0/N$ at precision $\varepsilon$. Therefore, any lower bound for approximate counting on this subclass is automatically a lower bound for general $A_1$ estimation.
\end{proof}

\noindent The worst-case hardness of $A_1$ estimation does not hide in complex spectra. Simple two-level instances, where $A_1$ reduces to counting, already saturate the query lower bound. Algorithms exploiting the sum-of-reciprocals structure of $A_1$ for multilevel spectra cannot beat algorithms for plain mean estimation.

At schedule-level precision, bounded-treewidth instances remain tractable for
exact $A_1$ computation (\autoref{prop:treewidth}). Ferromagnetic Ising models
also admit a polynomial-time FPRAS for $Z(\beta)$~\cite{jerrum1993polynomial},
which yields additive approximations for $A_1$ at coarse precision through the
integral formulas in Eq.~\eqref{eq:A1-laplace} and Eq.~\eqref{eq:A1-proxy}. The
difficulty appears at algorithmic precision
$\delta_{A_1} = \Theta(2^{-n/2})$. Reaching this regime requires multiplicative
accuracy $\mu = O(2^{-n/2}/B)$ with
$B = O(\log(1/\delta_{A_1})/\Delta_{\min})$, so the FPRAS runtime
$\mathrm{poly}(1/\mu)$ becomes exponential. With $B = O(n/\Delta_{\min})$, one
gets $\mathrm{poly}(1/\mu) = 2^{\Omega(n)}$. Thus ferromagnetic-Ising
tractability does not survive at the schedule-relevant precision.

The interpolation theorem (\autoref{thm:interpolation}) gives a quantitative law relating information to runtime. We state it in a communication picture.

Alice has the full classical description of $H_z$ (all eigenvalues and degeneracies). Bob has a quantum computer with oracle access to $H_z$, and Alice may send $C$ classical bits to Bob. Bob must find a ground state in at most $T$ queries.

In the circuit-oracle model, Bob needs no message from Alice and still achieves
$T = O(\sqrt{N/d_0})$ by running D\"urr-H\o yer. In the fixed-schedule adiabatic
model, Bob must match the schedule to the crossing and therefore needs $s^*$ to
precision $\Delta_* = O(2^{-n/2})$, which costs $C = \Theta(n)$ bits. Each
additional bit of $A_1$ precision cuts the adiabatic runtime by a factor of two.
Roughly $n/2$ bits are enough for optimal scaling. Formally, if Alice sends $C$
bits that encode $A_1$ to precision $\varepsilon = \Theta(2^{-C})$, then
\[
T(C) = T_{\mathrm{inf}} \cdot \Theta(\max(1, 2^{n/2 - C})).
\]

\begin{theorem}[Bit-runtime information law]
\label{thm:bit-runtime}
The classical communication cost for the adiabatic model to achieve target runtime $T$ is $C^*(T) = \max(0,\,n/2 - \log_2(T/T_{\mathrm{inf}}))$ bits, while $C^*_{\mathrm{circuit}}(T) = 0$ for all $T \geq T_{\mathrm{inf}}$.
\end{theorem}

\begin{proof}
Inverting $T(C) = T_{\mathrm{inf}} \cdot 2^{n/2 - C}$ gives $C = n/2 - \log_2(T/T_{\mathrm{inf}})$. Clamping $C \geq 0$ and noting that the circuit model achieves $T = T_{\mathrm{inf}}$ at $C = 0$ by the D\"urr-H\o yer algorithm gives both formulas.
\end{proof}

The complete model comparison, synthesizing this chapter with Chapter~8, is given below.

\begin{center}
\begin{tabular}{llll}
\hline
Model & Info needed & Runtime & Communication \\
\hline
Circuit (D\"urr-H\o yer) & None & $\Theta(\sqrt{N/d_0})$ & 0 bits \\
Fixed AQO, informed & $A_1$ to $2^{-n/2}$ & $O(\sqrt{N/d_0})$ & $\Theta(n)$ bits \\
Fixed AQO, $C$ bits & $A_1$ to $2^{-C}$ & $T_{\mathrm{inf}} \cdot 2^{n/2-C}$ & $C$ bits \\
Fixed AQO, uninformed & None & $\Omega(N/\sqrt{d_0})$ & 0 bits \\
Adaptive AQO & $O(n)$ measurements & $O(\sqrt{N/d_0})$ & 0 bits \\
Constant-control, two-level & None & $\Theta(\sqrt{N/d_0})$ & 0 bits \\
Quantum $A_1$ estimation & $\varepsilon = 2^{-n/2}$ & $\Theta(2^{n/2})$ queries & --- \\
Classical $A_1$ estimation & $\varepsilon = 2^{-n/2}$ & $\Theta(2^n)$ queries & --- \\
\hline
\end{tabular}
\end{center}

\noindent The circuit model and the adaptive adiabatic model both achieve optimal performance with zero classical communication. In contrast, the fixed adiabatic model follows a bit-runtime tradeoff in which each missing bit doubles the runtime. The resulting $\Theta(n)$-bit separation from the circuit model is exactly the information content of $A_1$ at schedule-relevant precision. This communication cost reflects the computational model rather than the underlying search task.

The running example makes the tradeoff concrete. With $N = 4$, $d_0 = 1$, and $n = 2$, the circuit model uses $O(2)$ queries at $C = 0$. The informed adiabatic model also uses $O(2)$ queries, but only after receiving $C = 1$ bit. Without that bit, the fixed adiabatic model requires $\Omega(4)$ queries. The single missing bit explains the factor-of-two slowdown.

The term ``information gap'' now has three precise meanings. The first is
spectral. Inside the adiabatic framework, $g_{\min}$ sets the timescale
$T = O(1/g_{\min})$, and the full profile $g(s)$ constrains which schedules are
valid. The second is epistemic. If crossing location is unknown, runtime pays
the factor $(s_R - s_L)/\Delta_*$. If $A_1$ is known to precision
$\varepsilon$, the overhead becomes
$\Theta(\max(1, \varepsilon/\delta_{A_1}))$. With $O(n)$ quantum measurements,
this overhead disappears. The third is model-dependent. Circuit algorithms
neither need nor expose $A_1$, fixed-schedule adiabatic algorithms require it
and face NP-hard recovery, and adaptive adiabatic algorithms can acquire it at
polynomial cost.

This gives a five-level taxonomy of ignorance, measured by the multiplicative
overhead $T/T_{\mathrm{inf}}$. Level 0 (no information) gives
$\Omega(2^{n/2})$. Level 1 (precision $\varepsilon$) gives
$\Theta(\max(1, \varepsilon/\delta_{A_1}))$. Level 2 (known interval
$[u_L, u_R]$) gives constant overhead proportional to $u_R-u_L$. Level 3
(quantum measurement access) gives $O(1)$ overhead with $O(n)$ measurements.
Level 4 (circuit model) has overhead $1$ with no spectral information.

The adiabatic approach to unstructured search therefore works, reaches Grover
scaling, and is optimal within its schedule class. Its information requirements
are a structural consequence of the rank-one interpolation path. They are not a
fundamental limit of quantum computation itself. They are a limit of this
specific model. The next chapter translates these results into machine-checked
formal proofs.
